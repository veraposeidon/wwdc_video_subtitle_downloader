1
00:00:03,937 --> 00:00:06,773 line:-1
Hello and welcome to WWDC.


2
00:00:08,108 --> 00:00:12,279 line:0
Hello, everyone. Welcome to our session
on natural language processing.


3
00:00:12,346 --> 00:00:15,749 line:0
The goal of this session
is to help you make your apps smarter


4
00:00:15,816 --> 00:00:19,920 line:0
by using the power of NLP
in the Natural Language framework.


5
00:00:19,987 --> 00:00:22,422 line:0
I'm Vivek, and I'll be jointly presenting
this session


6
00:00:22,489 --> 00:00:24,291 line:0
with my colleague Doug Davidson.


7
00:00:24,358 --> 00:00:25,826 line:-1
So let's get started.


8
00:00:26,927 --> 00:00:29,696 line:-2
Let's begin
with the central notion of language.


9
00:00:29,763 --> 00:00:32,833 line:-2
Language is a core system
that helps us humans


10
00:00:32,900 --> 00:00:35,235 line:-2
solve difficult problems
through communication,


11
00:00:35,302 --> 00:00:38,939 line:-2
and it also provides us with
a very unique type of social interaction.


12
00:00:40,073 --> 00:00:43,143 line:-2
If you think of how we communicate
using language,


13
00:00:43,210 --> 00:00:45,479 line:-1
language is an intermediate representation


14
00:00:45,546 --> 00:00:48,549 line:-2
that helps us translate
concepts into symbols,


15
00:00:48,615 --> 00:00:51,051 line:-2
which can then be expressed
in the form of words,


16
00:00:51,118 --> 00:00:54,821 line:-2
phrases or sentences
with some grammatical structure.


17
00:00:54,888 --> 00:00:57,391 line:-2
The medium of expression
can be through speech,


18
00:00:57,457 --> 00:01:00,394 line:-2
perhaps through writing
on a keyboard or Apple Pencil.


19
00:01:00,460 --> 00:01:04,831 line:-2
It can even be an image or a video
that you capture using your camera.


20
00:01:04,897 --> 00:01:07,935 line:-2
Now, language also has
this remarkable property


21
00:01:08,001 --> 00:01:11,371 line:-2
that not only helps us
translate concepts into symbols,


22
00:01:11,438 --> 00:01:15,475 line:-2
but it also helps us
assimilate content into concepts.


23
00:01:15,542 --> 00:01:16,977 line:-1
In the last few years,


24
00:01:17,044 --> 00:01:20,581 line:-2
as we have moved from human intelligence
into machine intelligence,


25
00:01:20,647 --> 00:01:24,418 line:-2
this central notion of language
has been replaced by NLP.


26
00:01:24,484 --> 00:01:27,621 line:-2
NLP has now become
the intermediate representation


27
00:01:27,688 --> 00:01:30,958 line:-2
that helps machines
translate concepts into symbols,


28
00:01:31,024 --> 00:01:33,760 line:-1
and also assimilate content into concepts.


29
00:01:33,827 --> 00:01:37,097 line:-2
Now, what does on-device NLP
at Apple look like?


30
00:01:37,798 --> 00:01:44,404 line:-2
Until 2017, the primary modality
where NLP was exposed at Apple


31
00:01:44,471 --> 00:01:48,275 line:-2
was through the NSLinguisticTagger class
in Foundation.


32
00:01:48,342 --> 00:01:50,444 line:-2
Now, this provides
fundamental text processing


33
00:01:50,511 --> 00:01:54,781 line:-2
such as language identification,
tokenization, and so on.


34
00:01:54,848 --> 00:01:58,685 line:-2
In 2018, we introduced
the Natural Language framework.


35
00:01:58,752 --> 00:02:00,187 line:-1
The Natural Language framework


36
00:02:00,254 --> 00:02:03,457 line:-2
provides everything
that NSLinguisticTagger can,


37
00:02:03,524 --> 00:02:07,094 line:-2
and on top of it, we started focusing on
state-of-the-art machine-learning


38
00:02:07,160 --> 00:02:11,732 line:-2
and modern NLP techniques
such as text embeddings and custom models.


39
00:02:11,798 --> 00:02:13,700 line:-1
Not only that, we also started


40
00:02:13,767 --> 00:02:15,903 line:-2
tightly integrating
Natural Language framework


41
00:02:15,969 --> 00:02:18,872 line:-2
with the rest of
the machine-learning ecosystem at Apple


42
00:02:18,939 --> 00:02:21,742 line:-2
through tight integration
with Create ML and Core ML.


43
00:02:22,843 --> 00:02:24,978 line:-2
Now, before we jump into
the rest of the session,


44
00:02:25,045 --> 00:02:27,214 line:-2
we'd like to tell you
that NSLinguisticTagger


45
00:02:27,281 --> 00:02:28,782 line:-1
has been marked for deprecation.


46
00:02:28,849 --> 00:02:32,719 line:-2
We strongly encourage you
to move towards Natural Language


47
00:02:32,786 --> 00:02:34,821 line:-1
for all your language-processing needs.


48
00:02:34,888 --> 00:02:37,724 line:-2
Now, if you look
at the kinds of functionalities


49
00:02:37,791 --> 00:02:39,760 line:-2
provided in
the Natural Language framework,


50
00:02:39,826 --> 00:02:43,330 line:-2
they can be broadly broken down
into three different categories.


51
00:02:43,397 --> 00:02:46,934 line:-2
The first is in the area
of fundamental text processing.


52
00:02:47,000 --> 00:02:50,037 line:-2
The second
is in the realm of text embeddings.


53
00:02:50,103 --> 00:02:52,606 line:-2
And the third is in the area
of custom models.


54
00:02:53,707 --> 00:02:56,043 line:-2
So, let's begin with
fundamental text processing.


55
00:02:56,877 --> 00:02:58,145 line:-1
Natural Language framework


56
00:02:58,212 --> 00:03:01,215 line:-2
provides several
basic fundamental building blocks,


57
00:03:01,281 --> 00:03:03,951 line:-2
such as language identification,
tokenization,


58
00:03:04,017 --> 00:03:07,688 line:-2
part of speech tagging, lemmatization,
and named entity recognition.


59
00:03:08,589 --> 00:03:12,292 line:-2
And we provide these APIs
across a wide variety of languages.


60
00:03:12,359 --> 00:03:14,161 line:-1
For more information about these APIs,


61
00:03:14,228 --> 00:03:18,465 line:-2
you can refer to
our 2018 and 2019 WWDC sessions.


62
00:03:18,532 --> 00:03:22,703 line:-2
But at the high level, all of these APIs
operate on a piece of text,


63
00:03:22,769 --> 00:03:26,507 line:-2
and what they give as an output
is a hypothesis or a prediction.


64
00:03:26,573 --> 00:03:30,544 line:-2
However, they did not tell us
a notion of confidence


65
00:03:30,611 --> 00:03:32,346 line:-1
associated with this prediction.


66
00:03:32,412 --> 00:03:36,650 line:-2
And this year, we have a brand-new API
called confidence scores.


67
00:03:36,717 --> 00:03:39,686 line:-2
So, this builds on top
of the existing functionality,


68
00:03:39,753 --> 00:03:42,689 line:-2
and in addition to the hypothesis
or the predicted labels,


69
00:03:42,756 --> 00:03:45,959 line:-2
you can also get the confidence scores
using the APIs.


70
00:03:46,760 --> 00:03:48,862 line:-1
Let's see how we can use this.


71
00:03:49,396 --> 00:03:52,599 line:-2
We start off by creating
an instance of NLTagger


72
00:03:52,666 --> 00:03:55,302 line:-1
and specify the tagScheme to be .nameType.


73
00:03:55,369 --> 00:03:57,938 line:-2
This is something
that you've been used to so far.


74
00:03:58,005 --> 00:04:02,242 line:-2
Now we have a brand-new API
called tagHypotheses.


75
00:04:02,309 --> 00:04:06,013 line:-2
So, when you use tagHypotheses,
in addition to getting the predictions


76
00:04:06,079 --> 00:04:08,348 line:-2
either at the sentence level
or the token level,


77
00:04:08,415 --> 00:04:11,785 line:-2
you also get a confidence score
associated with that prediction.


78
00:04:11,852 --> 00:04:14,521 line:-2
Let's look at how to use
these confidence scores


79
00:04:14,588 --> 00:04:17,057 line:-2
through the lens
of a hypothetical app called Buzz.


80
00:04:17,124 --> 00:04:19,560 line:-1
Buzz is a news reader app.


81
00:04:19,625 --> 00:04:21,261 line:-1
As part of this application,


82
00:04:21,327 --> 00:04:23,931 line:-2
you can browse articles,
you can bookmark them,


83
00:04:23,997 --> 00:04:27,067 line:-2
and you can organize them
so that you can read them later.


84
00:04:27,134 --> 00:04:31,305 line:-2
And what we would like to do
is add a new feature to this application


85
00:04:31,371 --> 00:04:34,942 line:-2
wherein we extract recent entities
from the articles that you've read.


86
00:04:35,008 --> 00:04:38,078 line:-2
So, we want to populate these entities
on the right-side pane,


87
00:04:38,145 --> 00:04:40,080 line:-1
and when you click on an entity,


88
00:04:40,147 --> 00:04:43,150 line:-2
you can be taken back to the article
that you've already read.


89
00:04:43,784 --> 00:04:45,085 line:-1
So, how do we do this?


90
00:04:45,152 --> 00:04:48,155 line:-2
So, we want to use
our named entity recognition API


91
00:04:48,222 --> 00:04:52,492 line:-2
to automatically analyze this text
and extract these named entities,


92
00:04:52,559 --> 00:04:57,164 line:-2
so that we get these named entities
such as Cartagena, and so on and so forth.


93
00:04:57,231 --> 00:05:00,300 line:-2
Now, if you take a close look
at the entities on the right side,


94
00:05:00,367 --> 00:05:03,036 line:-1
you'll see there is a spurious entry.


95
00:05:03,103 --> 00:05:05,772 line:-2
We have something called,
"Do Not Disturb While Driving."


96
00:05:05,839 --> 00:05:09,710 line:-2
So, while the named entity recognition API
gives us person names,


97
00:05:09,776 --> 00:05:12,446 line:-2
organization names,
as well as location names,


98
00:05:12,513 --> 00:05:17,184 line:-2
this seems like a false positive
from this machine-learning API.


99
00:05:17,251 --> 00:05:18,819 line:-1
So, how do we fix this?


100
00:05:18,886 --> 00:05:20,787 line:-1
Suppose we had an input sentence such as,


101
00:05:20,854 --> 00:05:24,224 line:-2
"He was driving with
Do Not Disturb While Driving turned on."


102
00:05:24,291 --> 00:05:28,028 line:-2
When we pass this sentence
through the named entity recognition API,


103
00:05:28,095 --> 00:05:31,098 line:-2
what it does is
it analyzes the sequence of tokens


104
00:05:31,164 --> 00:05:34,334 line:-2
and produces a span of tokens
as an organization name.


105
00:05:34,401 --> 00:05:38,071 line:0
Now, this hypothesis is incorrect
in this machine-learning model.


106
00:05:38,138 --> 00:05:40,841 line:0
Now, armed with the power
of confidence scores,


107
00:05:40,908 --> 00:05:44,411 line:0
you can also get the confidence scores
for each of these labels.


108
00:05:44,478 --> 00:05:47,214 line:0
As you can see,
the confidence score is pretty low.


109
00:05:47,281 --> 00:05:51,585 line:0
By setting a threshold of,
for instance, 0.8 for organization names,


110
00:05:51,652 --> 00:05:54,188 line:0
you can easily filter out
this false positive.


111
00:05:55,422 --> 00:05:59,526 line:-2
With this, if you now go back to the app
and incorporate this in your application,


112
00:05:59,593 --> 00:06:01,895 line:-2
you can easily filter out
the false positive,


113
00:06:01,962 --> 00:06:05,065 line:-2
and you have a much better
and enhanced user experience.


114
00:06:05,132 --> 00:06:08,635 line:-2
We do have a few recommendations
in terms of best practices.


115
00:06:09,837 --> 00:06:11,572 line:-1
First, we'd like to recommend


116
00:06:11,638 --> 00:06:15,008 line:-2
that you avoid heuristic hard coding
of these threshold values


117
00:06:15,075 --> 00:06:17,678 line:-1
and calibrate it on representative data


118
00:06:17,744 --> 00:06:20,647 line:-2
that is pertinent to your app
and domain of operation.


119
00:06:21,381 --> 00:06:26,486 line:-2
We'd also recommend that you consider
creating thresholds on a per-class basis.


120
00:06:26,553 --> 00:06:28,288 line:-1
Rather than setting a global threshold


121
00:06:28,355 --> 00:06:31,191 line:-2
for all the classes
in a particular application,


122
00:06:31,258 --> 00:06:34,895 line:-2
you can set it on a per-class basis
so that you get finer control


123
00:06:34,962 --> 00:06:38,031 line:-2
of false positives versus false negatives
in your app.


124
00:06:38,098 --> 00:06:41,802 line:-2
Now let's move on and shift our attention
to text embeddings.


125
00:06:43,003 --> 00:06:44,838 line:-1
Text embeddings are really important.


126
00:06:44,905 --> 00:06:49,009 line:-2
In fact, they have been the cornerstone
of recent advances in modern NLP.


127
00:06:49,076 --> 00:06:50,911 line:-1
To really understand text embeddings,


128
00:06:50,978 --> 00:06:53,847 line:-2
let's begin with the notion
of a text corpus.


129
00:06:53,914 --> 00:06:55,816 line:-1
What is a text corpus?


130
00:06:55,883 --> 00:06:58,652 line:-1
A text corpus is a collection of documents


131
00:06:58,719 --> 00:07:02,856 line:-2
which are comprised of paragraphs,
sentences, phrases, and words,


132
00:07:02,923 --> 00:07:06,326 line:-2
and in conventional NLP,
when we start with the text corpus,


133
00:07:06,393 --> 00:07:09,830 line:-2
the first thing that we do
is to tokenize this corpus.


134
00:07:09,897 --> 00:07:11,832 line:-1
When we tokenize a text corpus,


135
00:07:11,899 --> 00:07:15,936 line:-2
what we get is an inventory
of words in this corpus.


136
00:07:16,003 --> 00:07:17,337 line:-1
And this inventory of words


137
00:07:17,404 --> 00:07:20,307 line:-2
can be thought of
as a bag-of-words representation


138
00:07:20,374 --> 00:07:22,943 line:-1
where each word is independent.


139
00:07:23,010 --> 00:07:26,380 line:-2
Now, if you were to look at this
from a machine representation standpoint,


140
00:07:26,446 --> 00:07:28,782 line:-1
it is also called as one-hot encoding.


141
00:07:29,750 --> 00:07:32,019 line:-2
So, in this example,
we have a bunch of words:


142
00:07:32,085 --> 00:07:36,190 line:-2
food, burger, pizza,
automobile, bus, and car.


143
00:07:36,256 --> 00:07:39,426 line:-2
And we've gone over the corpus
and extracted these words.


144
00:07:39,493 --> 00:07:43,463 line:-2
Now, each word here
is represented by a bit vector


145
00:07:43,530 --> 00:07:46,900 line:-2
which has one bit on
and the rest of the bits off.


146
00:07:46,967 --> 00:07:48,535 line:-1
And the length of this vector


147
00:07:48,602 --> 00:07:52,005 line:-2
is the same as
the number of unique words in your corpus.


148
00:07:52,072 --> 00:07:56,476 line:-2
Now, as humans, we can see that food,
burger, and pizza are related concepts,


149
00:07:56,543 --> 00:08:00,447 line:-2
and similarly, automobile, car, and bus
are also related concepts.


150
00:08:00,514 --> 00:08:03,817 line:-2
However, if you just look
at this bit vector representation,


151
00:08:03,884 --> 00:08:08,422 line:-2
it doesn't provide any information about
the similarity or dissimilarity of words.


152
00:08:08,488 --> 00:08:10,858 line:-2
So, wouldn't it be great
if we had a representation


153
00:08:10,924 --> 00:08:15,128 line:-2
that also incorporated the information
about similarities of words?


154
00:08:15,195 --> 00:08:18,031 line:-2
And this is really where
word embeddings come in.


155
00:08:19,066 --> 00:08:22,135 line:-2
When you use word embeddings,
again, you start with a text corpus,


156
00:08:22,202 --> 00:08:27,241 line:-2
and what you get as an output
is a vector representation of words,


157
00:08:27,307 --> 00:08:30,210 line:-2
wherein words that are similar
are clustered together,


158
00:08:30,277 --> 00:08:33,313 line:-2
and words that are dissimilar
are clustered away.


159
00:08:33,380 --> 00:08:34,414 line:-1
So, in this example,


160
00:08:34,481 --> 00:08:38,352 line:-2
you can see that burger, pizza, and food
are clustered together,


161
00:08:38,418 --> 00:08:41,722 line:-2
and away from the concepts
of automobile, car, and bus.


162
00:08:41,788 --> 00:08:43,590 line:-1
Now, to obtain these word embeddings,


163
00:08:43,657 --> 00:08:46,193 line:-2
you can use different sorts
of machine-learning algorithms


164
00:08:46,260 --> 00:08:49,229 line:-2
which could be linear models
or nonlinear models,


165
00:08:49,296 --> 00:08:52,533 line:-2
but at a high level,
they capture this vector representation


166
00:08:52,599 --> 00:08:56,336 line:-2
by analyzing global co-occurrences
of words in the text corpus.


167
00:08:56,403 --> 00:09:00,541 line:-2
If you consider this and look at it
from a machine representation standpoint,


168
00:09:00,607 --> 00:09:03,277 line:-2
now the representation is different
from one-hot encoding.


169
00:09:03,343 --> 00:09:07,147 line:0
Each word gets a real-valued vector
of D dimensions,


170
00:09:07,214 --> 00:09:09,316 line:0
or you can think of it as D columns.


171
00:09:09,383 --> 00:09:13,153 line:0
And now, if you look at the vector
for food, burger, and pizza,


172
00:09:13,220 --> 00:09:15,522 line:0
they're close to each other
in the vector space.


173
00:09:15,589 --> 00:09:19,860 line:0
Similarly, the vectors for automobile,
car and bus are also close to each other,


174
00:09:19,927 --> 00:09:21,795 line:0
but far away from the food concepts.


175
00:09:22,262 --> 00:09:24,531 line:-1
Now that we've understood word embeddings,


176
00:09:24,598 --> 00:09:27,334 line:-2
let's look at
the different types of word embeddings.


177
00:09:27,868 --> 00:09:30,671 line:-2
The first is called
static word embeddings.


178
00:09:30,737 --> 00:09:32,139 line:-1
Let's understand this concept.


179
00:09:32,806 --> 00:09:37,477 line:-2
Suppose we had an input sentence,
"I want a burger from a fast food joint."


180
00:09:37,544 --> 00:09:40,881 line:-2
And we want to extract the word embedding
for the word "food".


181
00:09:42,282 --> 00:09:43,717 line:-1
For the case of static embeddings,


182
00:09:43,784 --> 00:09:46,687 line:-2
what we do is,
for all the words in the vocabulary,


183
00:09:46,753 --> 00:09:50,824 line:-2
we pre-compute the embeddings
and store it as a lookup table.


184
00:09:50,891 --> 00:09:52,860 line:0
Now, this lookup table is pre-computed


185
00:09:52,926 --> 00:09:55,696 line:0
and stored on-device
in an efficient manner.


186
00:09:55,762 --> 00:09:58,265 line:0
So, when we need to look up
the word embedding


187
00:09:58,332 --> 00:10:02,269 line:0
for a particular word such as "food",
we simply go into this lookup table,


188
00:10:02,336 --> 00:10:06,073 line:0
pick the corresponding vector,
and give it as the output.


189
00:10:06,139 --> 00:10:08,842 line:-2
Now, static word embeddings
are really useful.


190
00:10:08,909 --> 00:10:11,245 line:-2
They are very useful
to get the nearest neighbors of words


191
00:10:11,311 --> 00:10:12,579 line:-1
in the vector space,


192
00:10:12,646 --> 00:10:17,484 line:-2
and they're also very useful
as inputs to neural network algorithms.


193
00:10:17,551 --> 00:10:19,419 line:-1
But they do have some shortcomings.


194
00:10:20,020 --> 00:10:23,156 line:-2
Suppose we had an input sentence
such as, "It is food for thought,"


195
00:10:23,223 --> 00:10:24,892 line:-1
where the word "food"


196
00:10:24,958 --> 00:10:29,129 line:-2
is represented in a different connotation
based on the context.


197
00:10:29,196 --> 00:10:31,064 line:0
What happens in static word embeddings is


198
00:10:31,131 --> 00:10:33,967 line:0
you will still pass this
through the lookup table


199
00:10:34,034 --> 00:10:36,570 line:0
and extract the same vector
for the word "food",


200
00:10:37,404 --> 00:10:39,540 line:0
even though we know
that the connotation is different


201
00:10:39,606 --> 00:10:40,874 line:-1
and the context is different.


202
00:10:41,475 --> 00:10:45,279 line:-2
So, even though the semantic connotation
of the word "food" is different


203
00:10:45,345 --> 00:10:47,514 line:-1
because of the context in which it's used,


204
00:10:47,581 --> 00:10:49,917 line:-2
we still get
the same vector representation.


205
00:10:50,951 --> 00:10:52,186 line:-1
So, can we do better?


206
00:10:52,252 --> 00:10:55,489 line:-2
And this is where dynamic word embeddings
come into the picture.


207
00:10:55,556 --> 00:10:58,025 line:-2
So, in dynamic word embeddings,
what we do is


208
00:10:58,091 --> 00:11:01,495 line:-2
we pass every sentence
through a neural network,


209
00:11:01,562 --> 00:11:05,933 line:-2
and what we get is a dynamic embedding
for every word in that sequence


210
00:11:05,999 --> 00:11:07,668 line:-1
which is completely contextual.


211
00:11:08,969 --> 00:11:11,972 line:-2
So, if we pass these two sentences
through dynamic word embeddings,


212
00:11:12,039 --> 00:11:13,740 line:-1
which can be a neural network


213
00:11:13,807 --> 00:11:16,977 line:-2
such as a transformer network
or an ELMo-style model,


214
00:11:17,044 --> 00:11:20,581 line:-2
what we get as an output
is one vector for each word


215
00:11:20,647 --> 00:11:22,683 line:-1
that is different based on the context.


216
00:11:22,749 --> 00:11:23,750 line:-1
So, the word "food"


217
00:11:23,817 --> 00:11:26,220 line:-2
now gets completely different
vector representations


218
00:11:26,286 --> 00:11:30,290 line:-2
because the context of food
in these two sentences is different.


219
00:11:30,357 --> 00:11:32,693 line:-2
Now, on the OS,
we support static embeddings


220
00:11:32,759 --> 00:11:37,397 line:-2
in a variety of languages,
also across different Apple platforms.


221
00:11:37,464 --> 00:11:39,433 line:0
For more information
about static word embeddings,


222
00:11:39,499 --> 00:11:42,803 line:0
you can refer to our 2019 WWDC session.


223
00:11:42,870 --> 00:11:44,771 line:-1
In addition to static word embeddings,


224
00:11:44,838 --> 00:11:47,774 line:-2
we also support
what we call custom word embeddings,


225
00:11:47,841 --> 00:11:51,712 line:-2
wherein you can train your own embeddings
using a third-party toolkit


226
00:11:51,778 --> 00:11:54,581 line:-1
such as fasttext, word2vec, GloVe,


227
00:11:54,648 --> 00:11:58,585 line:-2
or perhaps even a custom neural network
in TensorFlow or PyTorch.


228
00:11:58,652 --> 00:12:02,689 line:-2
Once you do this, you can bring
these embeddings onto Apple platforms,


229
00:12:02,756 --> 00:12:06,460 line:-2
compress them, and store them,
and use them in an efficient way.


230
00:12:07,261 --> 00:12:10,230 line:-2
Once you convert them
to a representation on-device,


231
00:12:10,297 --> 00:12:14,034 line:-2
you can use them just the same way
as static word embeddings.


232
00:12:14,101 --> 00:12:18,172 line:-2
Now, in order to use word embeddings,
let's look at how you use it.


233
00:12:18,839 --> 00:12:22,543 line:-2
So, you create an instance
of NLEmbedding.wordEmbedding,


234
00:12:22,609 --> 00:12:25,479 line:-2
and you specify the language,
and once you have this,


235
00:12:25,546 --> 00:12:28,482 line:-2
you can perform
three different operations.


236
00:12:28,549 --> 00:12:30,918 line:-1
The first is, given a word,


237
00:12:30,984 --> 00:12:33,620 line:-2
you can get the vector representation
of the word.


238
00:12:33,687 --> 00:12:35,923 line:-1
The second is, given two words,


239
00:12:35,989 --> 00:12:40,060 line:-2
you can get the distance between
these two words in the vector space.


240
00:12:40,127 --> 00:12:42,629 line:-1
And the third is, given a word,


241
00:12:42,696 --> 00:12:45,832 line:-2
you can find the nearest neighbors
of the word in the vector space.


242
00:12:47,100 --> 00:12:51,538 line:-2
Let's look at the use of word embeddings
through a hypothetical app called Nosh.


243
00:12:51,605 --> 00:12:54,241 line:-1
Nosh is a food delivery app,


244
00:12:54,308 --> 00:12:58,445 line:-2
and as part of this application,
we have an FAQ section.


245
00:12:58,512 --> 00:13:02,182 line:-2
Now, the user experience in this app,
especially in the FAQ section,


246
00:13:02,249 --> 00:13:03,517 line:-1
is not great.


247
00:13:03,584 --> 00:13:05,552 line:-1
So, if I were to find some information,


248
00:13:05,619 --> 00:13:07,754 line:-2
I have to scroll through
all these questions


249
00:13:07,821 --> 00:13:10,090 line:-2
and look for the question
that I'm interested in,


250
00:13:10,157 --> 00:13:12,092 line:-1
and then for the corresponding answer.


251
00:13:12,926 --> 00:13:16,063 line:-2
So we want to improve this user experience
in the Nosh app


252
00:13:16,129 --> 00:13:18,966 line:-1
by adding an automatic search feature,


253
00:13:19,032 --> 00:13:22,236 line:-2
so that you can type,
or you can speak the query,


254
00:13:22,302 --> 00:13:26,740 line:-2
and we can pick the corresponding question
and show you the relevant answer.


255
00:13:26,807 --> 00:13:29,309 line:-2
How do we build this
using word embeddings?


256
00:13:30,077 --> 00:13:33,247 line:-2
So, one way to build this
is using static word embeddings.


257
00:13:33,313 --> 00:13:37,518 line:-2
Let's say you have an input query,
"Do you deliver to Cupertino?"


258
00:13:37,584 --> 00:13:40,220 line:-2
When you pass it
through the word embeddings API,


259
00:13:40,287 --> 00:13:42,322 line:-1
you can enumerate every word


260
00:13:42,389 --> 00:13:46,226 line:-2
and get one-vector representation
for each word in the sequence.


261
00:13:47,094 --> 00:13:51,431 line:-2
Once you do that, a heuristic way
of getting a sentence representation


262
00:13:51,498 --> 00:13:55,335 line:-2
is to simply take the average
of the vectors of every word.


263
00:13:55,402 --> 00:14:00,174 line:-2
And what you'd get as an output
is one vector of the same dimension.


264
00:14:00,240 --> 00:14:02,910 line:-2
Now, you can also pre-compute
the word embeddings


265
00:14:02,976 --> 00:14:06,079 line:-2
for every single FAQ question
in your database.


266
00:14:06,146 --> 00:14:09,550 line:-2
So you would take every question,
run it through word embeddings,


267
00:14:09,616 --> 00:14:13,187 line:-2
get the vectors, average them,
and pre-compute the embeddings.


268
00:14:13,253 --> 00:14:15,355 line:-1
So at runtime, given a query,


269
00:14:15,422 --> 00:14:19,059 line:-2
you find the question
that is closest to the input query vector,


270
00:14:19,126 --> 00:14:20,861 line:-1
and you pick the question,


271
00:14:20,928 --> 00:14:23,697 line:-2
and show the corresponding answer
in the UI.


272
00:14:23,764 --> 00:14:27,167 line:-2
Now, this seems like
a reasonable way of solving this problem,


273
00:14:27,234 --> 00:14:29,236 line:-1
but it does have several shortcomings.


274
00:14:29,303 --> 00:14:32,272 line:-1
The first is the issue with word coverage.


275
00:14:32,339 --> 00:14:35,442 line:-2
Since static word embeddings
work with a finite vocabulary,


276
00:14:35,509 --> 00:14:39,012 line:-2
if you have an input query that does not
have a word in the lookup table,


277
00:14:39,079 --> 00:14:40,981 line:-1
you will lose information.


278
00:14:41,048 --> 00:14:45,886 line:-2
The second is using this averaging process
is very noisy.


279
00:14:45,953 --> 00:14:50,424 line:-2
It's akin to a bag-of-words representation
that loses compositional knowledge.


280
00:14:50,490 --> 00:14:52,326 line:-1
For instance, if we had a query such as,


281
00:14:52,392 --> 00:14:55,662 line:-2
"Do you deliver from Cupertino
to San Jose?"


282
00:14:55,729 --> 00:14:59,299 line:-2
by simply taking the average,
we are jumbling up the words,


283
00:14:59,366 --> 00:15:01,235 line:-1
and we lose the compositional information


284
00:15:01,301 --> 00:15:03,670 line:-2
contained in words
such as "from" and "to".


285
00:15:04,238 --> 00:15:06,473 line:-1
So, the big question is: can we do better?


286
00:15:06,540 --> 00:15:08,542 line:-1
And yes, we certainly can.


287
00:15:08,609 --> 00:15:11,945 line:-2
And we are delighted to tell you
that we have a brand-new technology


288
00:15:12,012 --> 00:15:14,781 line:-2
called sentence embedding
that solves this problem.


289
00:15:15,282 --> 00:15:18,952 line:-2
Now, by using sentence embedding API,
when you pass an input query,


290
00:15:19,019 --> 00:15:21,822 line:-2
or a sentence such as,
"Do you deliver to Cupertino?"


291
00:15:21,889 --> 00:15:26,126 line:-2
it analyzes this entire sentence
and encodes this information


292
00:15:26,193 --> 00:15:28,529 line:-1
into a finite-dimensional vector.


293
00:15:28,595 --> 00:15:32,633 line:-2
In the current API, the dimension
of this vector is 512 dimensions.


294
00:15:34,334 --> 00:15:35,736 line:-1
So, how does this work?


295
00:15:35,802 --> 00:15:39,973 line:-2
Intuitively, you can think of it
as starting from a text corpus,


296
00:15:40,040 --> 00:15:41,608 line:-1
and in the text corpus,


297
00:15:41,675 --> 00:15:45,279 line:-2
if you were to tokenize the text
at the sentence level,


298
00:15:45,345 --> 00:15:47,381 line:-2
when you pass it
through the sentence embedding,


299
00:15:47,447 --> 00:15:51,318 line:-2
instead of working with words,
now you have sentence representations.


300
00:15:51,385 --> 00:15:55,556 line:-2
Each of these sentences are represented
in this vector space in such a way


301
00:15:55,622 --> 00:15:57,925 line:-2
that sentences
that are conceptually similar


302
00:15:57,991 --> 00:15:59,359 line:-1
are clustered together,


303
00:15:59,426 --> 00:16:02,696 line:-2
and sentences that are dissimilar
are clustered away from each other.


304
00:16:03,864 --> 00:16:07,334 line:-2
Now, the technology under this
is fairly complex


305
00:16:07,401 --> 00:16:10,204 line:-2
and utilizes
several machine-learning techniques,


306
00:16:10,270 --> 00:16:12,039 line:-1
one of which is pre-trained models


307
00:16:12,105 --> 00:16:15,576 line:-2
in conjunction with custom layers
such as bidirectional LSTM,


308
00:16:15,642 --> 00:16:17,244 line:-1
as well as fully-connected layers.


309
00:16:17,311 --> 00:16:20,380 line:-2
And we train this network
in a multitask training setup


310
00:16:20,447 --> 00:16:23,183 line:-2
on different tasks
such as natural language inference,


311
00:16:23,250 --> 00:16:26,787 line:-2
binary text similarity,
as well as next sentence prediction.


312
00:16:26,854 --> 00:16:29,656 line:-2
But to use it, you don't have to
worry about these details,


313
00:16:29,723 --> 00:16:31,625 line:-1
you simply have to ask for it.


314
00:16:31,692 --> 00:16:35,662 line:-2
So, you start
by importing NaturalLanguage,


315
00:16:35,729 --> 00:16:38,532 line:-2
and you create an instance
of NLEmbedding.sentenceEmbedding


316
00:16:38,599 --> 00:16:40,801 line:-1
and specify the language as English.


317
00:16:42,002 --> 00:16:46,206 line:-2
Once you have this, you can
ask for the vector of an input sentence.


318
00:16:46,273 --> 00:16:49,042 line:-2
When you do this, the sentence is run
through the neural network,


319
00:16:49,109 --> 00:16:53,146 line:-2
and what you get as an output
is a finite 512-dimensional vector


320
00:16:53,213 --> 00:16:55,282 line:-1
that encodes the meaning of this sentence.


321
00:16:57,351 --> 00:17:01,054 line:-2
Given two sentences, you can also find
the distance between these two sentences


322
00:17:01,121 --> 00:17:02,856 line:-1
in the vector space.


323
00:17:02,923 --> 00:17:05,692 line:-2
You simply run these two sentences
underneath this API


324
00:17:05,759 --> 00:17:08,561 line:-2
through the neural network,
get the vector representation,


325
00:17:08,628 --> 00:17:10,297 line:-1
and then compute the distance.


326
00:17:10,364 --> 00:17:13,567 line:-2
Now, there are a wide variety
of other potential applications


327
00:17:13,634 --> 00:17:15,402 line:-1
for this technology.


328
00:17:15,469 --> 00:17:19,339 line:-2
But since we don't have
a finite list of sentences right now,


329
00:17:19,406 --> 00:17:23,109 line:-2
and you cannot pre-compute the embedding
for all the sentences a priori,


330
00:17:23,176 --> 00:17:27,214 line:-2
there is no nearest neighbors API
available for this technology.


331
00:17:27,281 --> 00:17:29,449 line:-2
But later in the session,
Doug will tell you


332
00:17:29,516 --> 00:17:32,886 line:-2
how you can use sentence embeddings
and do nearest neighbors


333
00:17:32,953 --> 00:17:35,389 line:-1
by leveraging custom embedding technology.


334
00:17:37,191 --> 00:17:40,060 line:-2
Now, if we were to go back
to the Nosh application,


335
00:17:40,127 --> 00:17:43,096 line:-2
when you have a query such as,
"Do you deliver to Cupertino?"


336
00:17:43,163 --> 00:17:46,166 line:-2
you simply pass it
through the sentence embedding API,


337
00:17:46,233 --> 00:17:49,236 line:-2
and you get one vector
that encodes all of the meaning.


338
00:17:50,037 --> 00:17:53,841 line:-2
Similarly, for all of the FAQ questions
in your index,


339
00:17:53,907 --> 00:17:56,243 line:-2
you can pre-compute
the sentence embeddings.


340
00:17:56,310 --> 00:18:00,681 line:-2
And at runtime, given an input,
you simply find the closest question.


341
00:18:01,548 --> 00:18:02,549 line:-1
And once you do this,


342
00:18:02,616 --> 00:18:06,353 line:-2
you show the relevant answer
in the application at the UI level.


343
00:18:07,354 --> 00:18:10,557 line:-2
To see the sentence embeddings in action,
I'm going to hand it over to Doug,


344
00:18:10,624 --> 00:18:14,461 line:-2
who's going to show us a demo
of this working in the Nosh application.


345
00:18:14,528 --> 00:18:15,696 line:-1
Over to you, Doug.


346
00:18:15,762 --> 00:18:19,366 line:-2
Thanks, Vivek.
So, let's see some of this in action.


347
00:18:19,433 --> 00:18:20,968 line:-1
In our Nosh application,


348
00:18:21,034 --> 00:18:24,505 line:-2
what we're going to do is
to let the user type in a query string,


349
00:18:24,571 --> 00:18:27,474 line:-2
and then we're going to return
an appropriate answer


350
00:18:27,541 --> 00:18:31,478 line:-2
from our frequently asked questions
using sentence embeddings.


351
00:18:31,545 --> 00:18:32,913 line:-1
So, let's look at some code.


352
00:18:33,480 --> 00:18:35,582 line:-2
So, the first thing we're going to do
in this method


353
00:18:35,649 --> 00:18:40,554 line:-2
is to just get a sentenceEmbedding,
in this case, for English. Very simple.


354
00:18:40,621 --> 00:18:42,689 line:-1
And then we'll ask that embedding


355
00:18:42,756 --> 00:18:45,158 line:-2
for the vector
for the user's query string.


356
00:18:45,225 --> 00:18:47,961 line:-2
When we first constructed
this application,


357
00:18:48,028 --> 00:18:51,331 line:-1
we took each of our answers


358
00:18:51,398 --> 00:18:55,035 line:-2
and constructed for it
two or three example queries,


359
00:18:55,102 --> 00:18:57,804 line:-2
and pre-calculated
the sentence embedding vectors


360
00:18:57,871 --> 00:19:00,107 line:-1
for each one and put them in a table.


361
00:19:00,174 --> 00:19:03,710 line:-2
So what we're going to do
is just iterate through that table.


362
00:19:03,777 --> 00:19:08,849 line:-2
We'll go through... For each key,
we have two or three vectors


363
00:19:08,916 --> 00:19:11,351 line:-1
representing these example queries.


364
00:19:11,418 --> 00:19:16,190 line:-2
We'll find the distance between
each of those and our query vector,


365
00:19:16,256 --> 00:19:19,760 line:-2
and the smallest distance
is our nearest neighbor,


366
00:19:19,826 --> 00:19:22,262 line:-2
which represents the answer
that we're going to show,


367
00:19:22,329 --> 00:19:23,730 line:-1
and we return that answer.


368
00:19:24,331 --> 00:19:26,166 line:-1
So, let's try it out.


369
00:19:27,734 --> 00:19:30,304 line:-1
So, if the user types, for example,


370
00:19:30,871 --> 00:19:36,243 line:-1
"How do I use it?"


371
00:19:36,310 --> 00:19:40,414 line:-2
then we can search,
and go through and find a nearest neighbor


372
00:19:40,480 --> 00:19:43,750 line:-2
and point them to
the "How does it work?" section


373
00:19:43,817 --> 00:19:45,252 line:-1
of our frequently asked questions.


374
00:19:45,319 --> 00:19:51,625 line:-2
Or maybe they ask, uh,
"Where do you deliver?"


375
00:19:52,426 --> 00:19:55,329 line:-2
And then we'll search
and find a nearest neighbor,


376
00:19:55,395 --> 00:19:57,731 line:-2
and point them
to the "Delivery area" section


377
00:19:57,798 --> 00:19:59,132 line:-1
of our frequently asked questions.


378
00:19:59,199 --> 00:20:05,639 line:-2
Or maybe the user wants to know,
"Where is my order?"


379
00:20:06,573 --> 00:20:08,876 line:-1
In which case, we search,


380
00:20:08,942 --> 00:20:12,112 line:-2
and we can point them
directly to the "Order status" section


381
00:20:12,179 --> 00:20:13,981 line:-1
of our frequently asked questions.


382
00:20:14,047 --> 00:20:16,850 line:-2
Now, there are
many other possible uses for this.


383
00:20:16,917 --> 00:20:22,155 line:-2
Let's consider another hypothetical
sample application called Verse,


384
00:20:22,222 --> 00:20:26,326 line:-2
and Verse is an application
for showing poetry.


385
00:20:26,393 --> 00:20:29,229 line:-2
So, Verse has
many, many different poems in it.


386
00:20:29,296 --> 00:20:32,065 line:-1
And one obvious UI for this


387
00:20:32,132 --> 00:20:34,701 line:-2
is that we could just have
a long list of the poems


388
00:20:34,768 --> 00:20:37,771 line:-2
where the user picks one,
and then the user sees that poem,


389
00:20:37,838 --> 00:20:39,373 line:-1
and that's fine.


390
00:20:39,439 --> 00:20:42,709 line:-2
But wouldn't it be nice
to have some additional ways


391
00:20:42,776 --> 00:20:44,711 line:-1
of looking for these poems?


392
00:20:44,778 --> 00:20:49,850 line:-2
For example, suppose that I type in,
"You're beautiful."


393
00:20:50,517 --> 00:20:54,788 line:-2
Well, then we can find out
that Shakespeare said it better,


394
00:20:54,855 --> 00:20:58,091 line:-2
and we can do this
using sentence embeddings.


395
00:20:58,158 --> 00:21:02,462 line:-2
So, what we can do
is take each line of each poem


396
00:21:02,529 --> 00:21:06,567 line:-2
and calculate the sentence embedding
vector for that line


397
00:21:06,633 --> 00:21:08,302 line:-1
and then put them in a table,


398
00:21:08,368 --> 00:21:12,673 line:-2
and then iterate through them
just as we did in the Nosh application.


399
00:21:12,739 --> 00:21:14,908 line:-1
But there's one twist here,


400
00:21:14,975 --> 00:21:19,780 line:-2
and that is that we have hundreds of poems
and thousands of lines.


401
00:21:19,847 --> 00:21:23,584 line:-2
So it may be
that the simple linear search and table


402
00:21:23,650 --> 00:21:26,687 line:-2
that we used in the Nosh app
isn't efficient enough,


403
00:21:26,753 --> 00:21:28,021 line:-1
and we have a solution to that.


404
00:21:28,088 --> 00:21:32,192 line:-2
And the solution
is to make use of custom embeddings.


405
00:21:32,259 --> 00:21:35,796 line:-2
What do we need in order to create
a custom embedding?


406
00:21:35,863 --> 00:21:39,700 line:-2
We need a dictionary.
The keys in the dictionary are arbitrary.


407
00:21:39,766 --> 00:21:42,469 line:-2
So I've chosen them here to be,
for example,


408
00:21:42,536 --> 00:21:45,239 line:-1
poem_1_line_1, poem_1_line_2,


409
00:21:45,305 --> 00:21:48,375 line:-2
to be strings from which
we can readily determine


410
00:21:48,442 --> 00:21:52,079 line:-2
which poem we were looking at,
and which line.


411
00:21:52,145 --> 00:21:55,415 line:-2
And then the values are just these vectors
that we got for each line.


412
00:21:56,083 --> 00:21:58,719 line:-2
And from that,
we can produce a custom embedding,


413
00:21:58,785 --> 00:22:01,688 line:-2
and the custom embedding
has two important properties.


414
00:22:01,755 --> 00:22:07,661 line:-2
First, it gives a very space-efficient
representation of that dictionary,


415
00:22:07,728 --> 00:22:10,864 line:-2
and second, it has geometric information
that we can use


416
00:22:10,931 --> 00:22:12,966 line:-1
to do efficient nearest-neighbor search


417
00:22:13,033 --> 00:22:16,236 line:-2
without having to go through
the entire thing.


418
00:22:16,837 --> 00:22:21,375 line:-2
And now to create one of these
custom embeddings, it's very simple.


419
00:22:21,441 --> 00:22:23,343 line:-1
You can do this in Create ML,


420
00:22:23,410 --> 00:22:27,581 line:-2
and then all you do
is to take that dictionary


421
00:22:27,648 --> 00:22:29,816 line:-1
and pass it into Create ML.


422
00:22:29,883 --> 00:22:36,089 line:-2
And what comes out is a Core ML model
that represents that custom embedding.


423
00:22:36,423 --> 00:22:38,926 line:-1
So, let's take a look at this in action.


424
00:22:38,992 --> 00:22:42,029 line:-2
Let's take a look at some code
in our Verse application.


425
00:22:42,095 --> 00:22:45,532 line:-2
And here is the corresponding method
in Verse


426
00:22:45,599 --> 00:22:49,837 line:-2
that takes the user's query string
and returns the answer key.


427
00:22:49,903 --> 00:22:54,641 line:-2
So, just as before,
we get the sentence embedding for English,


428
00:22:54,708 --> 00:22:58,545 line:-2
and we get the query vector
for that embedding.


429
00:22:58,612 --> 00:23:00,647 line:-1
But now the rest is even simpler.


430
00:23:00,714 --> 00:23:04,818 line:-2
We just take our custom embedding
and pass it in that query vector


431
00:23:04,885 --> 00:23:08,255 line:-2
and it will directly return to us
the nearest neighbor,


432
00:23:08,322 --> 00:23:11,525 line:-2
and it will return the key
that we put into that dictionary


433
00:23:11,592 --> 00:23:13,560 line:-2
from which we created
the custom embedding.


434
00:23:13,627 --> 00:23:15,896 line:-2
And as I mentioned,
we can easily determine


435
00:23:15,963 --> 00:23:19,233 line:-2
which is the right poem
to return from that key.


436
00:23:19,666 --> 00:23:22,102 line:-1
So, let's try it out.


437
00:23:22,569 --> 00:23:27,441 line:-2
If the user types in something like,
let's say, "I love you,"


438
00:23:27,508 --> 00:23:29,943 line:-1
we can get a poetic expression for that


439
00:23:30,010 --> 00:23:33,847 line:-2
and find a poem
that represents that sentiment.


440
00:23:33,914 --> 00:23:36,316 line:-1
Or maybe they type in something like...


441
00:23:37,618 --> 00:23:42,923 line:-1
"Don't... forget me,"


442
00:23:42,990 --> 00:23:45,592 line:-2
and we can find a poem
that expresses that sentiment.


443
00:23:45,659 --> 00:23:49,463 line:-2
Just about anything we want,
we can find a suitable expression.


444
00:23:49,530 --> 00:23:51,565 line:-1
Maybe it's, uh...


445
00:23:51,632 --> 00:23:56,036 line:-1
"Love... isn't... everything."


446
00:23:57,404 --> 00:23:59,973 line:-1
And here's a poem for that, too, as well.


447
00:24:00,440 --> 00:24:02,476 line:-1
Now, I don't want to give the impression


448
00:24:02,543 --> 00:24:05,112 line:-2
that the only thing you can do
with sentence embeddings


449
00:24:05,179 --> 00:24:07,247 line:-1
is this sort of text retrieval,


450
00:24:07,314 --> 00:24:11,185 line:-2
because sentence embeddings are useful
for all sorts of different applications.


451
00:24:11,251 --> 00:24:15,589 line:-2
For example, consider a hypothetical app
called FindMyShot,


452
00:24:15,656 --> 00:24:18,158 line:-1
which stores images,


453
00:24:18,225 --> 00:24:20,894 line:-2
and happens to have captions
for each of those images.


454
00:24:20,961 --> 00:24:23,430 line:-2
Now, since the image
is associated with captions,


455
00:24:23,497 --> 00:24:27,234 line:-2
I can use sentence embeddings
to find an image


456
00:24:27,301 --> 00:24:31,004 line:-2
based on similarity between
the user's query text and the caption.


457
00:24:32,072 --> 00:24:34,441 line:-2
And there are many other
possible usages for these.


458
00:24:34,775 --> 00:24:37,377 line:-2
You can use them
for detecting paraphrases,


459
00:24:37,444 --> 00:24:42,015 line:-2
you can use them for input
for training more complicated models,


460
00:24:42,082 --> 00:24:44,051 line:-1
and you can use them for clustering.


461
00:24:44,117 --> 00:24:46,186 line:-2
So let me spend a moment
to talk about clustering.


462
00:24:46,553 --> 00:24:51,325 line:-2
If you don't have any prearranged text,
if all the text comes in from the user,


463
00:24:51,391 --> 00:24:53,727 line:-2
then you can still make use
of sentence embeddings.


464
00:24:53,794 --> 00:24:58,265 line:-2
For example, if you had messages,
or maybe reviews,


465
00:24:58,332 --> 00:25:01,702 line:-1
or maybe problem reports from your users,


466
00:25:01,768 --> 00:25:03,704 line:-1
you can take sentence embeddings


467
00:25:03,770 --> 00:25:06,273 line:-2
and calculate a vector
for each one of these.


468
00:25:06,340 --> 00:25:09,543 line:-2
And then you can use
standard clustering algorithms


469
00:25:09,610 --> 00:25:12,412 line:-2
to group these into
as many groups as you want.


470
00:25:12,479 --> 00:25:16,016 line:-1
And what sentence embedding means is that


471
00:25:16,083 --> 00:25:19,753 line:-2
these groups are going to be sentences
that are close together in meaning.


472
00:25:20,787 --> 00:25:23,323 line:-2
The availability
of the sentence embeddings


473
00:25:23,390 --> 00:25:25,392 line:-1
is for a number of different languages,


474
00:25:25,459 --> 00:25:30,931 line:-2
English, Spanish, French, German, Italian,
Portuguese, and Simplified Chinese,


475
00:25:30,998 --> 00:25:34,968 line:-1
and on macOS, iOS and iPadOS.


476
00:25:35,502 --> 00:25:37,604 line:-1
Now, these sentence embeddings


477
00:25:37,671 --> 00:25:41,642 line:-2
are intended for use
on natural language text,


478
00:25:42,075 --> 00:25:45,145 line:-2
um, especially text
that's come in from the user.


479
00:25:45,212 --> 00:25:48,115 line:-2
You don't have to do
a lot of preprocessing on this text.


480
00:25:48,182 --> 00:25:49,950 line:-2
You don't have to remove stop words,
for example,


481
00:25:50,017 --> 00:25:53,654 line:-2
because the sentence embeddings
have seen all this in their training.


482
00:25:53,720 --> 00:25:56,623 line:-2
And they're intended
for being applied to text


483
00:25:56,690 --> 00:25:59,426 line:-2
that is similar in length
to a single sentence,


484
00:25:59,493 --> 00:26:02,162 line:-2
maybe a couple of sentences
or a short paragraph.


485
00:26:02,696 --> 00:26:05,432 line:-2
Um, if you have text
that's longer than that,


486
00:26:05,499 --> 00:26:08,135 line:-1
then you can divide it up into sentences


487
00:26:08,202 --> 00:26:10,504 line:-2
and apply the sentence embeddings
to each one,


488
00:26:10,571 --> 00:26:12,539 line:-1
just as we did with our poems.


489
00:26:13,473 --> 00:26:15,309 line:-1
And, uh...


490
00:26:16,944 --> 00:26:20,714 line:-2
Also, you can make use
of the custom embeddings


491
00:26:20,781 --> 00:26:23,417 line:-2
in case you have large numbers
of the usage you want to store


492
00:26:23,483 --> 00:26:24,484 line:-1
and look through.


493
00:26:25,619 --> 00:26:29,756 line:-2
So, next, I'd like to turn
to the topic of custom models.


494
00:26:32,659 --> 00:26:36,697 line:-2
The idea in custom models is that
you bring in


495
00:26:36,763 --> 00:26:39,800 line:-1
your custom training data,


496
00:26:39,867 --> 00:26:43,170 line:-2
and we train a model for you
for some particular NLP task.


497
00:26:44,371 --> 00:26:48,709 line:-2
Now, there are two broad kinds
of NLP tasks that we support


498
00:26:48,775 --> 00:26:51,044 line:-1
that cover a wide range of functionality.


499
00:26:51,111 --> 00:26:53,514 line:-1
And the first is a text classifier,


500
00:26:53,580 --> 00:26:56,917 line:-2
which the object is
to take a piece of text


501
00:26:56,984 --> 00:26:59,219 line:-1
and supply a label to it.


502
00:26:59,286 --> 00:27:02,155 line:-1
And the other is a word tagger,


503
00:27:02,222 --> 00:27:05,259 line:-2
in which the object is to take
a sequence of words in a sentence


504
00:27:05,325 --> 00:27:07,528 line:-1
and supply a label for each one.


505
00:27:08,562 --> 00:27:12,466 line:-2
The custom model training is exposed
through Create ML.


506
00:27:12,533 --> 00:27:17,237 line:-2
You pass in your training data,
Create ML passes it to Natural Language.


507
00:27:17,304 --> 00:27:21,708 line:-2
Natural Language produces a model,
and what you get out is a Core ML model,


508
00:27:21,775 --> 00:27:26,046 line:-1
either a tagger or a text classifier.


509
00:27:26,113 --> 00:27:28,115 line:-1
And our focus for the last couple of years


510
00:27:28,182 --> 00:27:32,819 line:-2
has been on applying the power
of transfer learning to these models.


511
00:27:32,886 --> 00:27:38,192 line:0
With transfer learning,
the idea is that you can incorporate


512
00:27:38,258 --> 00:27:40,494 line:0
pre-existing knowledge of the language


513
00:27:40,561 --> 00:27:43,397 line:0
so that you don't have to supply
quite so much training data


514
00:27:43,463 --> 00:27:45,299 line:0
in order to produce a good model.


515
00:27:45,365 --> 00:27:50,571 line:-2
And this pre-existing knowledge comes in
by means of word embeddings,


516
00:27:50,637 --> 00:27:52,639 line:-2
because the word embeddings
have been trained


517
00:27:52,706 --> 00:27:55,175 line:-1
on large amounts of natural language text.


518
00:27:55,242 --> 00:28:00,013 line:-2
Now, we introduced this last year
for text classifiers,


519
00:28:00,080 --> 00:28:03,183 line:-2
and that provides
a very powerful solution for many apps.


520
00:28:03,250 --> 00:28:06,520 line:-2
For example, we can consider
a hypothetical app called Merch,


521
00:28:06,587 --> 00:28:10,991 line:-2
which is intended for transactions
between buyers and sellers,


522
00:28:11,058 --> 00:28:14,428 line:-2
and they communicate with each other
about these transactions.


523
00:28:14,494 --> 00:28:17,030 line:-1
But one complaint the users have, perhaps,


524
00:28:17,097 --> 00:28:19,867 line:-1
is that they get sometimes spam messages


525
00:28:19,933 --> 00:28:21,802 line:-2
and they don't want to
have to look at all these.


526
00:28:21,869 --> 00:28:27,307 line:-2
Well, one possible solution is that
you can train a text classifier


527
00:28:27,374 --> 00:28:30,310 line:-2
by bringing in large amounts
of example sentences


528
00:28:30,377 --> 00:28:35,282 line:-2
labeled as spam or not spam,
and then train a text classifier.


529
00:28:35,349 --> 00:28:40,454 line:-2
And a transfer-learning model is actually
very effective for this sort of task.


530
00:28:40,521 --> 00:28:43,657 line:-2
And then the model in your app
will tell you


531
00:28:43,724 --> 00:28:46,260 line:-2
whether a particular message
is likely to be spam,


532
00:28:46,326 --> 00:28:50,531 line:-2
and then you can show it, appropriately,
or not, to the user.


533
00:28:50,931 --> 00:28:53,567 line:-1
But what I really want to talk about today


534
00:28:53,634 --> 00:28:58,505 line:-2
is the application of transfer learning
to word tagging, which is new this year.


535
00:28:58,572 --> 00:29:02,309 line:-2
Now let's go back
and talk about the task of word tagging.


536
00:29:02,376 --> 00:29:06,813 line:-2
As I said, the object is to take
a sequence of words in a sentence


537
00:29:06,880 --> 00:29:08,782 line:-1
and supply a label for each one.


538
00:29:08,849 --> 00:29:11,051 line:-2
And probably
the prototypical task for this


539
00:29:11,118 --> 00:29:14,621 line:-2
is part of speech tagging,
but it can be used for many other things.


540
00:29:14,688 --> 00:29:18,792 line:-2
For example,
you can potentially use word tagging


541
00:29:18,859 --> 00:29:21,895 line:-1
to divide a sentence up into phrases,


542
00:29:21,962 --> 00:29:25,432 line:-2
or, and this is what
we're going to be talking about here,


543
00:29:25,499 --> 00:29:30,971 line:-2
you can take a sentence and extract
important pieces of information from it,


544
00:29:31,038 --> 00:29:32,706 line:-1
even though it's unstructured text.


545
00:29:32,773 --> 00:29:36,076 line:-2
For example, in a travel application,
I might want to know


546
00:29:36,143 --> 00:29:39,847 line:-2
where the user's coming from
and where they're going to.


547
00:29:40,714 --> 00:29:43,784 line:-2
Can we make use of this
in our Nosh application?


548
00:29:43,851 --> 00:29:46,019 line:-1
Well, let's take a look.


549
00:29:46,086 --> 00:29:50,424 line:-2
So, we saw that
with the sentence embedding vectors,


550
00:29:50,490 --> 00:29:54,361 line:-2
we could return general answers
to the users' queries.


551
00:29:54,728 --> 00:29:58,866 line:-2
But there are other things that I might
want to look at in a user sentence.


552
00:29:58,932 --> 00:30:03,170 line:-2
For example, I might want to know
what kind of food they're looking for,


553
00:30:03,237 --> 00:30:05,205 line:-1
or where they want to get it from.


554
00:30:05,272 --> 00:30:09,142 line:-2
And I could potentially label
these parts of the sentence


555
00:30:09,209 --> 00:30:14,314 line:-2
as food,
or a city where the food is coming from.


556
00:30:15,382 --> 00:30:20,153 line:-2
Now, the most obvious and simple way
to handle this sort of problem


557
00:30:20,220 --> 00:30:25,158 line:-2
would be to just list all
the potential foods and potential cities,


558
00:30:25,225 --> 00:30:28,295 line:-2
and then just search through the text
for each of those.


559
00:30:28,362 --> 00:30:30,397 line:-2
And, of course,
we support that sort of thing.


560
00:30:30,464 --> 00:30:33,333 line:-1
We have our NLGazetteer class,


561
00:30:33,400 --> 00:30:38,739 line:-2
which provides an efficient representation
for any number of tables of items


562
00:30:38,805 --> 00:30:40,774 line:-1
that you might want to look for in text.


563
00:30:42,075 --> 00:30:45,445 line:-2
But the problem with this approach
is that, in general,


564
00:30:45,512 --> 00:30:48,949 line:-2
you're not going to be able to list
all the potential values


565
00:30:49,016 --> 00:30:50,884 line:-1
that you might want to look for.


566
00:30:50,951 --> 00:30:55,289 line:-2
So, as soon as you encounter
some piece of text


567
00:30:55,355 --> 00:30:57,891 line:-1
that you hadn't thought of before,


568
00:30:57,958 --> 00:31:01,395 line:-2
then this simple search
is not going to help you.


569
00:31:03,063 --> 00:31:09,002 line:-2
And the other problem with this approach
is that it doesn't take into account


570
00:31:09,069 --> 00:31:11,538 line:-2
anything about the meaning
of words in context,


571
00:31:12,639 --> 00:31:16,877 line:-2
and a word tagger can solve
both of these problems.


572
00:31:18,045 --> 00:31:23,350 line:-2
In addition, it's possible to combine
a word tagger and an NLGazetteer


573
00:31:23,417 --> 00:31:25,118 line:-1
for even greater accuracy.


574
00:31:26,486 --> 00:31:30,824 line:-2
So, suppose I've decided that
I actually want to use a word tagger


575
00:31:30,891 --> 00:31:32,893 line:-1
for my Nosh application.


576
00:31:32,960 --> 00:31:34,328 line:-1
Where do I start?


577
00:31:34,394 --> 00:31:36,063 line:-1
The first thing to do is to decide


578
00:31:36,129 --> 00:31:38,699 line:-2
what pieces of information
I want to get out


579
00:31:38,765 --> 00:31:40,868 line:-1
and assign labels to those.


580
00:31:40,934 --> 00:31:44,905 line:-2
Then I collect
sufficiently many example sentences


581
00:31:44,972 --> 00:31:50,177 line:-2
that the user might enter,
and I decide how I'm going to label them.


582
00:31:50,244 --> 00:31:55,782 line:-2
And then I actually label those sentences
and continue to repeat this process


583
00:31:55,849 --> 00:31:58,619 line:-2
until I have enough data
to train a good model.


584
00:31:58,685 --> 00:32:00,888 line:-1
And I might have to continue repeating it


585
00:32:00,954 --> 00:32:05,959 line:-2
if my model ever runs across a situation
that it doesn't handle adequately.


586
00:32:06,026 --> 00:32:10,397 line:-2
Usually, the solution is to add some more
training data and retrain the model.


587
00:32:11,532 --> 00:32:13,734 line:-1
So, what does our training data look like?


588
00:32:14,368 --> 00:32:15,569 line:-1
Um...


589
00:32:15,636 --> 00:32:18,272 line:-1
So, in our Nosh application,


590
00:32:18,338 --> 00:32:21,942 line:-2
we're going to add some labels
to sentences like this.


591
00:32:22,009 --> 00:32:25,479 line:-2
So we'll use a neutral label,
"O" here, in this case, for "OTHER,"


592
00:32:25,546 --> 00:32:28,815 line:-2
for the pieces of text
that we're not particularly interested in,


593
00:32:28,882 --> 00:32:33,554 line:-2
and we'll use labels like FOOD,
FROM_CITY, RESTAURANT,


594
00:32:33,620 --> 00:32:36,757 line:-2
for the pieces of text
that we are specifically interested in.


595
00:32:36,823 --> 00:32:40,427 line:-2
Now, why did I say "FROM_CITY"
rather than just "CITY"?


596
00:32:40,494 --> 00:32:44,231 line:-2
Because I noticed that
in these example sentences,


597
00:32:44,298 --> 00:32:47,100 line:-2
there are two kinds of ways
where a city can come in.


598
00:32:47,167 --> 00:32:51,672 line:-2
The first is where it's the city
where the restaurant is located,


599
00:32:51,738 --> 00:32:54,107 line:-2
where the food
is supposed to be coming from,


600
00:32:54,174 --> 00:32:56,977 line:-2
and the second is where
it's the city the user's located,


601
00:32:57,044 --> 00:32:59,346 line:-1
where the food is being delivered to.


602
00:32:59,413 --> 00:33:02,883 line:-2
So I'm going to label those differently
as "FROM_CITY" and "TO_CITY."


603
00:33:02,950 --> 00:33:05,786 line:-2
And because the word tagger
can take advantage


604
00:33:05,853 --> 00:33:07,654 line:-1
of the meaning of words in context,


605
00:33:07,721 --> 00:33:09,890 line:-1
it can distinguish between these two,


606
00:33:09,957 --> 00:33:13,427 line:-2
provided I give it
sufficient training data.


607
00:33:13,493 --> 00:33:17,631 line:-2
And here is what the training data
looks like in JSON format,


608
00:33:17,698 --> 00:33:21,001 line:-2
which is very convenient
for use with Create ML.


609
00:33:21,068 --> 00:33:24,271 line:-2
So when I want to go and train a model
on Create ML, it's very simple.


610
00:33:24,338 --> 00:33:27,241 line:-2
If I'm doing it in code,
I just import Create ML.


611
00:33:27,908 --> 00:33:31,678 line:0
And then I ask Create ML
to provide me a model.


612
00:33:31,745 --> 00:33:34,081 line:0
Now, we've supported this
for a couple of years


613
00:33:34,147 --> 00:33:39,186 line:0
using an algorithm known as CRF,
conditional random fields,


614
00:33:39,253 --> 00:33:40,254 line:0
and it works well.


615
00:33:40,587 --> 00:33:43,090 line:-1
But what's new this year is that


616
00:33:43,156 --> 00:33:48,462 line:-2
we are applying the power
of transfer learning to word tagging.


617
00:33:49,096 --> 00:33:53,600 line:-2
And as I said before,
what transfer learning does


618
00:33:53,667 --> 00:33:57,804 line:-2
is to allow us to apply
pre-existing knowledge of the language


619
00:33:57,871 --> 00:34:01,141 line:-2
so that you don't have to supply
quite so much training data


620
00:34:01,208 --> 00:34:02,910 line:-1
in order to train a good model.


621
00:34:02,976 --> 00:34:06,680 line:-2
And the way in which
this knowledge comes in


622
00:34:06,747 --> 00:34:09,283 line:-1
is via dynamic word embeddings.


623
00:34:09,349 --> 00:34:12,219 line:-2
As we said before,
the dynamic word embeddings


624
00:34:12,286 --> 00:34:14,821 line:-2
understand something about
the meaning of words in context,


625
00:34:14,888 --> 00:34:16,757 line:-1
which is just what the word tagger wants.


626
00:34:16,822 --> 00:34:20,527 line:-2
So we use the dynamic word embedding
as an input layer,


627
00:34:20,594 --> 00:34:24,364 line:-2
and on top of it,
we take the data that you provide


628
00:34:24,431 --> 00:34:27,467 line:-1
and we train a multi-layer neural network,


629
00:34:27,534 --> 00:34:31,170 line:-2
and that is the thing
that actually produces the output labels.


630
00:34:31,772 --> 00:34:35,208 line:-2
Now, this sounds complex,
but if you want it,


631
00:34:35,275 --> 00:34:37,543 line:-1
all you have to do is ask for it.


632
00:34:37,610 --> 00:34:41,782 line:-2
So, instead of asking
for the CRF algorithm,


633
00:34:41,849 --> 00:34:45,219 line:-2
you just ask
for a transfer learning algorithm


634
00:34:45,284 --> 00:34:50,257 line:-2
with dynamic word embeddings,
and then it will train a model for you.


635
00:34:50,324 --> 00:34:52,192 line:-1
So let's take a look at that in action.


636
00:34:53,726 --> 00:34:56,096 line:-1
So here's the Nosh application,


637
00:34:56,163 --> 00:35:01,034 line:-2
and here is some of the training data
that I have added for it.


638
00:35:01,101 --> 00:35:06,707 line:-2
And I produced, oh, somewhat over
1,000 sentences of this format,


639
00:35:06,773 --> 00:35:09,543 line:-2
and you'll notice
these are in JSON format.


640
00:35:09,610 --> 00:35:13,881 line:-2
So each example is a parallel sequence
of tokens and labels,


641
00:35:13,947 --> 00:35:15,549 line:-1
one label for each token.


642
00:35:15,616 --> 00:35:20,153 line:-2
And you'll notice that cities are labeled
"FROM_CITY" or "TO_CITY,"


643
00:35:20,220 --> 00:35:23,957 line:-2
and you'll notice that foods are labeled
and restaurants are labeled.


644
00:35:24,024 --> 00:35:27,261 line:-2
And this is the data that I'm going to use
to train my model.


645
00:35:27,327 --> 00:35:31,031 line:-1
And so it's possible to train it in code,


646
00:35:31,098 --> 00:35:34,535 line:-2
but I'm going to train this model
using the Create ML application


647
00:35:34,601 --> 00:35:36,136 line:-1
which makes it very simple.


648
00:35:38,772 --> 00:35:40,541 line:-1
So here's the Create ML application.


649
00:35:40,607 --> 00:35:44,044 line:-2
All I have to do
is point it to my training data,


650
00:35:44,111 --> 00:35:48,248 line:-2
tell it which are the labels and tokens,
and tell it which algorithm I want to use.


651
00:35:48,315 --> 00:35:51,652 line:-2
In this case, we're going to use
the transfer learning algorithm,


652
00:35:51,718 --> 00:35:54,655 line:-1
and this is going to be for English.


653
00:35:54,721 --> 00:35:58,458 line:-2
And that's really
about all there is to it.


654
00:35:58,525 --> 00:36:01,695 line:-1
I just started off and set it to train.


655
00:36:03,063 --> 00:36:05,599 line:-2
And the first thing that it does
is to load all the data,


656
00:36:05,666 --> 00:36:07,601 line:-1
extract features from it,


657
00:36:07,668 --> 00:36:12,739 line:-2
and then it's going to start training
using transfer learning,


658
00:36:12,806 --> 00:36:16,143 line:-1
and it will train a neural network.


659
00:36:16,210 --> 00:36:18,345 line:-1
So, this takes a number of iterations.


660
00:36:18,412 --> 00:36:21,181 line:-2
With each iteration,
it gets more and more accurate.


661
00:36:21,248 --> 00:36:24,818 line:-2
Now, this particular training process
takes two or three minutes,


662
00:36:24,885 --> 00:36:27,554 line:-2
so I'm not going to make you
sit through all of it.


663
00:36:27,621 --> 00:36:31,859 line:-2
I actually have a pre-trained model,
so let's go back...


664
00:36:33,026 --> 00:36:37,397 line:-1
to... the application


665
00:36:37,464 --> 00:36:39,166 line:-1
and take a look at some code.


666
00:36:45,339 --> 00:36:48,175 line:-2
So here is an example method
in the Nosh application


667
00:36:48,242 --> 00:36:50,310 line:-2
that's going to make use
of our trained model.


668
00:36:51,578 --> 00:36:55,816 line:-2
So we're going to be passed in
the user's string that they've typed...


669
00:36:55,883 --> 00:37:00,320 line:-2
First thing we'll do is load our model,
our word tagger model as an NL model,


670
00:37:00,387 --> 00:37:03,524 line:-2
and then what we're going to do here
is use it with an NLTagger.


671
00:37:03,590 --> 00:37:06,159 line:-1
And that's convenient because the NLTagger


672
00:37:06,226 --> 00:37:09,863 line:-2
will take care of all of the tokenization
and application,


673
00:37:09,930 --> 00:37:12,332 line:-1
and just give us the results.


674
00:37:12,399 --> 00:37:15,035 line:-1
So we've created a custom tagScheme.


675
00:37:15,102 --> 00:37:19,806 line:-2
That's just a string constant
that refers to this set of tags.


676
00:37:19,873 --> 00:37:22,843 line:-2
And we'll tell our tagger
that that's what we want to use,


677
00:37:22,910 --> 00:37:26,113 line:-2
and then we tell our tagger
to use our custom model


678
00:37:26,180 --> 00:37:28,348 line:-1
for this custom tag scheme.


679
00:37:28,415 --> 00:37:32,719 line:-2
We attach the user's string to the tagger,
and that's really all there is to it.


680
00:37:32,786 --> 00:37:36,256 line:-2
We can then use the tagger
to go through the words,


681
00:37:36,323 --> 00:37:39,993 line:-2
and it will tell us, for each one,
what it thinks the label should be,


682
00:37:40,060 --> 00:37:44,164 line:-2
whether it's RESTAURANT, or FOOD,
or FROM_CITY, or TO_CITY, or nothing.


683
00:37:44,231 --> 00:37:48,035 line:-2
And then we take note
of those particular pieces


684
00:37:48,101 --> 00:37:49,937 line:-1
of what the user has entered.


685
00:37:50,003 --> 00:37:53,307 line:-2
And then we can use that
according to the needs of the application


686
00:37:53,373 --> 00:37:58,478 line:-2
to generate any sort of custom response
that we might want to provide.


687
00:37:58,545 --> 00:38:01,648 line:-2
So I've taken the liberty
of adding a few custom responses


688
00:38:01,715 --> 00:38:03,083 line:-1
to the Nosh application.


689
00:38:03,150 --> 00:38:04,384 line:-1
Let's try it out.


690
00:38:04,985 --> 00:38:08,822 line:-2
So, the user might type
something like, uh,


691
00:38:08,889 --> 00:38:13,026 line:-1
"Do you... deliver to...


692
00:38:14,828 --> 00:38:16,230 line:-1
Cupertino?"


693
00:38:16,296 --> 00:38:17,865 line:-1
So what is our model going to tell us?


694
00:38:17,931 --> 00:38:19,633 line:-1
It's going to look at all these words,


695
00:38:19,700 --> 00:38:22,102 line:-2
and it will notice
that Cupertino is a city,


696
00:38:22,169 --> 00:38:24,204 line:-1
and it's a city they want delivery to,


697
00:38:24,271 --> 00:38:28,775 line:-2
so we can generate a custom response
that is specific to Cupertino.


698
00:38:29,810 --> 00:38:35,082 line:-2
Or they might ask,
"Do you deliver... pizza?"


699
00:38:35,148 --> 00:38:38,485 line:-2
And then our model will notice
that pizza is a food name,


700
00:38:38,552 --> 00:38:42,222 line:-2
so we can generate a custom response
based on pizza.


701
00:38:42,289 --> 00:38:45,325 line:-1
Or maybe they ask


702
00:38:45,392 --> 00:38:50,163 line:-2
if we deliver from a specific restaurant,
say, Pizza City,


703
00:38:50,998 --> 00:38:55,269 line:-1
and the one in Cupertino.


704
00:38:55,335 --> 00:38:59,573 line:-2
And in that case, the model will tell us
that Pizza City is a restaurant name


705
00:38:59,640 --> 00:39:02,809 line:-2
and that Cupertino is a city
where the food is coming from,


706
00:39:02,876 --> 00:39:05,078 line:-1
and we can use either or both of those


707
00:39:05,145 --> 00:39:08,849 line:-2
to generate a custom response
that mentions those.


708
00:39:09,283 --> 00:39:12,419 line:-1
So that shows the power of word tagging


709
00:39:12,486 --> 00:39:16,023 line:-2
to extract pieces of information
from unstructured text.


710
00:39:17,724 --> 00:39:22,329 line:-2
So let's go back to the slides
and let me turn it back over to Vivek.


711
00:39:22,996 --> 00:39:27,201 line:-2
Thank you, Doug, for showing us a demo
of transfer learning with word tagging.


712
00:39:27,267 --> 00:39:30,671 line:-2
Now, transfer learning for word tagging
is supported for the same languages


713
00:39:30,737 --> 00:39:32,706 line:-2
as static embeddings
and sentence embeddings


714
00:39:32,773 --> 00:39:34,074 line:-1
across Apple platforms.


715
00:39:34,641 --> 00:39:36,610 line:-2
To get the best use
out of transfer learning


716
00:39:36,677 --> 00:39:39,546 line:-2
for word tagging technology,
we have a few recommendations.


717
00:39:39,613 --> 00:39:42,482 line:-2
We recommend that you first start off
with the conditional random field,


718
00:39:42,549 --> 00:39:44,885 line:-1
especially for languages such as English.


719
00:39:44,952 --> 00:39:46,787 line:-1
The conditional random field, or CRF,


720
00:39:46,854 --> 00:39:49,790 line:-2
pays particular attention
to syntactic features,


721
00:39:49,857 --> 00:39:52,226 line:-2
which are quite useful
in many applications.


722
00:39:52,292 --> 00:39:55,095 line:-2
However, if you do not know
the kind of distribution


723
00:39:55,162 --> 00:39:57,798 line:-1
that you will be encountering at runtime,


724
00:39:57,865 --> 00:40:01,435 line:-2
it is better to use transfer learning
because it provides better generalization.


725
00:40:02,836 --> 00:40:07,975 line:-2
We also recommend you to use more data
for transfer learning for word tagging.


726
00:40:08,041 --> 00:40:10,444 line:-2
Since the prediction
is at a per-token level


727
00:40:10,511 --> 00:40:12,646 line:-1
in contrast with text classification,


728
00:40:12,713 --> 00:40:15,816 line:-2
it requires an order
of more magnitude data.


729
00:40:15,883 --> 00:40:19,620 line:-2
As I mentioned, NSLinguisticTagger
has now been marked for deprecation.


730
00:40:19,686 --> 00:40:23,123 line:-2
We strongly encourage you to move towards
Natural Language framework.


731
00:40:23,757 --> 00:40:27,194 line:-2
We also told you how to use
confidence scores


732
00:40:27,261 --> 00:40:29,129 line:-1
along with existing APIs.


733
00:40:29,196 --> 00:40:33,667 line:-2
And this can be used to prune out
false positives in your application.


734
00:40:33,734 --> 00:40:36,937 line:-2
And then we provided an overview
of sentence embedding technology


735
00:40:37,004 --> 00:40:41,141 line:-2
and demonstrated how you can use this
in several hypothetical apps.


736
00:40:41,208 --> 00:40:45,245 line:-2
And we concluded with a new technology
for transfer learning for word tagging.


737
00:40:47,114 --> 00:40:49,816 line:-2
With that,
we'd like to conclude by saying,


738
00:40:49,883 --> 00:40:53,520 line:-2
make your apps smarter by using
the Natural Language framework.


739
00:40:53,587 --> 00:40:55,222 line:-1
Thank you for your attention.

