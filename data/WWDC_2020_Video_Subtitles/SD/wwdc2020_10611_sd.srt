1
00:00:03,904 --> 00:00:06,840 line:-1
Hello and welcome to WWDC.


2
00:00:08,842 --> 00:00:12,079 line:0
Hi, my name's Quinton,
and I'm an engineer on the ARKit team.


3
00:00:12,145 --> 00:00:15,516 line:0
Today, both Praveen and I get to show you
some of the new features in ARKit


4
00:00:15,582 --> 00:00:17,117 line:0
with iOS 14.


5
00:00:17,184 --> 00:00:19,920 line:0
So let's jump right in
and explore ARKit 4.


6
00:00:19,987 --> 00:00:22,489 line:-2
This release
adds many advancements to ARKit,


7
00:00:22,556 --> 00:00:25,425 line:-2
which already powers
the world's largest AR platform--


8
00:00:25,492 --> 00:00:26,693 line:-1
iOS.


9
00:00:26,760 --> 00:00:29,396 line:-2
ARKit gives you the tools
to create AR experiences


10
00:00:29,463 --> 00:00:31,598 line:-2
that change the way your users
see the world.


11
00:00:32,566 --> 00:00:35,602 line:-2
Some of these tools include
device motion tracking,


12
00:00:35,669 --> 00:00:38,505 line:-2
camera scene capture
and advanced scene processing,


13
00:00:38,572 --> 00:00:40,307 line:-1
which all help to simplify the task


14
00:00:40,374 --> 00:00:44,044 line:-2
of building a realistic
and immersive AR experience.


15
00:00:44,111 --> 00:00:46,013 line:-1
Let's see what's next with ARKit.


16
00:00:46,914 --> 00:00:50,484 line:-2
So first, we're gonna take a look
at the Location Anchor API.


17
00:00:50,551 --> 00:00:54,054 line:-2
Location anchors bring your AR experience
onto the global scale


18
00:00:54,121 --> 00:00:57,958 line:-2
by allowing you to position
virtual content in relation to the globe.


19
00:00:58,025 --> 00:01:00,594 line:-2
Then we'll see what
the new LiDAR sensor brings to ARKit


20
00:01:00,661 --> 00:01:02,196 line:-1
with Scene Geometry.


21
00:01:02,863 --> 00:01:06,400 line:-2
Scene Geometry provides apps
with a mesh of the surrounding environment


22
00:01:06,466 --> 00:01:09,503 line:-2
that can be used
for everything from occlusion to lighting.


23
00:01:10,204 --> 00:01:13,006 line:-2
Next, we'll look at the technology
that enables Scene Geometry--


24
00:01:13,073 --> 00:01:14,808 line:-1
the Depth API.


25
00:01:14,875 --> 00:01:18,545 line:-2
We're opening up this API to give apps
access to a dense depth map


26
00:01:18,612 --> 00:01:22,049 line:-2
to enable new possibilities
using the LiDAR sensor.


27
00:01:22,115 --> 00:01:25,252 line:-2
And additionally, the LiDAR sensor
improves object placement.


28
00:01:26,053 --> 00:01:27,554 line:-1
We'll go over some best practices


29
00:01:27,621 --> 00:01:31,191 line:-2
to make sure your apps take full advantage
of the newest object placement techniques.


30
00:01:32,025 --> 00:01:34,828 line:0
And we'll wrap up with some improvements
to Face Tracking.


31
00:01:36,163 --> 00:01:38,565 line:-1
Let's start with location anchors.


32
00:01:38,632 --> 00:01:41,869 line:-2
Before we get too far,
let's look at how we got to this point.


33
00:01:41,935 --> 00:01:44,805 line:-2
ARKit started on iOS
with the best tracking.


34
00:01:44,872 --> 00:01:47,808 line:-2
No QR codes,
no external equipment needed--


35
00:01:47,875 --> 00:01:51,411 line:-2
just start an AR experience
by placing content around you.


36
00:01:51,478 --> 00:01:54,248 line:-1
Then we added multi-user experiences.


37
00:01:54,314 --> 00:01:56,583 line:-2
Your AR content
could then be shared with a friend


38
00:01:56,650 --> 00:02:00,187 line:-2
using a separate device
to make experiences social.


39
00:02:00,254 --> 00:02:03,290 line:-2
And last year,
we brought people into ARKit.


40
00:02:03,357 --> 00:02:06,426 line:-2
AR experiences are now aware
of the people in the scene.


41
00:02:06,493 --> 00:02:09,896 line:-2
Motion Capture is possible
with just a single iOS device,


42
00:02:09,963 --> 00:02:12,900 line:-2
and People Occlusion
makes AR content even more immersive,


43
00:02:12,966 --> 00:02:15,736 line:-2
as people can walk
right in front of a virtual object.


44
00:02:16,970 --> 00:02:21,441 line:-2
All these features combine to make
some amazing experiences, but what's next?


45
00:02:21,508 --> 00:02:25,546 line:-2
So now we're bringing AR into the outdoors
with location anchors.


46
00:02:25,612 --> 00:02:29,416 line:-2
Location anchors enable you to place
AR content in relation to the globe.


47
00:02:30,517 --> 00:02:32,653 line:-2
This means you can
now place virtual objects


48
00:02:32,719 --> 00:02:37,991 line:-2
and create AR experiences by specifying
a latitude, longitude and altitude.


49
00:02:38,058 --> 00:02:40,494 line:-2
ARKit will take
your geographic coordinates,


50
00:02:40,561 --> 00:02:43,530 line:-2
as well as high-resolution map data
from Apple Maps,


51
00:02:43,597 --> 00:02:46,900 line:-2
to place your AR experiences
at the specific world location.


52
00:02:47,868 --> 00:02:51,038 line:-2
This whole process is called
visual localization,


53
00:02:51,104 --> 00:02:54,975 line:-2
and it will precisely locate your device
in relation to the surrounding environment


54
00:02:55,042 --> 00:02:58,312 line:-2
more accurately than
could be done before with just GPS.


55
00:02:59,646 --> 00:03:02,516 line:-2
All this is possible due to
advanced machine-learning techniques


56
00:03:02,583 --> 00:03:04,351 line:-1
running right on your device.


57
00:03:04,418 --> 00:03:07,788 line:-2
There's no processing in the cloud
and no images sent back to Apple.


58
00:03:09,056 --> 00:03:12,092 line:-2
ARKit also takes care
of merging the local coordinate system


59
00:03:12,159 --> 00:03:16,330 line:-2
to the geographic coordinate system
so you can work in one unified system


60
00:03:16,396 --> 00:03:19,700 line:-2
regardless of how you want
to create your AR experiences.


61
00:03:19,766 --> 00:03:23,003 line:-2
To access these features,
we've added a new configuration,


62
00:03:23,070 --> 00:03:25,239 line:-1
ARGeoTrackingConfiguration,


63
00:03:25,305 --> 00:03:29,009 line:-2
and ARGeoAnchors
are what you'll use to place in content


64
00:03:29,076 --> 00:03:31,311 line:-1
the same way as other ARKit anchors.


65
00:03:32,379 --> 00:03:34,281 line:-1
Let's see some location anchors in action.


66
00:03:34,348 --> 00:03:38,118 line:-2
We've got a video here in front
of the Ferry Building in San Francisco.


67
00:03:38,185 --> 00:03:39,887 line:-1
You can see a large virtual sculpture


68
00:03:39,953 --> 00:03:42,823 line:-2
that's actually the <i>Companion</i> sculpture
created by KAWS


69
00:03:42,890 --> 00:03:45,092 line:-1
and viewed in the Acute Art app.


70
00:03:45,158 --> 00:03:47,327 line:-1
Since it was placed with location anchors,


71
00:03:47,394 --> 00:03:49,563 line:-2
everyone who uses the app
at the Ferry Building


72
00:03:49,630 --> 00:03:53,767 line:-2
can enjoy the virtual art
in the same place and the same way.


73
00:03:53,834 --> 00:03:56,703 line:-2
Let's see what's under the hood
in ARKit to make this all work.


74
00:03:58,539 --> 00:04:00,440 line:-1
So when using geo-tracking,


75
00:04:00,507 --> 00:04:03,911 line:-2
we download all the detailed map data
from Apple Maps


76
00:04:03,977 --> 00:04:05,679 line:-1
around your current location.


77
00:04:06,613 --> 00:04:08,882 line:-1
Part of this data is a localization map


78
00:04:08,949 --> 00:04:11,318 line:-2
that contains feature points
of the surrounding area


79
00:04:11,385 --> 00:04:13,220 line:-1
that can be seen from the street.


80
00:04:13,287 --> 00:04:15,622 line:-1
Then, with the localization map,


81
00:04:15,689 --> 00:04:19,192 line:-2
your current location
and images from your device,


82
00:04:19,259 --> 00:04:21,295 line:-1
we can use advanced machine learning


83
00:04:21,361 --> 00:04:25,299 line:-2
to visually localize
and determine your device's position.


84
00:04:25,365 --> 00:04:27,968 line:-2
All this is happening
under the hood in ARKit


85
00:04:28,035 --> 00:04:30,470 line:-1
to give you a precise, globally aware pose


86
00:04:30,537 --> 00:04:32,673 line:-2
without worrying
about any of this complexity.


87
00:04:34,842 --> 00:04:38,312 line:-2
The Location Anchor API
can be broken down into three main parts.


88
00:04:38,378 --> 00:04:41,481 line:-2
ARGeoTrackingConfiguration
is the configuration that you'll use


89
00:04:41,548 --> 00:04:45,118 line:-2
to take advantage
of all the new location anchor features.


90
00:04:45,185 --> 00:04:48,255 line:-2
This configuration contains a subset
of the world-tracking features


91
00:04:48,322 --> 00:04:50,290 line:-1
that are compatible with geo-tracking.


92
00:04:51,525 --> 00:04:55,529 line:-2
Then, once you've started an AR session
with a geo-tracking configuration,


93
00:04:55,596 --> 00:04:59,600 line:-2
you'll be able to create ARGeoAnchors
just like any other ARKit anchor.


94
00:05:01,068 --> 00:05:02,903 line:-1
And also, while using geo-tracking,


95
00:05:02,970 --> 00:05:05,906 line:-2
there's a new tracking status
that's important to monitor.


96
00:05:05,973 --> 00:05:08,475 line:-2
This is contained
in ARGeoTrackingStatus


97
00:05:08,542 --> 00:05:11,912 line:-2
and provides valuable feedback
to improve the geo-tracking experience.


98
00:05:12,679 --> 00:05:16,517 line:-2
So building an app with location anchors
can be broken down into a few steps.


99
00:05:16,583 --> 00:05:19,653 line:-2
The first is checking availability
of geo-tracking.


100
00:05:19,720 --> 00:05:22,089 line:-2
ARGeoTrackingConfiguration
has a few methods


101
00:05:22,155 --> 00:05:26,059 line:-2
that let us check the preconditions to
using the rest of the Location Anchor API.


102
00:05:27,127 --> 00:05:31,031 line:-2
Then, location anchors can be added once
we know there's full geo-tracking support,


103
00:05:31,098 --> 00:05:32,599 line:-1
and after anchors are added,


104
00:05:32,666 --> 00:05:35,569 line:-2
we can use a rendering engine
to place virtual content.


105
00:05:36,403 --> 00:05:39,439 line:-2
We'll then need to take care
of geo-tracking transitions.


106
00:05:39,506 --> 00:05:42,209 line:-2
Once started, geo-tracking
will move through a few states


107
00:05:42,276 --> 00:05:45,946 line:-2
that may need some user intervention to
ensure the best geo-tracking experience.


108
00:05:46,013 --> 00:05:48,182 line:-1
Let's build a simple point-of-interest app


109
00:05:48,248 --> 00:05:50,450 line:-2
to see what these steps look like
in practice.


110
00:05:51,285 --> 00:05:54,054 line:-2
In our app, we're gonna start
with helping our users find


111
00:05:54,121 --> 00:05:57,891 line:-2
the iconic Ferry Building
in San Francisco, California.


112
00:05:57,958 --> 00:06:01,695 line:-2
As you can see, we've placed a sign
to make the building easy to spot.


113
00:06:01,762 --> 00:06:04,698 line:-2
To begin the app, let's first start
with checking availability.


114
00:06:04,765 --> 00:06:06,300 line:-1
As with many ARKit features,


115
00:06:06,366 --> 00:06:08,602 line:-2
we need to make sure
the current device is supported


116
00:06:08,669 --> 00:06:10,838 line:-1
before attempting to start an experience.


117
00:06:11,605 --> 00:06:15,576 line:-2
Location anchors are available on devices
with an A12 Bionic chip and newer


118
00:06:15,642 --> 00:06:17,311 line:-1
as well as GPS.


119
00:06:17,377 --> 00:06:20,814 line:-2
ARGeoTrackingConfiguration's
isSupported class method


120
00:06:20,881 --> 00:06:22,883 line:-1
should be used to check for this support.


121
00:06:24,451 --> 00:06:28,355 line:-2
For geo-tracking, we also need to check
if the current location is supported.


122
00:06:28,422 --> 00:06:33,227 line:-2
We need to be in a location that has
all the required Maps data to localize.


123
00:06:33,293 --> 00:06:35,229 line:-2
The geo-tracking configuration
has a method


124
00:06:35,295 --> 00:06:37,364 line:-1
to check your current location's support


125
00:06:37,431 --> 00:06:39,967 line:-2
as well as an arbitrary latitude
and longitude.


126
00:06:41,201 --> 00:06:44,137 line:-2
Additionally,
once a geo-tracking session is started,


127
00:06:44,204 --> 00:06:48,642 line:-2
ARKit will ask the user for permission
for both camera and location.


128
00:06:48,709 --> 00:06:50,777 line:-2
ARKit has always asked
for camera permission,


129
00:06:50,844 --> 00:06:54,081 line:-2
but location permission is new
to geo-tracking.


130
00:06:54,147 --> 00:06:56,116 line:-1
Let's see what this looks like in code.


131
00:06:57,751 --> 00:07:00,354 line:-2
ARGeoTrackingConfiguration
has all the class methods


132
00:07:00,420 --> 00:07:03,190 line:-2
that we need to check
before starting our AR session.


133
00:07:03,257 --> 00:07:07,628 line:-2
We'll first check if the current device
is supported with isSupported.


134
00:07:07,694 --> 00:07:10,697 line:-2
Then we'll check if our current location
is available for geo-tracking


135
00:07:10,764 --> 00:07:12,633 line:-1
with checkAvailability.


136
00:07:12,699 --> 00:07:17,104 line:-2
If this check fails, we'll get an error
with more info to display to the user--


137
00:07:17,171 --> 00:07:20,641 line:-2
for example, if the user
hasn't given the app location permissions.


138
00:07:21,608 --> 00:07:24,344 line:-2
Then once we know our current device
and location are supported,


139
00:07:24,411 --> 00:07:26,380 line:-1
we can go ahead and start the session.


140
00:07:26,446 --> 00:07:28,015 line:-1
Since we're using RealityKit,


141
00:07:28,081 --> 00:07:31,885 line:-2
we'll need our ARView
and then update the configuration.


142
00:07:31,952 --> 00:07:35,355 line:-2
By default, ARView uses
a world-tracking configuration,


143
00:07:35,422 --> 00:07:38,091 line:-2
and so we need pass in
a geo-tracking configuration


144
00:07:38,158 --> 00:07:39,660 line:-1
when running the session.


145
00:07:41,094 --> 00:07:43,497 line:-1
The next step is adding a location anchor.


146
00:07:44,164 --> 00:07:48,702 line:-2
To do this, we'll use
the new ARAnchor subclass, ARGeoAnchor.


147
00:07:48,769 --> 00:07:52,306 line:-2
Geo anchors are similar
to existing ARKit anchors in many ways.


148
00:07:52,372 --> 00:07:55,542 line:-2
However, because geo anchors
operate on global coordinates,


149
00:07:55,609 --> 00:07:58,212 line:-1
we can't create them with just transforms.


150
00:07:58,278 --> 00:08:00,514 line:-2
We need to specify
their geographic coordinates


151
00:08:00,581 --> 00:08:03,584 line:-1
with latitude, longitude and altitude.


152
00:08:03,650 --> 00:08:05,452 line:-1
The most common way to create geo anchors


153
00:08:05,519 --> 00:08:08,388 line:-2
will be though specifying
just latitude and longitude,


154
00:08:08,455 --> 00:08:10,924 line:-2
which this allows ARKit
to fill in the altitude


155
00:08:10,991 --> 00:08:13,160 line:-2
based on Maps data
of the correct ground level.


156
00:08:13,861 --> 00:08:16,830 line:-2
Let's now add a location anchor
to our point-of-interest app.


157
00:08:17,898 --> 00:08:22,336 line:-2
So for our app, we need to start
by finding the Ferry Building's location.


158
00:08:22,402 --> 00:08:25,873 line:-2
One way we can get the latitude
and longitude is through the Maps app.


159
00:08:25,939 --> 00:08:28,208 line:-1
When we place a marker in the Maps app,


160
00:08:28,275 --> 00:08:32,246 line:-2
we now get up to six digits of precision
after the decimal.


161
00:08:32,312 --> 00:08:34,081 line:-1
It's important to use six or more digits


162
00:08:34,147 --> 00:08:37,083 line:-2
so that we get a precise location
to place our content.


163
00:08:38,118 --> 00:08:42,121 line:-2
Once we have a latitude and longitude,
we can make a geo anchor.


164
00:08:42,188 --> 00:08:45,859 line:-2
We don't need to specify an altitude
because we'll let ARKit use Maps data


165
00:08:45,926 --> 00:08:48,695 line:-2
to determine
the elevation of the ground level.


166
00:08:49,696 --> 00:08:52,065 line:-2
Then, we'll add the geo anchor
to our session,


167
00:08:52,966 --> 00:08:55,903 line:-2
and since we're using RealityKit
to render our virtual content


168
00:08:55,969 --> 00:08:58,071 line:-1
and we've already created our geo anchor,


169
00:08:58,138 --> 00:09:01,775 line:-2
we can go ahead and attach the anchor
to an entity to mark the Ferry Building.


170
00:09:02,476 --> 00:09:04,745 line:-2
Let's run our app
and see what it looks like.


171
00:09:05,579 --> 00:09:08,215 line:-2
We'll start near the Ferry Building
in San Francisco


172
00:09:08,282 --> 00:09:09,650 line:-1
looking towards Market Street.


173
00:09:09,716 --> 00:09:11,919 line:-1
And as we pan around,


174
00:09:11,985 --> 00:09:15,155 line:-2
we can see some of the palm trees
that line the city.


175
00:09:15,222 --> 00:09:18,559 line:0
And soon,
the Ferry Building will come into view.


176
00:09:18,625 --> 00:09:21,461 line:0
Our sign looks to be on the ground,
which is expected,


177
00:09:21,528 --> 00:09:23,063 line:0
but the text is rotated.


178
00:09:23,130 --> 00:09:25,899 line:0
Since we'd like to find the Ferry Building
easily from a distance,


179
00:09:25,966 --> 00:09:28,735 line:0
we'd really like to have the sign floating
a few meters in the air


180
00:09:28,802 --> 00:09:30,037 line:0
and facing towards the city.


181
00:09:30,103 --> 00:09:31,805 line:-1
So, how do we do this?


182
00:09:32,873 --> 00:09:34,074 line:-1
To position this content,


183
00:09:34,141 --> 00:09:36,910 line:-2
we need to first look
at the coordinate system of a geo anchor.


184
00:09:37,477 --> 00:09:40,047 line:-2
Geo anchors are fixed
to the cardinal directions.


185
00:09:40,113 --> 00:09:42,249 line:-2
Their axes are set
when you create the anchor,


186
00:09:42,316 --> 00:09:46,019 line:-2
and this orientation will remain unchanged
for the rest of the session.


187
00:09:46,086 --> 00:09:48,956 line:-2
A geo anchor's x-axis
is always pointed east,


188
00:09:49,022 --> 00:09:52,793 line:-2
and the z-axis is always pointed south
for any geographic coordinate.


189
00:09:52,860 --> 00:09:55,095 line:-2
Since we're using
a right-handed coordinate system,


190
00:09:55,162 --> 00:09:58,398 line:-2
this leaves positive Y pointing up,
away from the ground.


191
00:09:58,465 --> 00:10:01,635 line:-2
Geo anchors, like all other ARKit anchors,
are immutable.


192
00:10:02,669 --> 00:10:04,671 line:-2
This means we'll need to use
our rendering engine


193
00:10:04,738 --> 00:10:09,042 line:-2
to rotate or translate our virtual objects
from the geo anchor's origin.


194
00:10:09,109 --> 00:10:11,879 line:-2
Let's clean up our sign that we placed
in front of the Ferry Building.


195
00:10:12,913 --> 00:10:16,049 line:-2
Here's some RealityKit code
to start updating our sign.


196
00:10:16,717 --> 00:10:19,820 line:-2
After getting the signEntity
and adding it to the geoAnchorEntity,


197
00:10:19,887 --> 00:10:22,189 line:-2
we want to rotate the sign
towards the city.


198
00:10:22,923 --> 00:10:27,261 line:-2
To do this, we'll rotate it by
a little less than 90 degrees clockwise,


199
00:10:27,327 --> 00:10:30,797 line:-2
and we'll elevate the sign's position
by 35 meters.


200
00:10:30,864 --> 00:10:34,034 line:-2
Both of these operations
are in relation to the geoAnchorEntity


201
00:10:34,101 --> 00:10:35,936 line:-1
that we had previously created.


202
00:10:36,003 --> 00:10:37,704 line:-1
Let's see what this looks like in the app.


203
00:10:39,506 --> 00:10:43,343 line:-2
Now when we pan around
and we get to our Ferry Building,


204
00:10:43,410 --> 00:10:46,747 line:-2
our sign is high in the air
and we can see it from a distance.


205
00:10:46,813 --> 00:10:49,183 line:-2
The text is much easier to read
in this orientation.


206
00:10:49,249 --> 00:10:50,317 line:-1
This looks great,


207
00:10:50,384 --> 00:10:53,654 line:-2
but we're missing some crucial information
here about the geo-tracking state


208
00:10:53,720 --> 00:10:57,191 line:-2
that we can use to guide the user
to the best geo-tracking experience.


209
00:10:57,257 --> 00:11:00,928 line:-2
When using a geo-tracking configuration,
there's a new geo-tracking status object


210
00:11:00,994 --> 00:11:04,364 line:-2
that's available on ARFrame
and ARSessionObserver.


211
00:11:04,431 --> 00:11:05,732 line:-1
ARGeoTrackingStatus


212
00:11:05,799 --> 00:11:08,902 line:-2
encapsulates all the current state
information of geo-tracking,


213
00:11:08,969 --> 00:11:13,173 line:-2
similar to the world-tracking information
that's available on ARCamera.


214
00:11:13,240 --> 00:11:16,043 line:-1
Within GeoTrackingStatus is a state.


215
00:11:16,109 --> 00:11:20,714 line:-2
This state indicates how far along
geo-tracking is during localization.


216
00:11:20,781 --> 00:11:23,116 line:-2
There's also a property
that provides more information


217
00:11:23,183 --> 00:11:26,887 line:-2
about the current localization state
called GeoTrackingStateReason.


218
00:11:27,654 --> 00:11:31,491 line:-2
And there's an accuracy provided
once geo-tracking localizes.


219
00:11:31,558 --> 00:11:33,894 line:-2
Let's take a closer look
at the GeoTrackingState.


220
00:11:34,728 --> 00:11:39,132 line:-2
When an AR session begins,
GeoTrackingState starts at Initializing.


221
00:11:39,199 --> 00:11:42,903 line:-2
At this point, geo-tracking is waiting
for world tracking to initialize.


222
00:11:43,770 --> 00:11:47,541 line:-2
From Initializing, the tracking state
can immediately go to Not Available


223
00:11:47,608 --> 00:11:51,044 line:-2
if geo-tracking isn't supported
in the current location.


224
00:11:51,111 --> 00:11:53,580 line:-2
If you're using
the checkAvailability class method


225
00:11:53,647 --> 00:11:57,384 line:-2
on GeoTrackingConfiguration,
you should rarely get into this state.


226
00:11:58,552 --> 00:12:00,787 line:-1
Once geo-tracking moves to Localizing,


227
00:12:00,854 --> 00:12:03,724 line:-2
ARKit is receiving images
as well as Maps data


228
00:12:03,790 --> 00:12:05,826 line:-1
and is trying to compute pose.


229
00:12:05,893 --> 00:12:09,162 line:-2
However, during both the Initializing
and Localizing states,


230
00:12:09,229 --> 00:12:13,000 line:-2
there could be issues detected
that prevent localization.


231
00:12:13,066 --> 00:12:16,570 line:-2
These issues are communicated through
GeoTrackingStateReason.


232
00:12:17,471 --> 00:12:19,506 line:-2
This reason should be used
to inform the user


233
00:12:19,573 --> 00:12:21,675 line:-1
how to help geo-tracking localize.


234
00:12:22,376 --> 00:12:25,712 line:-2
Some possible reasons include
the device is pointed too low,


235
00:12:25,779 --> 00:12:28,382 line:-2
which we'd then inform the user
to raise the device,


236
00:12:28,448 --> 00:12:30,117 line:-1
or geoDataNotLoaded,


237
00:12:30,184 --> 00:12:33,554 line:-2
and we'd inform the user
that a network connection is required.


238
00:12:33,620 --> 00:12:37,724 line:-2
For all possible reasons,
have a look at ARGeoTrackingTypes.h.


239
00:12:39,059 --> 00:12:42,763 line:-2
In general, we want to encourage users
to point their devices at buildings


240
00:12:42,829 --> 00:12:46,667 line:-2
and other stationary structures
that are visible from the street.


241
00:12:46,733 --> 00:12:48,368 line:-1
Parking lots, open fields,


242
00:12:48,435 --> 00:12:50,637 line:-2
and other environments
that dynamically change


243
00:12:50,704 --> 00:12:52,306 line:-1
have a lower chance of localizing.


244
00:12:52,372 --> 00:12:55,409 line:-2
After addressing
any GeoTrackingStateReasons,


245
00:12:55,475 --> 00:12:57,678 line:-1
geo-tracking should become Localized.


246
00:12:58,579 --> 00:13:01,882 line:-2
It's at this point
that should you start your AR experience.


247
00:13:01,949 --> 00:13:04,084 line:-1
If you place objects before localization,


248
00:13:04,151 --> 00:13:07,087 line:-2
the objects could jump
to unintended locations.


249
00:13:07,154 --> 00:13:10,958 line:-2
Additionally, once localized,
ARGeoTrackingAccuracy is provided


250
00:13:11,024 --> 00:13:13,594 line:-2
to help you gate
what experiences should be enabled.


251
00:13:13,660 --> 00:13:16,463 line:-2
It's also important to always monitor
GeoTrackingState,


252
00:13:16,530 --> 00:13:20,334 line:-2
as it's possible for geo-tracking
to move back to Localizing


253
00:13:20,400 --> 00:13:21,802 line:-1
or even Initializing,


254
00:13:21,869 --> 00:13:25,372 line:-2
such as when tracking's lost
or map data isn't available.


255
00:13:25,439 --> 00:13:28,876 line:-2
Let's take a look at how we can add this
tracking state to improve our sample app.


256
00:13:29,343 --> 00:13:32,045 line:-2
Now we can see this whole time
we were actually localizing


257
00:13:32,112 --> 00:13:35,716 line:-2
when looking at Market Street
and the surrounding buildings.


258
00:13:35,782 --> 00:13:38,886 line:0
As we pan around, we can see
from the tracking state that we localize


259
00:13:38,952 --> 00:13:40,954 line:0
and then the accuracy increases to high.


260
00:13:42,856 --> 00:13:44,925 line:-2
I think we've got our app
just about ready,


261
00:13:44,992 --> 00:13:46,960 line:-1
at least for the Ferry Building.


262
00:13:47,027 --> 00:13:49,696 line:-2
We've added a more expansive
location anchor sample project


263
00:13:49,763 --> 00:13:51,665 line:-1
on developer.apple.com


264
00:13:51,732 --> 00:13:54,368 line:-2
that I encourage you to check out
after this talk.


265
00:13:54,434 --> 00:13:57,371 line:-2
For more information
on the RealityKit features used,


266
00:13:57,437 --> 00:14:01,408 line:-2
check out last year's talk, "Introducing
RealityKit and Reality Composer."


267
00:14:02,643 --> 00:14:05,112 line:-2
In our sample app,
we saw how to create location anchors


268
00:14:05,179 --> 00:14:07,447 line:-1
by directly specifying coordinates.


269
00:14:07,514 --> 00:14:10,250 line:-2
We already knew the geographic coordinates
for the Ferry Building.


270
00:14:10,317 --> 00:14:13,387 line:-2
However, these coordinates
could have come from any source,


271
00:14:13,453 --> 00:14:17,491 line:-2
such as our App Bundle,
our web back end or, really, any database.


272
00:14:18,458 --> 00:14:22,196 line:-2
Another way to create a location anchor
is via user interaction.


273
00:14:22,262 --> 00:14:25,666 line:-2
We could expand on our app in the future
by allowing users to tap the screen


274
00:14:25,732 --> 00:14:28,135 line:-1
to save their own point of interest.


275
00:14:28,202 --> 00:14:32,840 line:-2
GetGeoLocation(ForPoint on ARSession
allows us to get geographic coordinates


276
00:14:32,906 --> 00:14:36,109 line:-2
from any world point
in ARKit coordinate space.


277
00:14:36,176 --> 00:14:39,546 line:-2
For example, this could have come
from a raycast or a location on a plane.


278
00:14:41,448 --> 00:14:44,685 line:-2
Location anchors are available
for you today with iOS 14,


279
00:14:44,751 --> 00:14:47,988 line:-2
and we're starting with support
in the San Francisco Bay Area,


280
00:14:48,055 --> 00:14:50,991 line:-1
New York, Los Angeles, Chicago and Miami,


281
00:14:51,058 --> 00:14:53,660 line:-2
with more cities coming through
this summer.


282
00:14:53,727 --> 00:14:56,630 line:-2
All iPhones and iPads
with an A12 Bionic chip and newer,


283
00:14:56,697 --> 00:14:58,932 line:-1
as well as GPS, are supported.


284
00:14:58,999 --> 00:15:02,436 line:-2
Also, for any apps
that require location anchors exclusively,


285
00:15:02,503 --> 00:15:05,873 line:-2
you can use device capability keys
to limit your app in the App Store


286
00:15:05,939 --> 00:15:07,908 line:-1
to only compatible hardware.


287
00:15:07,975 --> 00:15:10,878 line:-2
In addition to the GPS key,
you'll need to use a new key


288
00:15:10,944 --> 00:15:14,982 line:-2
for devices with an A12 Bionic chip
or newer that's available on iOS 14.


289
00:15:15,849 --> 00:15:20,354 line:-2
With location anchors, you can now bring
your AR experiences onto the global scale.


290
00:15:20,420 --> 00:15:23,156 line:-2
We went over how
ARGeoTrackingConfiguration


291
00:15:23,223 --> 00:15:26,293 line:-2
is the entry point
to adding location anchors to your app.


292
00:15:26,360 --> 00:15:29,229 line:-2
We saw how to add ARGeoAnchors
to your AR scene


293
00:15:29,296 --> 00:15:32,666 line:-2
and how to position content
in relation to those anchors.


294
00:15:32,733 --> 00:15:36,703 line:-2
We also saw how ARGeoTrackingStatus
can be used to help guide the user


295
00:15:36,770 --> 00:15:39,273 line:-1
to the best geo-tracking experience.


296
00:15:39,339 --> 00:15:42,576 line:-2
And now here's Praveen
to tell you more about Scene Geometry.


297
00:15:43,177 --> 00:15:45,579 line:0
Hi, everyone. I am Praveen Gowda.


298
00:15:45,646 --> 00:15:48,215 line:0
I am an engineer on the ARKit team.


299
00:15:48,282 --> 00:15:53,020 line:0
Today, I am going to take you through
some of the APIs available in iOS 14


300
00:15:53,086 --> 00:15:56,290 line:0
that help bring the power
of the LiDAR Scanner to your applications.


301
00:15:56,356 --> 00:16:00,794 line:-2
In ARKit 3.5,
we introduced the Scene Geometry API


302
00:16:00,861 --> 00:16:04,665 line:-2
powered by the LiDAR Scanner
on the new iPad Pro.


303
00:16:05,599 --> 00:16:07,734 line:-1
Before we go into Scene Geometry,


304
00:16:07,801 --> 00:16:10,504 line:-2
let's take a look
at how the LiDAR Scanner works.


305
00:16:10,571 --> 00:16:13,774 line:-2
The LiDAR shoots light
onto the surroundings


306
00:16:13,841 --> 00:16:17,511 line:-2
and then collects the light
reflected off the surfaces in the scene.


307
00:16:17,578 --> 00:16:20,614 line:-2
The depth is estimated
by measuring the time it took


308
00:16:20,681 --> 00:16:24,218 line:-2
for the light to go from the LiDAR
to the environment


309
00:16:24,284 --> 00:16:26,553 line:-1
and reflect back to the scanner.


310
00:16:26,620 --> 00:16:30,557 line:-2
And this entire process runs
millions of times every second.


311
00:16:31,592 --> 00:16:34,895 line:-2
The LiDAR Scanner is used
by the Scene Geometry API


312
00:16:34,962 --> 00:16:38,398 line:-2
to provide a topological map
of the environment.


313
00:16:38,465 --> 00:16:41,969 line:-2
This can be optionally fused
with semantic classification,


314
00:16:42,035 --> 00:16:46,607 line:-2
which enables apps to recognize
and classify physical objects.


315
00:16:46,673 --> 00:16:50,511 line:-2
This provides an opportunity
for creating richer AR experiences


316
00:16:50,577 --> 00:16:54,715 line:-2
where apps can now occlude
virtual objects with the real world


317
00:16:54,781 --> 00:16:57,551 line:-2
or use physics
to enable realistic interactions


318
00:16:57,618 --> 00:17:00,220 line:-1
between virtual and physical objects...


319
00:17:01,588 --> 00:17:04,925 line:-2
or to use virtual lighting
on real-world surfaces,


320
00:17:04,992 --> 00:17:07,928 line:-2
and in many other use cases
that we've yet to imagine.


321
00:17:08,694 --> 00:17:11,464 line:-2
Let's take a quick look
at Scene Geometry in action.


322
00:17:11,964 --> 00:17:13,300 line:-1
Here's a living room,


323
00:17:13,367 --> 00:17:15,836 line:-2
and once the Scene Geometry API
is turned on,


324
00:17:15,903 --> 00:17:18,338 line:-1
the entire visible room is meshed.


325
00:17:18,405 --> 00:17:22,910 line:-2
Triangles vary in size to show
the optimum detail for each surface.


326
00:17:22,976 --> 00:17:27,146 line:-2
The color mesh appears
when semantic classification is enabled.


327
00:17:27,214 --> 00:17:30,117 line:-2
Each color represents
a different classification,


328
00:17:30,184 --> 00:17:33,520 line:-2
such as blue for the seats
and green for the floor.


329
00:17:35,722 --> 00:17:38,192 line:-1
As we saw, the Scene Geometry feature


330
00:17:38,258 --> 00:17:42,596 line:-2
is built by leveraging the depth data
gathered from the LiDAR Scanner.


331
00:17:42,663 --> 00:17:46,033 line:-1
In iOS 14, we have a new ARKit Depth API


332
00:17:46,099 --> 00:17:48,569 line:-2
that provides access
to the same depth data.


333
00:17:48,969 --> 00:17:51,605 line:-1
The API provides a dense depth image


334
00:17:51,672 --> 00:17:56,510 line:-2
where a pixel in the image corresponds
to depth in meters from the camera.


335
00:17:56,577 --> 00:17:59,780 line:-2
What we see here
is a debug visualization of this depth


336
00:17:59,847 --> 00:18:02,449 line:-1
where there is a gradient from blue to red


337
00:18:02,516 --> 00:18:05,419 line:-2
where blue represents regions
closer to the camera


338
00:18:05,485 --> 00:18:07,487 line:-1
and red represents those away.


339
00:18:08,956 --> 00:18:14,595 line:-2
The depth data would be available at 60 Hz
associated with each ARFrame.


340
00:18:14,661 --> 00:18:18,265 line:-2
The Scene Geometry feature
is built on top of this API


341
00:18:18,332 --> 00:18:20,701 line:-1
where depth data across multiple frames


342
00:18:20,767 --> 00:18:24,905 line:-2
are aggregated and processed
to construct a 3D mesh.


343
00:18:24,972 --> 00:18:27,808 line:-1
This API is powered by the LiDAR Scanner


344
00:18:27,875 --> 00:18:30,777 line:-2
and hence will be available
on devices which have LiDAR.


345
00:18:30,844 --> 00:18:34,648 line:-2
Here is an illustration
of how the depth map is generated.


346
00:18:34,715 --> 00:18:38,018 line:-2
The colored RGB image
from the wide-angle camera


347
00:18:38,085 --> 00:18:40,320 line:-2
and the depth readings
from the LiDAR Scanner


348
00:18:40,387 --> 00:18:44,024 line:-2
are fused together
using advanced machine-learning algorithms


349
00:18:44,091 --> 00:18:48,829 line:-2
to create a dense depth map
that is exposed through the API.


350
00:18:48,896 --> 00:18:52,199 line:-1
This operation runs at 60 times per second


351
00:18:52,266 --> 00:18:55,669 line:-2
with the depth map
available on every ARFrame.


352
00:18:57,471 --> 00:18:59,373 line:-1
To access the depth data,


353
00:18:59,439 --> 00:19:03,877 line:-2
each ARFrame will have a new property
called sceneDepth.


354
00:19:03,944 --> 00:19:08,348 line:-2
This provides an object
of type ARDepthData.


355
00:19:08,415 --> 00:19:11,919 line:-2
ARDepthData
is a container for two buffers--


356
00:19:11,985 --> 00:19:16,356 line:-2
one is a depth map,
and the other is a confidence map.


357
00:19:17,624 --> 00:19:20,527 line:-1
The depth map is a CVPixelBuffer


358
00:19:20,594 --> 00:19:25,032 line:-2
where each pixel
represents depth and is in meters.


359
00:19:25,098 --> 00:19:27,434 line:-1
And this depth corresponds to the distance


360
00:19:27,501 --> 00:19:31,305 line:-2
from the plane of the camera
to a point in the world.


361
00:19:31,371 --> 00:19:34,842 line:-2
One thing to note is that the depth map
is smaller in resolution


362
00:19:34,908 --> 00:19:39,913 line:-2
compared to the capturedImage on ARFrame
but still preserves the same aspect ratio.


363
00:19:41,481 --> 00:19:44,084 line:-2
The other buffer
on the ARDepthData object


364
00:19:44,151 --> 00:19:45,786 line:-1
is the confidence map.


365
00:19:47,287 --> 00:19:49,756 line:-1
Since the measurement of depth using LiDAR


366
00:19:49,823 --> 00:19:52,659 line:-2
is based on the light
which reflects from objects,


367
00:19:52,726 --> 00:19:54,294 line:-1
the accuracy of the depth map


368
00:19:54,361 --> 00:19:57,764 line:-2
can be impacted by the nature
of the surrounding environment.


369
00:19:59,032 --> 00:20:00,367 line:-1
Challenging surfaces,


370
00:20:00,434 --> 00:20:04,104 line:-2
such as those which are highly reflective
or those with high absorption,


371
00:20:04,171 --> 00:20:06,507 line:-1
can lower the accuracy of the depth.


372
00:20:10,110 --> 00:20:14,681 line:-2
This accuracy is expressed
through a value we call "confidence."


373
00:20:14,748 --> 00:20:18,485 line:-2
For each depth pixel,
there is a corresponding confidence value


374
00:20:18,552 --> 00:20:21,154 line:-1
of type ARConfidenceLevel,


375
00:20:21,221 --> 00:20:25,526 line:-2
and this value
can either be low, medium or high


376
00:20:25,592 --> 00:20:29,563 line:-2
and will help to filter depth based
on the requirements of your application.


377
00:20:31,198 --> 00:20:34,268 line:-1
Let's see how we can use the Depth API.


378
00:20:34,334 --> 00:20:38,772 line:-2
I begin with creating an AR session
and a world-tracking configuration.


379
00:20:39,673 --> 00:20:42,543 line:-2
There is a new frame semantic
called sceneDepth


380
00:20:42,609 --> 00:20:45,712 line:-1
which allows you to turn on the Depth API.


381
00:20:45,779 --> 00:20:49,783 line:-2
As always, I check if the frame semantic
is supported on the device


382
00:20:49,850 --> 00:20:54,454 line:-2
using the supportsFrameSemantic method
on the configuration class.


383
00:20:54,521 --> 00:20:59,526 line:-2
Then, we can set the frame semantic
to sceneDepth and run the configuration.


384
00:20:59,593 --> 00:21:05,032 line:-2
After this, I can access the depth data
from the sceneDepth property on ARFrame


385
00:21:05,098 --> 00:21:07,935 line:-2
using
the didUpdateFrame delegate method.


386
00:21:10,437 --> 00:21:12,773 line:-1
Additionally, if you have an AR app


387
00:21:12,840 --> 00:21:15,609 line:-2
that uses People Occlusion feature
and hence sets


388
00:21:15,676 --> 00:21:18,679 line:-2
the personSegmentationWithDepth
frame semantic,


389
00:21:18,745 --> 00:21:20,714 line:-2
then you will
automatically get scene depth


390
00:21:20,781 --> 00:21:23,517 line:-2
on devices that support
the sceneDepth frame semantic


391
00:21:23,584 --> 00:21:26,286 line:-2
with no additional power cost
to your application.


392
00:21:26,353 --> 00:21:30,357 line:-2
Here is a demo of an app
that we built using the Depth API.


393
00:21:30,424 --> 00:21:35,495 line:-2
The depth from the depth map is
un-projected to 3D to form a point cloud.


394
00:21:35,562 --> 00:21:40,100 line:-2
The point cloud is colored
using the capturedImage on the ARFrame.


395
00:21:40,167 --> 00:21:43,837 line:-2
By accumulating depth data
across multiple ARFrames,


396
00:21:43,904 --> 00:21:47,975 line:-2
we get a dense 3D point cloud
like the one we see here.


397
00:21:48,041 --> 00:21:52,246 line:0
I can also filter the point clouds
based on the confidence level.


398
00:21:52,312 --> 00:21:55,349 line:0
This is the point cloud
formed by all the depth pixels,


399
00:21:55,415 --> 00:21:57,584 line:0
including those with low confidence.


400
00:21:58,752 --> 00:22:01,154 line:0
And here is the point cloud
by filtering depth


401
00:22:01,221 --> 00:22:04,558 line:0
whose confidence is medium or high.


402
00:22:04,625 --> 00:22:06,727 line:0
And this is the point cloud we get


403
00:22:06,793 --> 00:22:10,230 line:0
by using only the depth
which has high confidence.


404
00:22:10,297 --> 00:22:12,466 line:0
This gives us a clear picture


405
00:22:12,533 --> 00:22:17,204 line:0
of how the physical properties of surfaces
impact the confidence level of its depth.


406
00:22:17,271 --> 00:22:20,841 line:0
Your application and its tolerance
to inaccuracies in depth


407
00:22:20,908 --> 00:22:22,910 line:0
will determine
how you will filter the depth


408
00:22:22,976 --> 00:22:24,745 line:0
based on its confidence level.


409
00:22:25,345 --> 00:22:28,649 line:-2
Let's take a closer look
at how we built this app.


410
00:22:28,715 --> 00:22:32,286 line:-2
For each ARFrame,
we access the sceneDepth property


411
00:22:32,352 --> 00:22:34,288 line:-1
with the ARDepthData object,


412
00:22:34,354 --> 00:22:37,257 line:-2
providing us with the depth
and the confidence map.


413
00:22:37,891 --> 00:22:42,362 line:-2
The key part of the app is
a Metal vertex shader called "unproject."


414
00:22:43,797 --> 00:22:46,867 line:-2
As the name suggests,
it un-projects the depth data


415
00:22:46,934 --> 00:22:51,572 line:-2
from the depth map to the 3D space
using parameters on the ARCamera


416
00:22:51,638 --> 00:22:56,109 line:-2
such as the camera's transform,
its intrinsics and the projection matrix.


417
00:22:56,176 --> 00:23:01,281 line:-2
The shader also uses capturedImage
to sample color for each depth pixel.


418
00:23:01,348 --> 00:23:04,484 line:-2
What we get as an output of this
is a 3D point cloud


419
00:23:04,551 --> 00:23:06,587 line:-1
which is then rendered using Metal.


420
00:23:07,621 --> 00:23:10,891 line:-2
To summarize,
we have a new Depth API in ARKit 4


421
00:23:10,958 --> 00:23:14,194 line:-2
which gives a highly accurate
representation of the world.


422
00:23:14,261 --> 00:23:16,663 line:-2
There is a frame semantic
called sceneDepth


423
00:23:16,730 --> 00:23:19,299 line:-1
which allows you to enable the feature.


424
00:23:19,366 --> 00:23:24,805 line:-2
Once enabled, the depth data will be
available at 60 Hz on each ARFrame.


425
00:23:24,872 --> 00:23:29,209 line:-2
The depth data will have
a depth map and a confidence map.


426
00:23:29,276 --> 00:23:32,646 line:-2
And the API is supported
on devices with a LiDAR Scanner.


427
00:23:33,480 --> 00:23:37,684 line:-2
One of the fundamental tasks
in many AR apps is placing objects.


428
00:23:37,751 --> 00:23:41,088 line:-2
And in ARKit 3,
we introduced the raycasting API


429
00:23:41,154 --> 00:23:43,457 line:-1
to make object placement easier.


430
00:23:43,524 --> 00:23:48,428 line:-2
In ARKit 4, the LiDAR Scanner brings
some great improvement to raycasting.


431
00:23:49,830 --> 00:23:53,066 line:-2
Raycasting is highly optimized
for object placement


432
00:23:53,133 --> 00:23:58,338 line:-2
and makes it easy to precisely place
virtual objects in your AR app.


433
00:23:58,405 --> 00:24:02,242 line:-2
Placing objects in ARKit 4
is more precise and quicker


434
00:24:02,309 --> 00:24:04,344 line:-1
thanks to the LiDAR Scanner.


435
00:24:04,411 --> 00:24:06,446 line:-1
Your apps that already use raycasting


436
00:24:06,513 --> 00:24:10,450 line:-2
will automatically benefit
on a LiDAR-enabled device.


437
00:24:10,517 --> 00:24:14,888 line:-2
Raycasting also leverages Scene Depth
or Scene Geometry when available


438
00:24:14,955 --> 00:24:18,358 line:-1
to instantly place objects in AR.


439
00:24:18,425 --> 00:24:20,861 line:-2
This works great,
even on featureless surfaces


440
00:24:20,928 --> 00:24:22,563 line:-1
such as white walls.


441
00:24:22,629 --> 00:24:23,964 line:-1
In iOS 14,


442
00:24:24,031 --> 00:24:28,235 line:-2
the raycast API is recommended
over hit-testing for object placement.


443
00:24:28,302 --> 00:24:30,304 line:-1
Before you can start raycasting,


444
00:24:30,370 --> 00:24:33,140 line:-1
you will need to create a raycast query.


445
00:24:33,207 --> 00:24:34,808 line:-1
A raycast query describes


446
00:24:34,875 --> 00:24:38,946 line:-2
the direction and the behavior
of the ray used for raycasting.


447
00:24:39,913 --> 00:24:41,949 line:-1
It is composed of a raycast target


448
00:24:42,015 --> 00:24:45,686 line:-2
which describes the type of surface
that a ray can intersect with.


449
00:24:45,752 --> 00:24:48,956 line:-2
Existing planes correspond
to planes detected by ARKit


450
00:24:49,022 --> 00:24:52,192 line:-2
while considering
the shape and size of the plane.


451
00:24:52,259 --> 00:24:56,563 line:-2
Infinite planes are the same planes,
but with their shape and size ignored.


452
00:24:56,630 --> 00:24:59,967 line:-2
And estimated planes
are planes of arbitrary orientation


453
00:25:00,033 --> 00:25:02,669 line:-2
formed from the feature points
around a surface.


454
00:25:03,971 --> 00:25:06,273 line:-1
The raycast target alignment specifies


455
00:25:06,340 --> 00:25:10,277 line:-2
the alignment of surfaces
that a ray can intersect with.


456
00:25:10,344 --> 00:25:13,347 line:-1
This can be horizontal, vertical or any.


457
00:25:14,448 --> 00:25:16,517 line:-1
There are two types of raycasts.


458
00:25:16,583 --> 00:25:20,220 line:-2
There are single-shot raycasts
which return a one-time result,


459
00:25:20,287 --> 00:25:23,624 line:-2
and then there are tracked raycasts
which continuously update the results


460
00:25:23,690 --> 00:25:26,226 line:-2
as ARKit's
understanding of the world evolves.


461
00:25:27,227 --> 00:25:30,163 line:-2
In order to get the latest features
for object placement,


462
00:25:30,230 --> 00:25:33,166 line:-2
we're recommending migrating
to the raycasting API


463
00:25:33,233 --> 00:25:36,203 line:-1
as we deprecate hit-testing.


464
00:25:36,270 --> 00:25:39,373 line:-2
The code we see on the top
is extracted from a sample app


465
00:25:39,439 --> 00:25:41,408 line:-1
which uses hit-testing to place objects.


466
00:25:42,209 --> 00:25:45,612 line:-2
It performs a hit-test with three
different kinds of hit-test options,


467
00:25:45,679 --> 00:25:48,182 line:-2
and it is usually followed
by some custom heuristics


468
00:25:48,248 --> 00:25:51,218 line:-2
to filter those results
and figure out where to place the object.


469
00:25:51,285 --> 00:25:54,721 line:0
All of that can be replaced
with a few lines of raycasting code,


470
00:25:54,788 --> 00:25:56,390 line:0
like the one we see below,


471
00:25:56,456 --> 00:25:58,959 line:0
and ARKit will do the heavy lifting
under the hood


472
00:25:59,026 --> 00:26:02,596 line:0
to make sure that your virtual objects
always stay at the right place.


473
00:26:03,664 --> 00:26:06,400 line:-2
Raycasting makes it easier
than ever before


474
00:26:06,466 --> 00:26:10,037 line:-2
to precisely place virtual objects
in your ARKit applications.


475
00:26:10,103 --> 00:26:12,172 line:-1
Let's move over to Face Tracking.


476
00:26:13,207 --> 00:26:18,045 line:-2
Face Tracking allows you to detect faces
in your front-camera AR experience,


477
00:26:18,111 --> 00:26:19,847 line:-1
overlay virtual content on them


478
00:26:19,913 --> 00:26:23,317 line:-2
and animate facial expressions
in real time.


479
00:26:23,383 --> 00:26:27,788 line:-2
This is supported on all devices
with a TrueDepth camera.


480
00:26:27,855 --> 00:26:29,656 line:-1
Now, with ARKit 4,


481
00:26:29,723 --> 00:26:33,760 line:-2
Face Tracking support is extended
to devices without a TrueDepth camera


482
00:26:33,827 --> 00:26:37,598 line:-2
as long as they have
an Apple A12 Bionic processor or later.


483
00:26:39,199 --> 00:26:42,336 line:-2
This includes devices
without the TrueDepth camera,


484
00:26:42,402 --> 00:26:44,905 line:-1
such as the new iPhone SE.


485
00:26:44,972 --> 00:26:46,473 line:-1
Elements of Face Tracking,


486
00:26:46,540 --> 00:26:49,843 line:-2
such as face anchors,
face geometry and blend shapes,


487
00:26:49,910 --> 00:26:52,779 line:-2
will be available
on all supported devices.


488
00:26:52,846 --> 00:26:57,451 line:0
But captured depth data will be limited
to devices with a TrueDepth camera.


489
00:26:57,518 --> 00:26:59,520 line:-1
And that's ARKit 4.


490
00:27:00,721 --> 00:27:02,022 line:-1
With location anchors,


491
00:27:02,089 --> 00:27:05,926 line:0
you can now bring your AR experiences
onto the global scale.


492
00:27:05,993 --> 00:27:09,596 line:0
And we looked at how you can use the LiDAR
to build rich AR apps


493
00:27:09,663 --> 00:27:12,499 line:0
using the Scene Geometry
and the Depth API.


494
00:27:13,233 --> 00:27:15,669 line:0
There are exciting improvements
in raycasting


495
00:27:15,736 --> 00:27:19,306 line:0
to make object placement in AR
easier than ever before.


496
00:27:19,373 --> 00:27:20,374 line:0
And finally,


497
00:27:20,440 --> 00:27:23,410 line:0
Face Tracking is now supported
on a wider range of devices.


498
00:27:23,477 --> 00:27:24,611 line:-1
Thank you,


499
00:27:24,678 --> 00:27:26,914 line:-2
and we can't wait to check out
all the great apps


500
00:27:26,980 --> 00:27:29,149 line:-1
that you will build using ARKit 4.

