1
00:00:03,971 --> 00:00:06,540 line:-1
Hello and welcome to WWDC.


2
00:00:07,975 --> 00:00:11,011 line:0
My name's Saad,
and I'm from the RealityKit team.


3
00:00:11,078 --> 00:00:15,682 line:0
Last year we released RealityKit,
an AR-focused 3D engine.


4
00:00:15,983 --> 00:00:19,119 line:0
And since then, we've been working hard
to add many new features.


5
00:00:19,186 --> 00:00:21,288 line:-2
Today, I'm excited
to tell you about some of them.


6
00:00:21,822 --> 00:00:23,724 line:-1
To start off, we have Video Materials,


7
00:00:23,957 --> 00:00:28,128 line:-2
and Video Materials allow you
to use videos as materials in RealityKit.


8
00:00:28,996 --> 00:00:32,665 line:-2
Second is scene understanding
using the brand-new LiDAR sensor.


9
00:00:33,800 --> 00:00:36,336 line:-2
Thanks to ARKit processing data
from this new sensor,


10
00:00:36,403 --> 00:00:39,039 line:-2
we're able to bring the real world
into your virtual one.


11
00:00:39,940 --> 00:00:42,176 line:-2
As a result,
we have a huge set of features


12
00:00:42,242 --> 00:00:45,245 line:-2
that allow you to make your virtual
content interact with the real world.


13
00:00:46,246 --> 00:00:48,482 line:-1
Next is improved rendering debugging.


14
00:00:49,583 --> 00:00:51,885 line:-1
With rendering being such a huge pipeline,


15
00:00:51,952 --> 00:00:54,688 line:-2
we've added the ability to inspect
various properties


16
00:00:54,755 --> 00:00:56,290 line:-1
related to the rendering of your entity.


17
00:00:57,057 --> 00:01:00,394 line:-2
And the next two updates
are related to ARKit 4 integration.


18
00:01:01,161 --> 00:01:04,932 line:-2
ARKit has extended its face-tracking
support to work on more devices.


19
00:01:05,566 --> 00:01:08,602 line:-2
This means that now face anchors
will also work on more devices.


20
00:01:09,636 --> 00:01:11,772 line:-1
And ARKit has also added location anchors.


21
00:01:12,439 --> 00:01:15,108 line:-2
Location anchors
allow you to place AR content


22
00:01:15,175 --> 00:01:18,278 line:-2
at specific locations in the real world
using RealityKit.


23
00:01:19,046 --> 00:01:20,814 line:-1
With these new features in mind,


24
00:01:20,881 --> 00:01:24,651 line:-2
I want to now show you an experience
that you can now create using RealityKit.


25
00:01:25,419 --> 00:01:29,389 line:-2
This experience is built upon a problem
we face as developers every day.


26
00:01:29,990 --> 00:01:31,892 line:-1
When we code, what do we get?


27
00:01:32,292 --> 00:01:34,828 line:-1
We get bugs like this one.


28
00:01:35,596 --> 00:01:37,464 line:-1
And this is quite a simple-looking bug.


29
00:01:37,531 --> 00:01:40,100 line:-2
And in real life,
bugs are never this simple.


30
00:01:40,567 --> 00:01:45,138 line:-2
So, let's add some complexity to it
using our new Video Materials feature.


31
00:01:45,472 --> 00:01:48,609 line:-2
The bug now has a glowing body
as well as glittering in its eyes.


32
00:01:49,543 --> 00:01:52,446 line:-2
So what does a bug normally do?
Well, it hides.


33
00:01:52,846 --> 00:01:55,516 line:-2
As the bug runs to hide,
the tree occludes the bug.


34
00:01:56,316 --> 00:02:00,187 line:-2
But it can't hide forever.
You know you'll fix it.


35
00:02:00,254 --> 00:02:03,090 line:-2
But until then, you know
it's gonna try to run away from you.


36
00:02:03,156 --> 00:02:06,493 line:-2
And using scene understanding,
we can implement this logic.


37
00:02:08,862 --> 00:02:12,099 line:-2
Once you finally catch it,
you want to make sure you fix it for good.


38
00:02:12,599 --> 00:02:15,769 line:-1
So, let's pick it up and crush it...


39
00:02:15,836 --> 00:02:17,137 line:-1
into the pavement.


40
00:02:19,907 --> 00:02:21,408 line:-1
And because of scene understanding,


41
00:02:21,475 --> 00:02:24,411 line:-2
the bug disintegrates
when it collides against the real world.


42
00:02:25,078 --> 00:02:26,547 line:-1
And that's it for the experience.


43
00:02:27,714 --> 00:02:32,252 line:-2
Now let's take a deep dive into these
new features and see how you can use them.


44
00:02:33,720 --> 00:02:35,355 line:-1
We'll start with Video Materials.


45
00:02:36,957 --> 00:02:39,226 line:-2
We saw from the experience
that Video Materials


46
00:02:39,293 --> 00:02:43,363 line:-2
were used to give the bug a glowing effect
as well as glittering in its eyes.


47
00:02:43,830 --> 00:02:45,699 line:-1
Now let's have a look at the video.


48
00:02:47,100 --> 00:02:50,838 line:-2
On the left, you can see the video
associated with the texture on the bug.


49
00:02:50,904 --> 00:02:53,807 line:-2
You can see how the eyes map on,
and you can also see


50
00:02:53,874 --> 00:02:57,211 line:-2
how we're able to share
one portion of the texture for both eyes.


51
00:02:58,011 --> 00:03:00,447 line:-2
You can also see
how the body is mapped on.


52
00:03:00,514 --> 00:03:04,284 line:-2
As the glow pulses in the texture,
you can see it pulse through the body.


53
00:03:04,785 --> 00:03:06,053 line:-1
So in a nutshell,


54
00:03:06,119 --> 00:03:09,723 line:-2
Video Materials allow you to have textures
that change over time.


55
00:03:10,224 --> 00:03:12,559 line:-1
They get these textures from a video.


56
00:03:13,560 --> 00:03:15,062 line:-1
You can use this for a lot of things.


57
00:03:15,829 --> 00:03:18,198 line:-2
You can use it
to simulate the glow effect you saw.


58
00:03:18,665 --> 00:03:21,401 line:-2
You can also use it
to provide video instructions.


59
00:03:21,468 --> 00:03:24,137 line:-2
Simply create a plane
and map a video onto it.


60
00:03:25,472 --> 00:03:28,675 line:-2
You can even combine ARKit image anchors
with Video Materials


61
00:03:28,742 --> 00:03:30,244 line:-1
to bring images to life.


62
00:03:31,245 --> 00:03:35,048 line:-2
But that's not all.
Video Materials also play audio.


63
00:03:35,449 --> 00:03:36,950 line:-1
Spatialized audio.


64
00:03:38,018 --> 00:03:40,787 line:-2
When you apply a video material
onto an entity,


65
00:03:40,854 --> 00:03:44,658 line:-2
that entity becomes a spatialized
audio source for your video.


66
00:03:44,725 --> 00:03:46,860 line:-1
Spatialized audio is when the sound source


67
00:03:46,927 --> 00:03:49,630 line:-2
acts as if it's emitted
from a specific location.


68
00:03:49,930 --> 00:03:52,232 line:-1
In this case, from the entity.


69
00:03:52,599 --> 00:03:54,968 line:-2
This helps build
a more immersive experience.


70
00:03:56,637 --> 00:03:59,339 line:-2
Now, because all of this is done
under the hood,


71
00:03:59,406 --> 00:04:02,109 line:-2
when you apply the material,
you don't have to do extra work


72
00:04:02,176 --> 00:04:05,612 line:-2
like synchronization
or doing your own manual audio playback.


73
00:04:06,413 --> 00:04:09,783 line:-2
So now that you know what they are,
how do we use them?


74
00:04:12,186 --> 00:04:13,654 line:-1
The flow is quite simple.


75
00:04:14,354 --> 00:04:16,390 line:-1
First, we need to load the video.


76
00:04:16,723 --> 00:04:20,160 line:-2
RealityKit leverages the power
of AVFoundation's AVPlayer


77
00:04:20,226 --> 00:04:21,928 line:-1
to use as a video source.


78
00:04:22,529 --> 00:04:26,767 line:-2
You load the video using AVFoundation
and create an AVPlayer object.


79
00:04:27,534 --> 00:04:32,172 line:-2
Once you have an AVPlayer object, you can
then use it to create a video material.


80
00:04:32,906 --> 00:04:36,343 line:-2
This video material is just like
any other RealityKit material.


81
00:04:36,577 --> 00:04:39,413 line:-2
You can assign it on any entity
that you want to use it on.


82
00:04:40,480 --> 00:04:42,883 line:-1
So let's see how these steps map to code.


83
00:04:43,517 --> 00:04:46,153 line:-2
First,
we use AVFoundation to load the video.


84
00:04:47,120 --> 00:04:50,591 line:-2
Next, we create a VideoMaterial
using an AVPlayer object


85
00:04:50,657 --> 00:04:52,059 line:-1
and assign it to the bugEntity.


86
00:04:54,027 --> 00:04:57,064 line:-2
Once we've assigned the material,
we can then play the video.


87
00:04:58,265 --> 00:05:01,468 line:-2
Now, video can be controlled
as you would through AVPlayer,


88
00:05:01,535 --> 00:05:03,770 line:-2
and this is quite a simple example
of video playback.


89
00:05:04,838 --> 00:05:08,742 line:-2
However, since RealityKit's Video Material
uses AVPlayer,


90
00:05:08,809 --> 00:05:12,546 line:-2
you get all of the great functionality
and features that AVFoundation brings.


91
00:05:14,014 --> 00:05:15,382 line:-1
For example,


92
00:05:15,449 --> 00:05:21,455 line:-2
you can use AVPlayer to directly play,
pause and seek inside your media.


93
00:05:21,522 --> 00:05:25,692 line:-2
This allows you to use a video atlas
instead of having one video per texture.


94
00:05:26,894 --> 00:05:30,464 line:-2
You can use AVPlayer properties
to trigger state transitions in your app.


95
00:05:31,164 --> 00:05:33,367 line:-1
For example, when your video finishes,


96
00:05:33,433 --> 00:05:35,435 line:-2
you can use this
to go to the next stage of your app.


97
00:05:37,037 --> 00:05:40,407 line:-2
You can use AVPlayerLooper
to play one video in a loop,


98
00:05:40,474 --> 00:05:45,112 line:-2
and you can use AVQueuePlayer
to sequentially play a queue of videos.


99
00:05:45,179 --> 00:05:50,284 line:-2
And lastly, you can even play remote media
served using HTTP Live Streaming.


100
00:05:50,350 --> 00:05:54,454 line:0
If you want to learn more,
see the "Advances in AVFoundation" session


101
00:05:54,521 --> 00:05:57,124 line:0
in WWDC 2016 for more information.


102
00:05:58,959 --> 00:06:00,460 line:-1
This concludes Video Materials.


103
00:06:00,994 --> 00:06:04,665 line:-2
To summarize,
Video Materials allow you to use videos


104
00:06:04,731 --> 00:06:07,901 line:-2
as a texture source
and spatial audio source.


105
00:06:07,968 --> 00:06:11,338 line:-2
Let's move on to our next big feature,
scene understanding.


106
00:06:12,873 --> 00:06:15,509 line:-1
Scene understanding has one main goal.


107
00:06:16,376 --> 00:06:19,980 line:-2
Its goal is to make virtual content
interact with the real world.


108
00:06:20,581 --> 00:06:24,117 line:-2
To achieve this, we want to bring
everything in the real world


109
00:06:24,184 --> 00:06:25,752 line:-1
into your virtual one.


110
00:06:26,420 --> 00:06:28,288 line:-1
So let's see how we can do this.


111
00:06:29,523 --> 00:06:31,725 line:-1
We start in the ARView.


112
00:06:32,059 --> 00:06:35,562 line:-2
The ARView has a list of settings
related to the real world


113
00:06:35,629 --> 00:06:37,364 line:-1
under the environment struct.


114
00:06:37,764 --> 00:06:40,501 line:-2
These settings configure
the background image,


115
00:06:40,567 --> 00:06:43,704 line:-2
environment-based lighting,
as well as spatial audio options.


116
00:06:43,770 --> 00:06:46,773 line:-2
And we consider the real world
to be part of your environment.


117
00:06:47,274 --> 00:06:50,210 line:-2
As a result, we've added
a new scene understanding option set.


118
00:06:50,944 --> 00:06:53,146 line:-1
This new option set lets you configure


119
00:06:53,213 --> 00:06:55,682 line:-2
how the real world interacts
with your virtual.


120
00:06:56,183 --> 00:06:57,651 line:-1
It has four options.


121
00:06:58,285 --> 00:07:00,521 line:-1
First, we have Occlusion.


122
00:07:01,154 --> 00:07:04,124 line:-2
This means real-world objects
occlude virtual objects.


123
00:07:04,958 --> 00:07:07,494 line:-1
Second is Receives lighting.


124
00:07:07,561 --> 00:07:11,198 line:0
This allows virtual objects
to cast shadows on real-world surfaces.


125
00:07:12,399 --> 00:07:14,268 line:-1
Third is Physics.


126
00:07:14,868 --> 00:07:17,237 line:-1
This enables the act of virtual objects


127
00:07:17,304 --> 00:07:19,506 line:-2
physically interacting
with the real world.


128
00:07:20,274 --> 00:07:23,310 line:-1
And last but not least, we have Collision.


129
00:07:23,377 --> 00:07:25,779 line:-2
This enables the generation
of collision events


130
00:07:25,846 --> 00:07:28,615 line:-2
as well as the ability to ray cast
against the real world.


131
00:07:29,816 --> 00:07:33,887 line:-2
One thing to know: Receives lighting
automatically turns on Occlusion,


132
00:07:33,954 --> 00:07:37,124 line:-2
and likewise,
Physics automatically turns on Collision.


133
00:07:38,158 --> 00:07:41,295 line:-2
Let's have a look at these options
in a bit more detail.


134
00:07:41,695 --> 00:07:44,097 line:-1
We'll start with Occlusion.


135
00:07:45,465 --> 00:07:48,035 line:-1
On the left, without Occlusion,


136
00:07:48,101 --> 00:07:50,237 line:-2
you can see that the bug
is always visible.


137
00:07:52,005 --> 00:07:56,343 line:-2
On the right, with Occlusion,
the bug is hidden behind the tree.


138
00:07:58,679 --> 00:08:00,814 line:-2
You can see how Occlusion helps
with realism


139
00:08:00,881 --> 00:08:03,550 line:-2
by hiding virtual objects
behind real ones.


140
00:08:04,785 --> 00:08:06,153 line:-1
To use Occlusion,


141
00:08:06,220 --> 00:08:09,656 line:-2
simply add .occlusion
into your sceneUnderstanding.options.


142
00:08:11,024 --> 00:08:13,760 line:-1
Our next option was Receives lighting.


143
00:08:14,895 --> 00:08:17,130 line:-1
On the left, without Receives lighting,


144
00:08:17,197 --> 00:08:20,534 line:-2
the bug looks like it's floating
because of the lack of shadows.


145
00:08:21,535 --> 00:08:24,238 line:-2
And now on the right,
with Receives lighting,


146
00:08:24,304 --> 00:08:27,007 line:-2
the bug casts the shadow
and looks grounded.


147
00:08:28,642 --> 00:08:31,245 line:-1
To use shadows, like Occlusion,


148
00:08:31,311 --> 00:08:34,581 line:-2
simply add .receivesLighting
into your options set.


149
00:08:35,849 --> 00:08:39,318 line:-2
These new shadows are similar
to the shadows that you're used to seeing


150
00:08:39,385 --> 00:08:41,522 line:-1
on horizontally anchored objects.


151
00:08:41,587 --> 00:08:44,725 line:-2
However,
because we're using real-world surfaces,


152
00:08:44,791 --> 00:08:47,928 line:-2
you no longer have to anchor these objects
to horizontal planes.


153
00:08:48,495 --> 00:08:51,632 line:-2
This means that any entity
that you have in the virtual world


154
00:08:51,698 --> 00:08:53,901 line:-1
will cast a shadow onto the real world.


155
00:08:54,368 --> 00:08:55,736 line:-1
It's important to note, though,


156
00:08:55,802 --> 00:08:58,739 line:-2
that the shadows imitate
a light shining straight down.


157
00:08:59,072 --> 00:09:01,175 line:-1
This means you won't see shadows on walls.


158
00:09:01,475 --> 00:09:04,912 line:-2
For those shadows, you still have
to anchor your entity vertically.


159
00:09:06,180 --> 00:09:09,082 line:-2
We can now move on to our third option,
Physics.


160
00:09:11,351 --> 00:09:15,522 line:-2
Previously, for AR experiences,
you were just limited to using planes


161
00:09:15,589 --> 00:09:17,958 line:-2
or various primitives
to represent your world.


162
00:09:18,358 --> 00:09:21,795 line:-2
This is unrealistic because the real world
doesn't conform to these shapes.


163
00:09:22,262 --> 00:09:24,698 line:-2
Now,
because of scene understanding physics,


164
00:09:24,998 --> 00:09:26,900 line:-2
you can see
that when the bug disintegrates,


165
00:09:26,967 --> 00:09:30,470 line:-2
the small pieces bounce off the steps
and scatter all around.


166
00:09:33,073 --> 00:09:36,443 line:-2
To use Physics,
you add the .physics option


167
00:09:36,510 --> 00:09:38,779 line:-1
into your sceneUnderstanding.options set.


168
00:09:39,246 --> 00:09:41,682 line:-2
There's a couple of specifics
you should know, however.


169
00:09:42,149 --> 00:09:47,120 line:-2
First, we consider real-world objects
to be static, with infinite mass.


170
00:09:47,821 --> 00:09:51,391 line:-2
This means that they're not movable,
as you'd expect in real life.


171
00:09:52,726 --> 00:09:56,063 line:-2
Second,
these meshes are constantly updating.


172
00:09:56,563 --> 00:09:59,433 line:-2
This means that you should never
expect objects to stay still,


173
00:09:59,499 --> 00:10:01,969 line:-1
especially on nonplanar surfaces.


174
00:10:03,937 --> 00:10:08,509 line:-2
Third, the reconstructed mesh is limited
to regions where the user has scanned.


175
00:10:08,575 --> 00:10:11,111 line:-2
This means that if the user
has never scanned the floor,


176
00:10:11,178 --> 00:10:13,213 line:-1
there will be no floor in your scene,


177
00:10:13,280 --> 00:10:15,716 line:-2
and as a result,
objects will fall right through.


178
00:10:16,283 --> 00:10:17,684 line:-1
You should design your experience


179
00:10:17,751 --> 00:10:20,721 line:-2
to make sure that the user
has scanned the room before starting.


180
00:10:22,155 --> 00:10:25,826 line:-2
And fourth, the mesh is an approximation
of the real world.


181
00:10:26,059 --> 00:10:30,564 line:-2
While the Occlusion mask is very accurate,
the mesh for Physics is less so.


182
00:10:31,131 --> 00:10:34,401 line:-2
As a result, don't expect the mesh
to have super crisp edges,


183
00:10:34,468 --> 00:10:36,703 line:-1
as you can see in the image on the right.


184
00:10:37,437 --> 00:10:41,975 line:0
Lastly, if you do use Physics,
collaborative sessions are not supported.


185
00:10:42,042 --> 00:10:44,044 line:0
Collaborative sessions will still work,


186
00:10:44,111 --> 00:10:46,213 line:0
just with the other scene understanding
options.


187
00:10:47,314 --> 00:10:50,350 line:-2
Let's now look at our last option,
Collision.


188
00:10:51,118 --> 00:10:53,153 line:-1
Collision is a bigger topic.


189
00:10:53,220 --> 00:10:54,955 line:-1
But before we get into that,


190
00:10:55,022 --> 00:10:58,292 line:-2
to use Collision,
similar to the previous three options,


191
00:10:58,358 --> 00:11:01,795 line:-2
just insert .collision
into your sceneUnderstanding.options.


192
00:11:02,896 --> 00:11:06,767 line:-2
The reason Collision is a bigger topic
is because it has two use cases.


193
00:11:07,334 --> 00:11:09,236 line:-1
The first is ray casting.


194
00:11:09,636 --> 00:11:14,174 line:-2
Ray casting is a super powerful tool.
You can use it to do a lot of things,


195
00:11:14,241 --> 00:11:17,477 line:-2
such as pathfinding,
initial object placement,


196
00:11:17,544 --> 00:11:20,480 line:-2
line-of-sight testing
and keeping things on meshes.


197
00:11:20,547 --> 00:11:23,550 line:-1
The next use case is collision events.


198
00:11:23,617 --> 00:11:26,620 line:-2
We want to do some action
when we detect a collision


199
00:11:26,687 --> 00:11:29,723 line:-2
between a real-world object
and a virtual one.


200
00:11:30,457 --> 00:11:31,825 line:-1
So let's recap.


201
00:11:32,259 --> 00:11:35,395 line:-2
Let's see how we used the two use cases
in our experience.


202
00:11:35,462 --> 00:11:37,231 line:-1
We'll start by looking at ray casting.


203
00:11:39,633 --> 00:11:43,504 line:-2
You can see how the bug stays on the tree
and finds different points to visit.


204
00:11:43,871 --> 00:11:46,707 line:-2
Ray casting allows us
to keep the bug on the tree


205
00:11:46,773 --> 00:11:49,610 line:-2
by ray casting from the body
towards the tree


206
00:11:49,676 --> 00:11:51,545 line:-1
to find the closest contact point.


207
00:11:51,979 --> 00:11:55,949 line:-2
And ray casting is also used to find
different points for the bug to visit.


208
00:11:56,617 --> 00:11:59,620 line:-2
And let's look at the next case,
collision events.


209
00:12:01,555 --> 00:12:03,190 line:-1
By using collision events,


210
00:12:03,257 --> 00:12:07,227 line:-2
we were able to detect when the bug
is tossed and hits the real world.


211
00:12:07,794 --> 00:12:10,731 line:-1
When it does, we make it disintegrate.


212
00:12:12,966 --> 00:12:17,638 line:-2
So now that we've seen these two use cases
and how they helped build the experience,


213
00:12:17,704 --> 00:12:19,673 line:-2
there's still one more thing
you need to know about using them.


214
00:12:21,441 --> 00:12:24,344 line:-1
Ray casting returns a list of entities,


215
00:12:24,411 --> 00:12:27,147 line:-2
and collision events happen
between two entities.


216
00:12:27,714 --> 00:12:32,653 line:-2
This means we need an entity, an entity
that corresponds to a real-world object.


217
00:12:33,020 --> 00:12:36,857 line:-2
As a result, we're introducing
the scene understanding entity.


218
00:12:38,091 --> 00:12:40,894 line:-2
A scene understanding entity
is just an entity,


219
00:12:41,562 --> 00:12:44,531 line:-2
an entity
that consists of various components.


220
00:12:45,232 --> 00:12:48,769 line:-2
It contains a Transform component,
a Collision component


221
00:12:48,836 --> 00:12:50,537 line:-1
and a Physics component,


222
00:12:50,604 --> 00:12:54,241 line:-2
and these are automatically added
based on your scene understanding options.


223
00:12:54,808 --> 00:12:56,577 line:-1
There's still one more component,


224
00:12:57,177 --> 00:12:59,146 line:-2
and that is
the SceneUnderstandingComponent.


225
00:13:00,180 --> 00:13:03,617 line:-2
The SceneUnderstandingComponent
is unique to a scene understanding entity.


226
00:13:04,284 --> 00:13:08,255 line:-2
It is what makes a scene understanding
entity a scene understanding entity.


227
00:13:08,922 --> 00:13:10,958 line:-1
When looking for a real-world object


228
00:13:11,024 --> 00:13:13,827 line:-2
in a collision event
or ray-casting results,


229
00:13:13,894 --> 00:13:16,396 line:-2
you simply need to find an entity
with this component.


230
00:13:18,031 --> 00:13:20,300 line:-1
And similarly, with this new component,


231
00:13:20,734 --> 00:13:23,070 line:-2
we have a new
HasSceneUnderstanding trait.


232
00:13:23,136 --> 00:13:25,839 line:-2
All scene understanding entities
will conform to this trait.


233
00:13:27,007 --> 00:13:29,977 line:-2
It's really important to realize
that these entities


234
00:13:30,043 --> 00:13:32,613 line:-1
are created and managed by RealityKit.


235
00:13:33,146 --> 00:13:35,148 line:-2
You should consider these entities
read-only


236
00:13:35,215 --> 00:13:37,451 line:-2
and not modify properties
on their components.


237
00:13:37,684 --> 00:13:40,254 line:-1
Doing so can result in undefined behavior.


238
00:13:40,988 --> 00:13:43,991 line:-2
So now that we have scene
understanding entities in our toolbox


239
00:13:44,057 --> 00:13:45,993 line:-1
and we know how to find them,


240
00:13:46,059 --> 00:13:50,330 line:-2
let's look at how we can use them in code
for the collision use cases we saw.


241
00:13:50,764 --> 00:13:52,666 line:-1
We'll start with ray-casting.


242
00:13:54,368 --> 00:13:55,636 line:-1
In this sample,


243
00:13:55,702 --> 00:13:59,506 line:-2
we'll look at implementing some simple
object avoidance for our bug entity.


244
00:14:00,507 --> 00:14:03,143 line:-2
We start by figuring out
where the bug is looking


245
00:14:03,644 --> 00:14:05,078 line:-1
and get a corresponding ray.


246
00:14:06,346 --> 00:14:10,017 line:-2
Once we have this ray,
we can then do a RealityKit ray cast.


247
00:14:10,851 --> 00:14:15,155 line:-2
The ray cast returns a list containing
both virtual and real-world entities.


248
00:14:15,722 --> 00:14:17,691 line:-1
We only want the real-world entities.


249
00:14:17,758 --> 00:14:20,460 line:-2
Thus, we filter using
the HasSceneUnderstanding trait.


250
00:14:21,695 --> 00:14:23,463 line:-1
And with these filtered results,


251
00:14:23,530 --> 00:14:26,033 line:-2
we know that the first entity
into the results


252
00:14:26,099 --> 00:14:27,868 line:-1
is the closest real-world object.


253
00:14:29,002 --> 00:14:32,072 line:0
Then finally,
we can do some simple object avoidance


254
00:14:32,139 --> 00:14:35,642 line:0
by looking at the distance to
the nearest object and taking an action.


255
00:14:35,709 --> 00:14:38,045 line:-1
Maybe we go right, maybe we go left.


256
00:14:39,513 --> 00:14:41,782 line:-2
So now let's move on
to another code example,


257
00:14:41,849 --> 00:14:44,117 line:-1
a code example using collision events.


258
00:14:46,053 --> 00:14:48,355 line:-1
In this sample, we'll look at implementing


259
00:14:48,422 --> 00:14:51,391 line:-2
something similar to the bug
disintegration we saw in the experience.


260
00:14:52,492 --> 00:14:56,096 line:-2
We start by subscribing
to collision events between all entities.


261
00:14:57,798 --> 00:14:59,633 line:-1
When we get a callback for a collision,


262
00:14:59,700 --> 00:15:02,503 line:-2
we need to figure out
if it was with a real-world object.


263
00:15:03,170 --> 00:15:06,940 line:-2
We need to see if either entity
is a sceneUnderstandingEntity


264
00:15:07,007 --> 00:15:08,809 line:-2
by using
the HasSceneUnderstanding trait.


265
00:15:10,244 --> 00:15:13,313 line:-2
If neither of these entities
conform to this trait,


266
00:15:13,380 --> 00:15:16,283 line:-2
then, well, we know that we don't have
a real-world collision.


267
00:15:16,917 --> 00:15:19,586 line:-2
If, however,
one entity does conform to this trait,


268
00:15:19,653 --> 00:15:21,788 line:-2
then we know that we have
a real-world collision


269
00:15:21,855 --> 00:15:23,857 line:-1
and the other entity is the bug.


270
00:15:24,892 --> 00:15:28,362 line:-2
We can then make the bug disintegrate
by doing an asset swap.


271
00:15:29,530 --> 00:15:32,165 line:-2
This leads to one question
that I want to discuss.


272
00:15:32,866 --> 00:15:35,969 line:-2
We can react when objects collide
against the real world,


273
00:15:36,303 --> 00:15:38,739 line:-2
but what if we don't want them
to collide at all?


274
00:15:39,640 --> 00:15:42,543 line:-2
And to do this,
we need to use collision filters.


275
00:15:42,609 --> 00:15:45,345 line:-2
And collision filters
need collision groups.


276
00:15:46,446 --> 00:15:49,650 line:-2
If we want objects not to collide
with the real world,


277
00:15:49,716 --> 00:15:52,953 line:-2
we need to filter them out using
the collision group for the real world.


278
00:15:54,354 --> 00:15:57,090 line:-2
And as a result,
we're introducing a new collision group


279
00:15:57,157 --> 00:16:00,060 line:-2
called SceneUnderstanding
for real-world objects.


280
00:16:00,794 --> 00:16:03,630 line:-2
To use this group,
set your collision filter


281
00:16:04,164 --> 00:16:06,266 line:-2
to include or not include
SceneUnderstanding


282
00:16:06,567 --> 00:16:08,368 line:-1
to filter the collision appropriately.


283
00:16:09,903 --> 00:16:12,773 line:-2
And because RealityKit manages
scene understanding entities,


284
00:16:13,407 --> 00:16:15,709 line:-2
it automatically sets
the collision groups on them,


285
00:16:15,776 --> 00:16:17,511 line:-1
so you don't have to manually do this.


286
00:16:18,846 --> 00:16:22,249 line:-2
That covers all of the collision-related
aspects of scene understanding.


287
00:16:22,816 --> 00:16:25,552 line:-2
Using the all of the scene understanding
features I talked about,


288
00:16:25,619 --> 00:16:27,588 line:-1
you can now create a great app.


289
00:16:27,654 --> 00:16:31,124 line:-2
However, once you start building your app,
you might run into some issues.


290
00:16:31,525 --> 00:16:34,962 line:-2
And then you're not sure if it's the mesh
that's the problem or your logic.


291
00:16:36,396 --> 00:16:40,200 line:-2
So to help with this, we've added
the ability to visualize the mesh.


292
00:16:41,001 --> 00:16:42,002 line:-1
Let's take a look.


293
00:16:43,937 --> 00:16:47,441 line:-2
Here you can see a video with
the debug mesh visualization turned on.


294
00:16:47,941 --> 00:16:50,744 line:-1
This shows us the raw real-world mesh.


295
00:16:51,812 --> 00:16:54,681 line:-2
The mesh is color-coded
by the distance away from the camera.


296
00:16:55,916 --> 00:16:58,719 line:-2
You can also see how this mesh
is constantly updating


297
00:16:58,785 --> 00:17:02,122 line:-2
and the reconstructed mesh
is an approximation of the real world.


298
00:17:02,189 --> 00:17:05,459 line:-2
You can see how the mesh is not crisp
at the edges of the stairs.


299
00:17:07,060 --> 00:17:08,729 line:-1
To enable this visualization,


300
00:17:08,795 --> 00:17:13,099 line:-2
add the .showSceneUnderstanding option
into the ARView's debug options.


301
00:17:14,468 --> 00:17:18,338 line:-2
The colors you saw in the video
are color-coded using the chart below.


302
00:17:18,405 --> 00:17:20,973 line:-2
You can see that the color
varies by distance,


303
00:17:21,040 --> 00:17:23,544 line:-2
and past five meters,
everything is colored white.


304
00:17:24,944 --> 00:17:29,216 line:0
It's important to note that this option
shows you the raw real-world mesh.


305
00:17:29,850 --> 00:17:31,852 line:0
If you want to see the physics mesh,


306
00:17:31,919 --> 00:17:35,556 line:0
then you just turn on the regular physics
debug view that's already available.


307
00:17:36,890 --> 00:17:38,959 line:-2
And that covers everything
scene understanding.


308
00:17:39,026 --> 00:17:43,096 line:-2
As you can tell, there's a lot of cool
stuff you can do with scene understanding.


309
00:17:43,163 --> 00:17:45,199 line:-1
I want to summarize some key takeaways.


310
00:17:46,500 --> 00:17:48,802 line:-2
The first is the goal
of scene understanding.


311
00:17:49,536 --> 00:17:51,004 line:-1
The goal of scene understanding


312
00:17:51,071 --> 00:17:53,841 line:-2
is to make your virtual content
interact with the real world.


313
00:17:55,542 --> 00:17:58,846 line:-2
And one set of these interactions
is through occlusion and shadows.


314
00:17:59,780 --> 00:18:03,450 line:-2
Real-world objects will occlude
and receive shadows from virtual ones.


315
00:18:05,152 --> 00:18:08,288 line:-2
Another set of interactions
is through physics and collision.


316
00:18:09,323 --> 00:18:12,392 line:-2
Physics lets objects
physically interact with the real world.


317
00:18:13,160 --> 00:18:14,628 line:-1
Collision, on the other hand,


318
00:18:14,695 --> 00:18:17,130 line:-2
lets you know
when objects physically collide,


319
00:18:17,764 --> 00:18:20,567 line:-2
and it also enables you
to ray-cast against the real world.


320
00:18:23,103 --> 00:18:26,807 line:-2
Real-world objects have corresponding
scene understanding entities.


321
00:18:27,641 --> 00:18:31,178 line:-2
These entities can be identified
using the HasSeenUnderstanding trait.


322
00:18:31,879 --> 00:18:34,181 line:-2
It's important to remember
that these entities


323
00:18:34,248 --> 00:18:38,151 line:-2
are created and managed by RealityKit
and they should be considered read-only.


324
00:18:40,487 --> 00:18:43,557 line:-2
And lastly,
we have debug mesh visualization.


325
00:18:43,624 --> 00:18:46,393 line:-2
This allows you to view
the raw real-world mesh.


326
00:18:47,327 --> 00:18:48,829 line:-1
And that's it for scene understanding.


327
00:18:49,463 --> 00:18:51,598 line:-2
We can now move on
to our next major feature:


328
00:18:51,665 --> 00:18:53,367 line:-1
improved rendering debugging.


329
00:18:56,069 --> 00:18:59,406 line:-2
Rendering is a huge pipeline
with lots of small components.


330
00:19:00,140 --> 00:19:03,310 line:-2
This includes model loading,
material setup,


331
00:19:03,377 --> 00:19:05,279 line:-1
scene lighting, and much more.


332
00:19:06,280 --> 00:19:08,482 line:-2
And to help debug
rendering-related issues,


333
00:19:08,549 --> 00:19:11,652 line:-2
we've added the ability
to inspect various properties


334
00:19:11,718 --> 00:19:13,520 line:-1
related to the rendering of your entity.


335
00:19:14,488 --> 00:19:17,391 line:-2
Let me show you an example of what I mean
by looking at our bug.


336
00:19:18,559 --> 00:19:20,861 line:-2
Let's have a look
at its base color texture.


337
00:19:22,229 --> 00:19:23,397 line:-1
How about its normal map?


338
00:19:24,264 --> 00:19:25,632 line:-1
Or its texture coordinates?


339
00:19:26,567 --> 00:19:30,237 line:-2
So, we can see these properties,
but how do they actually help us?


340
00:19:31,171 --> 00:19:34,908 line:-2
They can help us because you can
look at normals and texture coordinates


341
00:19:34,975 --> 00:19:37,010 line:-2
to ensure that your model
was loaded correctly.


342
00:19:37,778 --> 00:19:40,714 line:-2
This is especially important
if you find a model off the Internet


343
00:19:40,781 --> 00:19:42,950 line:-1
and are having issues with its rendering.


344
00:19:43,016 --> 00:19:44,651 line:-1
Maybe the model was just bad.


345
00:19:45,652 --> 00:19:47,921 line:-2
If you're using a simple material
on an entity


346
00:19:47,988 --> 00:19:49,790 line:-1
and setting the base color, roughness


347
00:19:49,857 --> 00:19:52,259 line:-2
or metallic parameter
and things don't look right,


348
00:19:52,759 --> 00:19:55,863 line:-2
you can inspect those parameters
to verify that they are set correctly.


349
00:19:57,030 --> 00:20:00,133 line:-2
And finally,
you can use PBR-related outputs,


350
00:20:00,200 --> 00:20:03,770 line:-2
such as diffuse lighting received
or specular lighting received


351
00:20:03,837 --> 00:20:06,106 line:-2
to know how much to tweak
your material parameters.


352
00:20:07,241 --> 00:20:10,277 line:-2
Now, to visualize these properties,
we've added a new component,


353
00:20:11,211 --> 00:20:12,679 line:-1
the DebugModelComponent.


354
00:20:14,014 --> 00:20:17,451 line:-2
To enable the visualization of a property,
simply choose a property,


355
00:20:18,285 --> 00:20:20,854 line:-2
create a DebugModelComponent
using the property,


356
00:20:20,921 --> 00:20:22,689 line:-1
and then assign it to your entity.


357
00:20:23,490 --> 00:20:25,893 line:-2
You can choose
from a huge list of properties.


358
00:20:25,959 --> 00:20:28,061 line:-1
Currently, we have 16.


359
00:20:29,763 --> 00:20:34,468 line:-2
These can be grouped as vertex attributes,
material parameters and PBR outputs.


360
00:20:37,905 --> 00:20:39,840 line:-1
Finally, one last thing to note:


361
00:20:39,907 --> 00:20:43,210 line:-2
The visualization only applies
to the targeted entity


362
00:20:43,277 --> 00:20:45,112 line:-1
and is not inherited by its children.


363
00:20:45,812 --> 00:20:49,683 line:0
USDz files may have multiple entities
with varying hierarchy.


364
00:20:50,217 --> 00:20:52,452 line:0
As a result, you need to add the component


365
00:20:52,519 --> 00:20:55,022 line:0
to each and every entity
that you want to inspect.


366
00:20:56,557 --> 00:20:58,825 line:-2
And that's it.
Hopefully, with this component


367
00:20:58,892 --> 00:21:01,929 line:-2
you're able to iterate and debug
rendering problems much faster.


368
00:21:03,697 --> 00:21:06,567 line:-2
That covers everything related
to improved rendering debugging.


369
00:21:07,234 --> 00:21:10,771 line:-2
We've also covered all of the features
that are RealityKit-specific.


370
00:21:12,506 --> 00:21:16,777 line:-2
Let's move on to our next section:
integration with ARKit 4.


371
00:21:18,478 --> 00:21:20,781 line:-1
ARKit 4 has many updates this year.


372
00:21:20,848 --> 00:21:23,350 line:-2
There's two updates
that relate to RealityKit.


373
00:21:24,084 --> 00:21:25,953 line:-1
The first is Face Tracking.


374
00:21:26,653 --> 00:21:28,121 line:-1
Support for Face Tracking


375
00:21:28,188 --> 00:21:31,458 line:-2
is now extended to devices
without a TrueDepth camera,


376
00:21:31,925 --> 00:21:34,228 line:-2
as long as they have
an A12 processor or later.


377
00:21:34,294 --> 00:21:37,097 line:-2
This includes new devices,
such as the iPhone SE.


378
00:21:38,198 --> 00:21:40,033 line:-1
As a RealityKit developer,


379
00:21:40,100 --> 00:21:42,603 line:-1
if you were using Face Anchors before,


380
00:21:42,669 --> 00:21:46,173 line:-2
whether you created them using code
or Reality Composer,


381
00:21:46,240 --> 00:21:49,376 line:-2
your applications should now work
without any changes.


382
00:21:50,644 --> 00:21:52,246 line:-1
The second ARKit feature


383
00:21:53,013 --> 00:21:54,381 line:-1
is Location Anchors.


384
00:21:55,115 --> 00:21:59,219 line:-2
Location Anchors lets you create anchors
using real-world coordinates.


385
00:22:00,053 --> 00:22:01,955 line:-1
ARKit takes these coordinates


386
00:22:02,022 --> 00:22:04,591 line:-2
and coverts them to locations
relative to your device.


387
00:22:06,260 --> 00:22:08,629 line:-2
This means that you can now place
AR content


388
00:22:08,695 --> 00:22:10,998 line:-1
at specific locations in the real world.


389
00:22:11,965 --> 00:22:15,269 line:-2
As a RealityKit developer,
to use Location Anchors,


390
00:22:15,369 --> 00:22:18,105 line:-2
you create an anchor entity
using a Location Anchor.


391
00:22:19,206 --> 00:22:22,142 line:-2
Any virtual content
that you anchor under this entity


392
00:22:22,376 --> 00:22:23,944 line:-1
will show at the specific location.


393
00:22:26,013 --> 00:22:28,682 line:-2
And creating an anchor entity
is also super simple.


394
00:22:29,683 --> 00:22:34,488 line:-2
Since ARKit's new ARGeoAnchor class
is just a subclass of ARAnchor,


395
00:22:35,122 --> 00:22:37,991 line:-2
you can call the already existing
anchor initializer


396
00:22:38,058 --> 00:22:39,393 line:-1
to create an anchor entity.


397
00:22:40,861 --> 00:22:43,997 line:-2
Now, one thing to note
is that ARKit also introduced


398
00:22:44,064 --> 00:22:47,367 line:-2
a new AR geotracking configuration
for Location Anchors.


399
00:22:49,136 --> 00:22:51,572 line:-2
And since this is not
a world tracking configuration,


400
00:22:51,638 --> 00:22:53,240 line:-1
you need to manually configure


401
00:22:53,307 --> 00:22:55,676 line:-2
and start a Location Anchor session
in the ARView.


402
00:22:56,743 --> 00:22:59,546 line:-2
This also means certain features,
like scene understanding,


403
00:22:59,613 --> 00:23:01,648 line:-1
that I just talked about, will not work.


404
00:23:03,116 --> 00:23:04,618 line:-1
If you want to learn more,


405
00:23:04,685 --> 00:23:07,354 line:-2
I suggest checking out
the "Introducing ARKit 4" talk.


406
00:23:08,121 --> 00:23:10,591 line:-2
It will go over
how to set up your session,


407
00:23:10,657 --> 00:23:13,861 line:-2
how to create anchors,
and how to use them with RealityKit.


408
00:23:15,262 --> 00:23:18,599 line:-2
And that summarizes the main things
you need to know about ARKit 4


409
00:23:18,665 --> 00:23:21,502 line:-2
and its integration with RealityKit
for iOS 14.


410
00:23:23,871 --> 00:23:28,008 line:-2
This also concludes all of the features
I want to talk to you about this year.


411
00:23:28,642 --> 00:23:30,544 line:-1
And there were a lot of them,


412
00:23:30,611 --> 00:23:32,746 line:-2
so I want to summarize
everything we've learned so far.


413
00:23:35,482 --> 00:23:37,417 line:-1
We started with Video Materials.


414
00:23:37,651 --> 00:23:40,420 line:-1
Video Materials allow you to use videos


415
00:23:40,487 --> 00:23:43,156 line:-2
as a texture and spatial
audio source on your entity.


416
00:23:44,124 --> 00:23:46,093 line:-1
This can be used for a lot of things,


417
00:23:46,159 --> 00:23:49,029 line:-2
such as sprite-like effects
or instructional videos.


418
00:23:50,531 --> 00:23:54,234 line:-2
Next, thanks to scene understanding,
using the brand new LiDAR sensor,


419
00:23:55,169 --> 00:23:58,172 line:-2
we're able to bring the real world
into your virtual one


420
00:23:58,238 --> 00:24:02,009 line:-2
through many ways, such as occlusion,
shadows and physics.


421
00:24:02,776 --> 00:24:06,747 line:-2
You can now leverage ray-casting
to implement smart character behavior


422
00:24:06,813 --> 00:24:08,382 line:-1
to have it react with the real world.


423
00:24:08,982 --> 00:24:12,252 line:-2
And you can also use collision events
to implement collision responses


424
00:24:12,319 --> 00:24:14,321 line:-2
when your objects collide
with the real world.


425
00:24:15,389 --> 00:24:19,126 line:0
And then our next update was an improved
ability to debug rendering problems.


426
00:24:19,726 --> 00:24:21,662 line:0
With our new DebugModelComponent,


427
00:24:21,728 --> 00:24:24,831 line:0
you can inspect any
rendering-related property on your entity,


428
00:24:25,465 --> 00:24:28,335 line:0
such as vertex attributes,
material parameters


429
00:24:28,402 --> 00:24:30,037 line:0
and PBR-related outputs.


430
00:24:31,004 --> 00:24:33,207 line:0
We then had ARKit-related integrations.


431
00:24:34,541 --> 00:24:38,579 line:0
ARKit extended Face Tracking support
on devices without TrueDepth camera.


432
00:24:39,413 --> 00:24:42,916 line:0
This also means Face Anchors
will work on a lot more devices,


433
00:24:42,983 --> 00:24:46,386 line:0
like the new iPhone SE,
without requiring any code changes.


434
00:24:48,322 --> 00:24:51,725 line:0
And finally, ARKit added Location Anchors.


435
00:24:51,792 --> 00:24:54,995 line:0
Location Anchors allow you
to place AR content


436
00:24:55,062 --> 00:24:58,198 line:0
at specific locations in the real world
using RealityKit.


437
00:24:58,899 --> 00:25:00,334 line:0
And that concludes my talk.


438
00:25:01,101 --> 00:25:02,536 line:-1
Thank you for your time.


439
00:25:02,603 --> 00:25:04,605 line:-2
I can't wait to see
what incredible content


440
00:25:04,671 --> 00:25:06,340 line:-1
you will make using these new features.

