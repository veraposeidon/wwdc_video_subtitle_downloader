1
00:00:03,737 --> 00:00:07,207 line:-1
Hello and welcome to WWDC.


2
00:00:08,442 --> 00:00:12,412 line:0
Hello. My name is Lizi,
and I'm a machine learning manager


3
00:00:12,479 --> 00:00:14,248 line:0
on the Create ML team.


4
00:00:14,314 --> 00:00:17,851 line:0
Today I'm thrilled to introduce
a set of new APIs


5
00:00:17,918 --> 00:00:21,288 line:0
for checkpointing
and asynchronous training in Create ML.


6
00:00:22,055 --> 00:00:24,057 line:-1
To make sure we're all on the same page,


7
00:00:24,124 --> 00:00:28,028 line:-2
let's talk about the Create ML app
and the Create ML framework.


8
00:00:28,095 --> 00:00:30,497 line:-1
The Create ML app is great.


9
00:00:30,564 --> 00:00:34,368 line:-2
If you want to easily create models
without a single line of code,


10
00:00:34,434 --> 00:00:36,036 line:-1
I encourage you to start there.


11
00:00:37,171 --> 00:00:41,975 line:-2
Under the hood, the app is powered
by a rich and easy-to-use API:


12
00:00:42,042 --> 00:00:44,244 line:-1
the Create ML framework.


13
00:00:44,311 --> 00:00:49,583 line:-2
For macOS Big Sur, we're bringing you
a set of powerful and flexible APIs


14
00:00:49,650 --> 00:00:51,985 line:-1
for controlling your training workflow.


15
00:00:52,052 --> 00:00:56,623 line:-2
These will enable you to customize the way
you create machine learning models


16
00:00:56,690 --> 00:01:00,327 line:-2
and leverage all the same technology
that powers the app.


17
00:01:00,827 --> 00:01:04,531 line:-2
With that, let's start by reviewing
the training process.


18
00:01:06,266 --> 00:01:07,668 line:-1
If you want to train a model,


19
00:01:07,734 --> 00:01:10,237 line:-2
the first thing you have to do
is collect some data.


20
00:01:11,638 --> 00:01:15,576 line:-2
You also want to look at
all the training options available to you.


21
00:01:15,642 --> 00:01:17,177 line:-1
If you are using Create ML,


22
00:01:17,244 --> 00:01:20,280 line:-2
you can always accept
the default parameter settings,


23
00:01:20,347 --> 00:01:23,283 line:-2
but it doesn't hurt to understand
what the options mean.


24
00:01:25,586 --> 00:01:27,754 line:-1
Next, preprocessing.


25
00:01:27,821 --> 00:01:30,190 line:-1
Some tasks require extracting features


26
00:01:30,257 --> 00:01:33,260 line:-2
or transforming data
into the right structure.


27
00:01:33,327 --> 00:01:37,698 line:-2
For example, raw audio may be transformed
by taking its spectrogram


28
00:01:37,764 --> 00:01:41,201 line:-2
or video may have keypoints
extracted from every frame.


29
00:01:42,236 --> 00:01:45,439 line:-1
Then, training. This is the fun part.


30
00:01:45,506 --> 00:01:47,875 line:-2
And we'll spend some time
looking at this in depth,


31
00:01:47,941 --> 00:01:49,743 line:-1
so let's leave it at that for now.


32
00:01:51,545 --> 00:01:55,516 line:-2
Next, we evaluate the model
by presenting it with new data.


33
00:01:55,582 --> 00:01:58,185 line:-2
Here, it's important to use
a different set of data


34
00:01:58,252 --> 00:02:00,654 line:-2
that the model hasn't seen
during training.


35
00:02:00,721 --> 00:02:03,490 line:-2
This will help you determine
if your model is generalizing


36
00:02:03,557 --> 00:02:05,893 line:-1
instead of memorizing training examples.


37
00:02:06,793 --> 00:02:10,964 line:-2
At this point, you may realize that
your model is not quite what you expect.


38
00:02:11,031 --> 00:02:12,799 line:-1
Maybe it needs more data.


39
00:02:12,866 --> 00:02:15,335 line:-2
Maybe it needs a different set
of training parameters.


40
00:02:15,402 --> 00:02:18,305 line:-2
Or maybe it just needs
to train a bit longer.


41
00:02:18,939 --> 00:02:21,208 line:-1
Model development is an iterative process,


42
00:02:21,275 --> 00:02:24,878 line:-2
so you can return to setup or train
until your model meets your needs.


43
00:02:27,548 --> 00:02:28,815 line:-1
Once you are satisfied,


44
00:02:28,882 --> 00:02:31,885 line:-2
you can use your trained model
to perform predictions.


45
00:02:31,952 --> 00:02:34,621 line:-1
At this point, you're ready to deploy.


46
00:02:35,422 --> 00:02:39,359 line:-2
With the training process in mind,
let's talk about control.


47
00:02:39,426 --> 00:02:44,398 line:-2
Training control is mainly
having the ability to pause and resume,


48
00:02:44,464 --> 00:02:49,303 line:-2
but also the ability to observe and alter
the training process while it's happening.


49
00:02:50,637 --> 00:02:52,739 line:-1
And you can stop training at any point.


50
00:02:54,975 --> 00:02:56,977 line:-1
Why is this important?


51
00:02:57,044 --> 00:02:59,546 line:-2
Training a machine learning model
takes time.


52
00:02:59,613 --> 00:03:04,151 line:-2
In particular, training an object
detection model can take five hours.


53
00:03:04,218 --> 00:03:07,988 line:-2
Imagine you were training
and after several hours, training stops


54
00:03:08,055 --> 00:03:10,591 line:-2
because it reached
the maximum number of iterations.


55
00:03:12,092 --> 00:03:14,795 line:-2
But then you realize
that the loss was still decreasing,


56
00:03:14,862 --> 00:03:17,798 line:-2
and there was still room
to converge to a better model.


57
00:03:18,365 --> 00:03:21,134 line:-2
Without training control,
you would have to start from scratch


58
00:03:21,201 --> 00:03:24,238 line:-2
and choose a larger number
of maximum iterations.


59
00:03:24,304 --> 00:03:27,274 line:-2
But now you can continue
from where training stopped.


60
00:03:29,643 --> 00:03:31,311 line:-1
Another reason this is important


61
00:03:31,378 --> 00:03:34,748 line:-2
is results are specific
to what you are trying to achieve.


62
00:03:34,815 --> 00:03:37,417 line:-2
Using a built-in mechanism
to evaluate the model


63
00:03:37,484 --> 00:03:41,588 line:-2
will not necessarily give the best results
for your specific use-case.


64
00:03:42,155 --> 00:03:44,625 line:0
This is particularly true
in Style Transfer


65
00:03:44,691 --> 00:03:47,194 line:0
where the quality of a model
is subjective.


66
00:03:47,261 --> 00:03:50,964 line:-2
For instance, here, the model is applying
the fish mosaic style


67
00:03:51,031 --> 00:03:53,200 line:-1
to the photo of the dancers.


68
00:03:53,267 --> 00:03:55,636 line:-2
It's up to you to decide
when this is good enough


69
00:03:55,702 --> 00:03:57,037 line:-1
and stop training.


70
00:03:57,571 --> 00:04:01,808 line:-2
Also, choosing when to stop
is specific to your needs.


71
00:04:01,875 --> 00:04:03,810 line:-1
Maybe you need a model for a prototype


72
00:04:03,877 --> 00:04:06,113 line:-2
and don't want to spend
too much time on it.


73
00:04:06,180 --> 00:04:08,015 line:-1
Or maybe this is an important model


74
00:04:08,081 --> 00:04:10,250 line:-2
and you are willing
to spend extra time training


75
00:04:10,317 --> 00:04:12,286 line:-1
to achieve the best result possible.


76
00:04:13,220 --> 00:04:17,224 line:-2
Using training control,
you can define custom stopping criteria,


77
00:04:17,291 --> 00:04:20,494 line:-2
meaning you are not limited
to the number of iterations.


78
00:04:20,560 --> 00:04:23,597 line:-2
You may have a target accuracy
for an image classifier,


79
00:04:23,664 --> 00:04:26,767 line:-2
or you may have a specific amount of time
you can allow for training.


80
00:04:28,202 --> 00:04:31,705 line:-2
Training control is available
in the Create ML app.


81
00:04:31,772 --> 00:04:35,742 line:-2
But with the API, you can customize
this behavior to your needs.


82
00:04:35,809 --> 00:04:36,910 line:-1
Let me show you.


83
00:04:38,912 --> 00:04:42,916 line:-2
Before, you would call
a model constructor, time would stop,


84
00:04:42,983 --> 00:04:44,818 line:-1
and a trained model would be returned.


85
00:04:46,420 --> 00:04:48,455 line:-1
With the new asynchronous API,


86
00:04:48,522 --> 00:04:51,458 line:-2
you can now call the train method
which returns a job.


87
00:04:53,760 --> 00:04:56,797 line:-2
The train method
takes the usual training parameters


88
00:04:56,864 --> 00:04:59,833 line:-2
as well as a sessionParameters argument
which lets you specify


89
00:04:59,900 --> 00:05:03,370 line:-2
the cadence of reports
and total number of iterations.


90
00:05:03,437 --> 00:05:06,373 line:-2
You can always omit the arguments
to use the defaults.


91
00:05:08,008 --> 00:05:11,812 line:-2
The first argument to sessionParameters
is the sessionDirectory.


92
00:05:11,879 --> 00:05:15,449 line:-2
This directory will be created
if it doesn't already exist.


93
00:05:15,516 --> 00:05:18,318 line:-2
In subsequent invocations,
the contents of the directory


94
00:05:18,385 --> 00:05:21,221 line:-1
will be used to resume a previous session.


95
00:05:21,288 --> 00:05:22,756 line:-1
If you want to start a new session,


96
00:05:22,823 --> 00:05:25,626 line:-2
simply delete the directory
or choose a new location.


97
00:05:26,860 --> 00:05:31,431 line:-2
The next two arguments control the cadence
of progress reports and checkpoints.


98
00:05:31,498 --> 00:05:34,034 line:-2
Smaller intervals
will give more frequent updates,


99
00:05:34,101 --> 00:05:35,969 line:-1
but may come at a performance cost.


100
00:05:37,271 --> 00:05:40,741 line:-2
The last argument
is the total number of iterations.


101
00:05:40,807 --> 00:05:43,043 line:-2
Training will stop
at this iteration number.


102
00:05:43,110 --> 00:05:45,712 line:-2
But don't worry,
you can always increase this number


103
00:05:45,779 --> 00:05:47,347 line:-1
in subsequent calls to train,


104
00:05:47,414 --> 00:05:51,151 line:-2
and you can always stop before
this number of iterations is reached.


105
00:05:52,419 --> 00:05:56,023 line:-2
The job returned from train
is an instance of MLJob


106
00:05:56,089 --> 00:05:58,458 line:-1
which represents an active training job.


107
00:05:58,859 --> 00:06:02,129 line:-2
It contains a progress publisher,
a checkpoint publisher


108
00:06:02,196 --> 00:06:03,830 line:-1
and a result publisher.


109
00:06:04,932 --> 00:06:08,268 line:0
These leverage the Combine framework,
which you can learn more about


110
00:06:08,335 --> 00:06:13,240 line:0
in "Introducing Combine"
and "Combine in Practice" from WWDC19.


111
00:06:14,708 --> 00:06:17,211 line:-1
MLJob also provides a cancel method


112
00:06:17,277 --> 00:06:20,347 line:-2
so you can stop training easily
at any point.


113
00:06:21,048 --> 00:06:24,418 line:-2
To give you an idea, here is how
you would use the result publisher


114
00:06:24,484 --> 00:06:26,687 line:-1
to get a trained model.


115
00:06:26,753 --> 00:06:30,057 line:-2
First, you register a sink
on the result publisher.


116
00:06:30,123 --> 00:06:34,862 line:-2
Its first closure is invoked on completion
and receives a success or failure result.


117
00:06:35,696 --> 00:06:38,165 line:-2
It also receives an error
in the failure case,


118
00:06:38,232 --> 00:06:39,766 line:-1
so you should handle errors here.


119
00:06:41,635 --> 00:06:43,971 line:-2
The second closure
receives the final model


120
00:06:44,037 --> 00:06:45,339 line:-1
when training is complete.


121
00:06:47,007 --> 00:06:50,410 line:-2
Finally, you store the subscription
until you are done with it.


122
00:06:53,013 --> 00:06:55,883 line:-2
You can observe progress
such as the fraction completed,


123
00:06:55,949 --> 00:06:59,753 line:-2
the current training iteration,
total number of iterations,


124
00:06:59,820 --> 00:07:03,490 line:-2
and custom metrics such as
the current loss or accuracy value.


125
00:07:05,192 --> 00:07:07,828 line:-2
For example,
you can observe and print progress


126
00:07:07,895 --> 00:07:10,964 line:-2
by accessing the progress publisher
on MLJob.


127
00:07:11,031 --> 00:07:15,302 line:-2
Note that this is the standard instance
of Progress from Foundation.


128
00:07:15,369 --> 00:07:20,007 line:-2
To access custom metrics,
we first register a sink closure.


129
00:07:20,073 --> 00:07:24,278 line:-2
As a precaution, we capture the job weakly
to avoid retain cycles.


130
00:07:25,913 --> 00:07:31,251 line:-2
Next, we ensure the job still exists
and create an instance of MLProgress.


131
00:07:31,318 --> 00:07:34,888 line:-2
The MLProgress helper type
allows us to access custom metrics


132
00:07:34,955 --> 00:07:37,658 line:-1
from an instance of Foundation Progress.


133
00:07:37,724 --> 00:07:40,694 line:-2
Note that there may be cases
where an instance of MLProgress


134
00:07:40,761 --> 00:07:42,429 line:-1
cannot be initialized yet,


135
00:07:42,496 --> 00:07:46,733 line:-2
such as when a new session is just created
or a new phase is starting.


136
00:07:46,800 --> 00:07:48,936 line:-1
We handle that by returning early.


137
00:07:50,671 --> 00:07:53,674 line:-2
Then we can access
all progress information


138
00:07:53,740 --> 00:07:56,844 line:-2
like the current item
and total number of items.


139
00:07:56,910 --> 00:08:00,080 line:-2
In feature extraction,
the item refers to the current file


140
00:08:00,147 --> 00:08:01,882 line:-1
or record being processed.


141
00:08:01,949 --> 00:08:06,153 line:-2
For training, it refers to
the current iteration number.


142
00:08:06,220 --> 00:08:09,756 line:-1
And finally, we can get training metrics.


143
00:08:09,823 --> 00:08:12,359 line:-2
Metrics are only available
in the training phase,


144
00:08:12,426 --> 00:08:14,661 line:-1
and are specific to each task.


145
00:08:14,728 --> 00:08:19,333 line:-2
Some tasks have accuracy
while others have loss or other metrics.


146
00:08:19,399 --> 00:08:21,168 line:-1
If you provided validation data,


147
00:08:21,235 --> 00:08:24,271 line:-2
you may also be able
to access validation metrics.


148
00:08:25,138 --> 00:08:27,875 line:-2
Now let's take a look
at how you can train a model


149
00:08:27,941 --> 00:08:30,077 line:-1
using the new asynchronous APIs.


150
00:08:31,512 --> 00:08:34,581 line:-2
I've been really interested in training
a few Style Transfer models


151
00:08:34,648 --> 00:08:38,719 line:-2
after watching "Build Image and Video
Style Transfer Models in Create ML,"


152
00:08:39,152 --> 00:08:43,357 line:-2
so I've started setting up
my model creation workflow in Playgrounds.


153
00:08:43,423 --> 00:08:48,595 line:-2
Here, I have a few different styles images
I can use in my resource bundle


154
00:08:48,662 --> 00:08:50,531 line:-1
and I also have some validation images


155
00:08:50,597 --> 00:08:53,600 line:-2
of different artists
that I'd like to stylize.


156
00:08:55,002 --> 00:08:58,438 line:-2
Back in my Playground,
I first specified URLs


157
00:08:58,505 --> 00:09:01,575 line:-2
to the style image and validation image
that I want to use.


158
00:09:03,443 --> 00:09:05,412 line:-1
One nice thing about Playgrounds


159
00:09:05,479 --> 00:09:09,550 line:-2
is it allows me
to visualize my data inline as I go.


160
00:09:09,616 --> 00:09:12,653 line:-2
For example, I can see the style
that I've designated


161
00:09:12,719 --> 00:09:14,555 line:-1
and validation image that we'll use.


162
00:09:16,123 --> 00:09:21,361 line:-2
The next thing I'll do is specify
a dataSource to define our training data


163
00:09:21,962 --> 00:09:25,199 line:-2
and I'll pass list the styleImageURL
and contentDirectory


164
00:09:26,633 --> 00:09:30,170 line:-2
and then proceed to initialize parameters
we need to set up our session.


165
00:09:32,940 --> 00:09:35,943 line:-2
I'll first initialize
MLTrainingSessionParameters


166
00:09:36,009 --> 00:09:38,612 line:0
that takes a sessionDirectory.


167
00:09:38,679 --> 00:09:40,681 line:0
I can also provide a reportInterval


168
00:09:40,747 --> 00:09:44,251 line:0
to specify the number of iterations
between progress reports,


169
00:09:44,318 --> 00:09:48,188 line:0
a checkpointInterval to specify the
number of iterations between checkpoints,


170
00:09:48,255 --> 00:09:50,090 line:0
and iterations to extend training.


171
00:09:51,692 --> 00:09:54,161 line:-1
Next, I'll set up my model parameters.


172
00:09:57,331 --> 00:09:58,966 line:-1
Now we're ready to begin training.


173
00:10:02,402 --> 00:10:04,104 line:-1
To start the training process,


174
00:10:04,171 --> 00:10:08,509 line:-2
I'll call the train method
on the type of model I want to create.


175
00:10:12,946 --> 00:10:15,249 line:-1
Here, I'll provide my dataSource,


176
00:10:16,783 --> 00:10:17,851 line:-1
trainingParameters...


177
00:10:22,523 --> 00:10:23,891 line:-1
and sessionParameters.


178
00:10:30,364 --> 00:10:32,399 line:-1
This returns an instance of MLJob.


179
00:10:33,734 --> 00:10:36,803 line:-1
MLJob contains a results publisher.


180
00:10:36,870 --> 00:10:40,607 line:-2
We can use the results publisher to
receive errors that occur during training


181
00:10:40,674 --> 00:10:43,277 line:-1
and access the model once we're done.


182
00:10:44,244 --> 00:10:48,248 line:-2
I can also use the job to control
and observe training while it's happening.


183
00:10:48,649 --> 00:10:52,052 line:-2
For example, if I want to access
progress programmatically,


184
00:10:52,119 --> 00:10:55,055 line:-2
I can do so
by using the progress publisher.


185
00:10:55,122 --> 00:10:57,691 line:-2
And since this is an instance
of Foundation Progress,


186
00:10:57,758 --> 00:11:01,461 line:-2
I'm going to specify
that we want the fractionCompleted


187
00:11:01,528 --> 00:11:03,797 line:-1
by providing a key path to that property.


188
00:11:05,132 --> 00:11:08,235 line:-2
Next, I'll call sink
which will return a cancellable


189
00:11:08,302 --> 00:11:11,305 line:-2
that we can later use
to terminate the subscription.


190
00:11:11,972 --> 00:11:13,540 line:-1
Beyond the fractionCompleted,


191
00:11:13,607 --> 00:11:18,378 line:-2
I can also access metrics specific
to the ML model I am trying to train.


192
00:11:18,445 --> 00:11:21,782 line:-2
To do this,
I'll initialize an instance of MLProgress


193
00:11:21,849 --> 00:11:24,651 line:-1
from the instance of Foundation Progress.


194
00:11:24,718 --> 00:11:26,954 line:-1
I can then access specific metrics


195
00:11:27,020 --> 00:11:32,593 line:-2
such as the style loss
and content loss across iterations.


196
00:11:33,126 --> 00:11:36,129 line:-2
And with the power of Xcode Playgrounds,
when I hit "train,"


197
00:11:36,196 --> 00:11:38,599 line:-2
I can visualize progress
as it's happening.


198
00:11:45,572 --> 00:11:49,943 line:-2
And in Playgrounds, I can view this
as the latest value or a graph


199
00:11:50,010 --> 00:11:52,980 line:-2
as it trains across iterations.
Pretty cool.


200
00:11:55,382 --> 00:11:59,086 line:-2
New this release is also the ability
to control the training process


201
00:11:59,152 --> 00:12:01,154 line:-1
with pausing, resuming and extending.


202
00:12:01,922 --> 00:12:06,193 line:-2
To stop, all I need to do
is call cancel on the job.


203
00:12:08,095 --> 00:12:11,598 line:-1
To resume, it's as simple as calling train


204
00:12:11,665 --> 00:12:14,468 line:-2
with the same session directory
as I used before.


205
00:12:16,003 --> 00:12:17,638 line:-1
It's almost automatic.


206
00:12:18,438 --> 00:12:22,576 line:-2
Now that you've seen how to train a model
using the new asynchronous APIs,


207
00:12:22,643 --> 00:12:24,845 line:-1
I want to introduce checkpoints.


208
00:12:24,912 --> 00:12:29,283 line:-2
With checkpoints, you can capture
the state of your model over time.


209
00:12:29,349 --> 00:12:31,785 line:-2
You can use them to see
how training progressed


210
00:12:31,852 --> 00:12:34,421 line:-2
and easily compare
against previous results.


211
00:12:34,855 --> 00:12:38,392 line:-2
Also, if you decide that a checkpoint
needs a bit more training,


212
00:12:38,458 --> 00:12:40,694 line:-2
you can use it to resume
where you left off.


213
00:12:42,930 --> 00:12:45,399 line:-1
MLCheckpoint is the struct that represents


214
00:12:45,465 --> 00:12:48,001 line:-2
either a training
or feature extraction checkpoint.


215
00:12:49,736 --> 00:12:53,774 line:-2
Unlike in other frameworks,
MLCheckpoints are created automatically


216
00:12:53,841 --> 00:12:56,009 line:-1
when using the new asynchronous APIs,


217
00:12:57,578 --> 00:12:59,513 line:-1
and they're easy to resume from.


218
00:12:59,580 --> 00:13:02,282 line:-2
All you need to do
is provide a sessionDirectory


219
00:13:02,349 --> 00:13:05,485 line:-2
where you are starting a new session
or continuing an existing one.


220
00:13:06,186 --> 00:13:08,655 line:-2
This is great in environments
such as Playgrounds


221
00:13:08,722 --> 00:13:11,658 line:-2
so you can iterate
without losing progress.


222
00:13:12,826 --> 00:13:15,996 line:-2
Let's go back to the training process
for a moment.


223
00:13:16,063 --> 00:13:20,734 line:-2
I want to zoom into two of these phases:
preprocessing and training.


224
00:13:20,801 --> 00:13:23,504 line:-2
These are the two phases
where checkpoints are generated.


225
00:13:24,805 --> 00:13:26,240 line:-1
Let's start with preprocessing.


226
00:13:28,308 --> 00:13:30,644 line:-1
In this phase, we take data elements,


227
00:13:30,711 --> 00:13:33,480 line:-2
such as individual files
or rows in a table,


228
00:13:33,547 --> 00:13:35,782 line:-1
and process each one of them.


229
00:13:35,849 --> 00:13:39,086 line:-2
Progress is measured by how many
of these elements have been processed.


230
00:13:41,522 --> 00:13:42,823 line:-1
Every few elements,


231
00:13:42,890 --> 00:13:45,659 line:-2
we store the current progress
as a checkpoint.


232
00:13:45,726 --> 00:13:48,161 line:-2
We only keep
the last preprocessing checkpoint


233
00:13:48,228 --> 00:13:50,264 line:-1
since this phase is strictly additive.


234
00:13:51,598 --> 00:13:54,902 line:-2
The training phase
is also an iterative process.


235
00:13:54,968 --> 00:13:59,039 line:-2
The model improves in discrete steps
called iterations.


236
00:13:59,106 --> 00:14:02,709 line:-2
An iteration consists of running
a batch of data through the network,


237
00:14:02,776 --> 00:14:05,245 line:-2
computing the loss
and updating the weights.


238
00:14:06,380 --> 00:14:10,417 line:-2
A training checkpoint is the state
of the model at a particular iteration.


239
00:14:10,484 --> 00:14:15,222 line:-2
Unlike preprocessing, training preserves
all checkpoints you create.


240
00:14:15,289 --> 00:14:16,657 line:-1
As training progresses,


241
00:14:16,723 --> 00:14:19,993 line:-2
we also get metrics
such as loss or accuracy.


242
00:14:20,060 --> 00:14:22,763 line:-2
These metrics are stored
along with the checkpoints


243
00:14:22,829 --> 00:14:24,231 line:-1
to help you identify them.


244
00:14:26,934 --> 00:14:28,802 line:-1
Let's look at some code.


245
00:14:28,869 --> 00:14:32,539 line:-2
Here is an example of how you would use
the checkpoints publisher.


246
00:14:32,606 --> 00:14:36,143 line:-2
All checkpoints are automatically saved
to the session folder.


247
00:14:36,210 --> 00:14:39,646 line:-2
But this publisher gives you a chance
of generating custom metrics,


248
00:14:39,713 --> 00:14:43,283 line:-2
generating a model,
or stopping training altogether.


249
00:14:43,350 --> 00:14:46,687 line:-2
It's a chance for you to act
on new checkpoints as they are created.


250
00:14:48,822 --> 00:14:53,026 line:-2
For instance, this is how you can generate
a model from a checkpoint.


251
00:14:53,093 --> 00:14:55,596 line:-2
Once you have a model,
you can save it to disk


252
00:14:55,662 --> 00:14:57,197 line:-1
or use it to make predictions.


253
00:14:58,899 --> 00:15:01,935 line:-2
Note that you can only create
a model from training checkpoints,


254
00:15:02,002 --> 00:15:05,539 line:-2
not feature extraction checkpoints,
so you should check its training phase.


255
00:15:07,708 --> 00:15:10,344 line:-2
You can make checkpoints
during the feature extraction phase


256
00:15:10,410 --> 00:15:12,946 line:-2
for image classification,
sound classification


257
00:15:13,013 --> 00:15:15,015 line:-1
and action classification.


258
00:15:15,082 --> 00:15:18,051 line:-2
Training checkpoints are available
for action classification,


259
00:15:18,118 --> 00:15:21,688 line:-2
object detection, Style Transfer
and activity classification.


260
00:15:22,623 --> 00:15:24,992 line:-1
Now let's talk about sessions.


261
00:15:25,058 --> 00:15:29,530 line:-2
Think of a session as the aggregate
of all checkpoints and their metadata.


262
00:15:29,596 --> 00:15:32,366 line:-2
This is represented
by the MLTrainingSession type.


263
00:15:32,432 --> 00:15:36,103 line:-2
It lets you query details
like when the session was created,


264
00:15:36,170 --> 00:15:39,039 line:-2
the current training phase,
and the current iteration number.


265
00:15:40,908 --> 00:15:43,143 line:-2
If you want to access
the session directly,


266
00:15:43,210 --> 00:15:46,680 line:-2
you can use the restoreTrainingSession
method on a model type.


267
00:15:48,215 --> 00:15:52,452 line:-2
From that, you can do things
like access the loss values.


268
00:15:52,519 --> 00:15:55,122 line:-2
You can also remove checkpoints
that you don't need anymore,


269
00:15:55,189 --> 00:15:56,757 line:-1
freeing up disk space.


270
00:15:57,324 --> 00:16:00,060 line:-2
Now let's take a look
at checkpoints in action.


271
00:16:02,629 --> 00:16:05,265 line:-2
We already saw how you could observe
progress and results


272
00:16:05,332 --> 00:16:06,900 line:-1
from an instance of MLJob


273
00:16:06,967 --> 00:16:10,003 line:-2
while training a Style Transfer model
in Playgrounds.


274
00:16:10,070 --> 00:16:11,972 line:-1
Back in our same Playground,


275
00:16:12,039 --> 00:16:15,175 line:-2
we can see that MLJob contains
a checkpoints publisher


276
00:16:15,242 --> 00:16:18,979 line:-2
that we can use
to observe checkpoints over time.


277
00:16:19,046 --> 00:16:20,914 line:-1
For Style Transfer, I can access


278
00:16:20,981 --> 00:16:24,151 line:-2
the stylized validation image
on each checkpoint


279
00:16:24,218 --> 00:16:27,888 line:-2
to help us visualize how our model
is progressing as it trains.


280
00:16:31,225 --> 00:16:33,627 line:-1
To do this, I can use compactMap


281
00:16:33,694 --> 00:16:37,264 line:-2
to transform the checkpoint
into a form we need


282
00:16:37,331 --> 00:16:40,234 line:-1
and then map the image URLs into NSImages.


283
00:16:41,502 --> 00:16:43,036 line:-1
I'm then going to call sink


284
00:16:43,103 --> 00:16:45,806 line:-2
to attach a subscriber
to the checkpoints publisher


285
00:16:45,873 --> 00:16:49,977 line:-2
and store the AnyCancellable
return type to our subscriptions list.


286
00:16:58,418 --> 00:17:01,288 line:-2
And now we can see
our stylized validation image


287
00:17:01,355 --> 00:17:03,223 line:-1
over time as the model trains,


288
00:17:04,057 --> 00:17:05,858 line:-1
directly inline.


289
00:17:06,760 --> 00:17:10,196 line:-2
But wouldn't it be great
if we could take things even further?


290
00:17:10,263 --> 00:17:12,266 line:-1
Well, now, using SwiftUI,


291
00:17:12,332 --> 00:17:15,801 line:-2
we can easily render more
in the Xcode Playgrounds Live View.


292
00:17:16,837 --> 00:17:20,540 line:-2
The first thing I'll do
is to find a new view object


293
00:17:20,607 --> 00:17:24,511 line:-2
and pass it images of our style image,
stylized validation image


294
00:17:24,578 --> 00:17:26,213 line:-1
and original validation


295
00:17:26,280 --> 00:17:30,450 line:-2
so we can compare
how our model is performing over time.


296
00:17:30,517 --> 00:17:35,556 line:-2
And since we're using UI,
I'll receive this on the main queue


297
00:17:35,622 --> 00:17:40,394 line:-2
and then I'll set the Playground Live View
so we can populate the results there.


298
00:17:44,631 --> 00:17:46,800 line:-1
And the Live View comes to life!


299
00:17:46,867 --> 00:17:49,536 line:-2
Now we can visualize our model
as it trains


300
00:17:49,603 --> 00:17:52,673 line:-2
along with our reference style
and the validation image,


301
00:17:52,739 --> 00:17:56,009 line:-2
thanks to the new checkpointing
and asynchronous training APIs.


302
00:17:57,978 --> 00:18:00,447 line:-1
Let's recap what we saw in this session.


303
00:18:00,514 --> 00:18:01,949 line:-1
We learned how to train a model


304
00:18:02,015 --> 00:18:05,986 line:-2
using the new asynchronous APIs
available in Create ML.


305
00:18:06,053 --> 00:18:08,455 line:-2
We learned about checkpoints
and how to use them


306
00:18:08,522 --> 00:18:11,024 line:-1
to generate models and resume training.


307
00:18:11,091 --> 00:18:14,228 line:-2
And lastly,
we saw how to use Combine publishers


308
00:18:14,294 --> 00:18:17,197 line:-2
to get progress reports
and handle results.


309
00:18:17,598 --> 00:18:20,000 line:-1
So where do you go from here?


310
00:18:20,067 --> 00:18:22,603 line:-2
I encourage you
to leverage these new tools


311
00:18:22,669 --> 00:18:25,739 line:-2
if you need to customize
your training workflow.


312
00:18:25,806 --> 00:18:29,710 line:-2
Training control allows you to control
not just your model quality,


313
00:18:29,776 --> 00:18:31,645 line:-1
but the entire training process,


314
00:18:31,712 --> 00:18:35,115 line:-2
whether you are developing models,
automating workflows


315
00:18:35,182 --> 00:18:38,852 line:-2
or creating models on demand
in your macOS apps.


316
00:18:38,919 --> 00:18:41,989 line:-1
Thank you, and enjoy the rest of WWDC.

