1
00:00:03,770 --> 00:00:06,807 line:-1
Hello and welcome to WWDC.


2
00:00:09,309 --> 00:00:12,079 line:0
Hello. My name is Takayuki Mizuno.


3
00:00:12,145 --> 00:00:14,982 line:0
I am a CoreMedia engineer at Apple.


4
00:00:15,048 --> 00:00:19,686 line:0
This session is about a new feature
of AVFoundation


5
00:00:19,753 --> 00:00:23,524 line:0
for writing fragmented MP4 files for HLS.


6
00:00:24,324 --> 00:00:28,595 line:-2
Here is a diagram
that shows a typical workflow for HLS.


7
00:00:29,496 --> 00:00:31,365 line:-1
There is a source material.


8
00:00:31,431 --> 00:00:35,536 line:-2
This may be a video on demand material
or live material.


9
00:00:36,303 --> 00:00:38,805 line:-1
There is a part that encodes media data.


10
00:00:39,506 --> 00:00:45,012 line:-2
There the video samples are encoded to,
for example, HEVC,


11
00:00:45,078 --> 00:00:49,116 line:-2
and the audio samples are encoded to,
for example, AAC.


12
00:00:50,551 --> 00:00:53,153 line:-1
Then there is a segmenter here...


13
00:00:54,421 --> 00:00:57,858 line:-2
which segments media data
in a specific format.


14
00:00:58,892 --> 00:01:01,628 line:-1
The segmenter also creates a playlist


15
00:01:01,695 --> 00:01:04,364 line:-2
that lists those segments
at the same time.


16
00:01:05,498 --> 00:01:10,037 line:-2
Finally, there is a web server
that hosts the content.


17
00:01:10,103 --> 00:01:14,241 line:-2
AVFoundation provides new features
that is useful


18
00:01:14,308 --> 00:01:17,211 line:-1
mainly for what the segmenter does.


19
00:01:18,212 --> 00:01:22,950 line:-2
AVFoundation has AVAssetWriter
to allow media authoring applications


20
00:01:23,016 --> 00:01:29,456 line:-2
to write media data to a new file
of a specified container type such as MP4.


21
00:01:29,523 --> 00:01:35,429 line:-2
We enhanced it to output the media data
in fragmented MP4 format for HLS.


22
00:01:36,864 --> 00:01:41,802 line:-2
Apple HLS supports other formats
such as MPEG-2 transport stream,


23
00:01:41,869 --> 00:01:44,771 line:-1
ADTS and MPEG audio.


24
00:01:44,838 --> 00:01:48,675 line:-2
But this new feature is specific
to fragmented MP4.


25
00:01:49,376 --> 00:01:51,512 line:-1
Here is an example of a use case.


26
00:01:52,646 --> 00:01:55,816 line:-2
My example is that the source is
video on demand material.


27
00:01:57,084 --> 00:02:03,090 line:-2
AVFoundation has AVAssetReader,
which is an object to read sample data


28
00:02:03,156 --> 00:02:04,625 line:-1
from a media file.


29
00:02:05,659 --> 00:02:11,398 line:-2
You read sample data from a source movie,
push the samples to AVAssetWriter


30
00:02:11,465 --> 00:02:14,168 line:-2
and write the fragmented
MP4 segment files.


31
00:02:15,702 --> 00:02:18,605 line:0
Another example
is that the source is live material.


32
00:02:19,606 --> 00:02:23,143 line:0
AVFoundation has AVCaptureVideoDataOutput


33
00:02:23,210 --> 00:02:25,913 line:0
and AVCaptureAudioDataOutput,


34
00:02:25,979 --> 00:02:29,583 line:0
which are objects to provide you
with captured sample data


35
00:02:29,650 --> 00:02:31,218 line:0
from a connected device.


36
00:02:32,319 --> 00:02:34,721 line:0
You receive sample data from a device,


37
00:02:35,389 --> 00:02:39,693 line:0
push the samples to AVAssetWriter and
write the fragmented MP4 segment files.


38
00:02:40,427 --> 00:02:43,197 line:-1
Fragmented MP4 is a streaming media format


39
00:02:43,263 --> 00:02:46,133 line:-1
based on ISO base media file format.


40
00:02:46,200 --> 00:02:52,673 line:-2
Apple HTTP Live Streaming has supported
fragmented MP4 since 2016.


41
00:02:53,340 --> 00:02:56,443 line:-2
Here's a basic diagram
of a regular MP4 file.


42
00:02:57,110 --> 00:02:59,046 line:-1
It starts with a file type box


43
00:02:59,112 --> 00:03:03,417 line:-2
that indicates which of the file formats
this file conforms to.


44
00:03:04,852 --> 00:03:08,222 line:-2
There is a movie box
that is organizing the information


45
00:03:08,288 --> 00:03:10,457 line:-1
about all that sample data,


46
00:03:11,058 --> 00:03:15,996 line:-2
then there is a media data box
that contains all that sample data.


47
00:03:16,396 --> 00:03:20,868 line:-2
The movie box contains the information
relevant to the entire sample


48
00:03:20,934 --> 00:03:23,003 line:-1
such as audio and video codecs.


49
00:03:24,705 --> 00:03:28,008 line:-2
It also contains references
to the sample data


50
00:03:28,075 --> 00:03:30,277 line:-1
such as location of the sample data.


51
00:03:31,178 --> 00:03:34,348 line:-2
The order of most of those boxes
is arbitrary.


52
00:03:35,816 --> 00:03:38,051 line:-1
The file type box has to come first,


53
00:03:38,118 --> 00:03:42,055 line:-2
but the other boxes
can come in pretty much any order.


54
00:03:42,589 --> 00:03:46,660 line:-2
If you have used AVAssetWriter,
you may know that AVAssetWriter has


55
00:03:46,727 --> 00:03:49,363 line:-1
a movieFragmentInterval property.


56
00:03:49,429 --> 00:03:54,067 line:-2
This specifies the frequency
at which movie fragments will be written.


57
00:03:55,102 --> 00:03:58,872 line:-2
The resulting movie
is what is called a fragmented movie.


58
00:03:59,406 --> 00:04:02,543 line:-2
Here is a basic diagram
of a fragmented movie file.


59
00:04:04,545 --> 00:04:07,347 line:-2
There are movie fragment boxes
in this movie


60
00:04:07,414 --> 00:04:12,486 line:-2
which referenced all the sample data
held in the media data box followed by it.


61
00:04:13,620 --> 00:04:17,958 line:-2
This format structure is useful,
for example, for live capture


62
00:04:18,024 --> 00:04:21,428 line:-2
since even if writing
is unexpectedly interrupted


63
00:04:21,495 --> 00:04:22,930 line:-1
by a crash or something,


64
00:04:23,830 --> 00:04:29,069 line:-2
data that is partially written
is still accessible and playable.


65
00:04:31,271 --> 00:04:34,408 line:-2
Please note
that this fragmented movie case,


66
00:04:34,474 --> 00:04:37,244 line:-1
if writing finishes successfully,


67
00:04:37,311 --> 00:04:41,782 line:-2
AVAssetWriter defragments the file
as the last step.


68
00:04:41,849 --> 00:04:44,451 line:-2
So, this ends up making
a regular movie file.


69
00:04:46,119 --> 00:04:49,790 line:-2
Here is a basic diagram
of a fragmented MP4 file.


70
00:04:50,724 --> 00:04:52,960 line:-1
The file type box comes first,


71
00:04:53,026 --> 00:04:56,430 line:-2
and the movie box comes
after the file type box.


72
00:04:56,496 --> 00:05:01,468 line:-2
Then there is a movie fragment box
which is followed by a media data box.


73
00:05:02,202 --> 00:05:05,038 line:-2
And after that,
a pair of movie fragment boxes


74
00:05:05,105 --> 00:05:07,374 line:-1
and the media data box continues.


75
00:05:07,441 --> 00:05:11,245 line:-2
The order of those boxes
should be like this.


76
00:05:11,311 --> 00:05:15,282 line:-2
The major difference between
fragmented movie and the fragmented MP4


77
00:05:15,349 --> 00:05:18,785 line:-2
is that the movie box
of the fragmented MP4


78
00:05:18,852 --> 00:05:23,290 line:-2
does not contain references
to the sample data.


79
00:05:23,357 --> 00:05:27,327 line:-2
It only contains the information relevant
to the entire samples,


80
00:05:27,394 --> 00:05:32,332 line:-2
and the order of the movie fragment box
and the media data box is reversed.


81
00:05:33,901 --> 00:05:36,970 line:-2
Now I'll talk
about how to use AVAssetWriter.


82
00:05:37,538 --> 00:05:41,208 line:-2
This shows how to create
an instance of AVAssetWriter.


83
00:05:41,942 --> 00:05:44,211 line:-1
You do not provide output URL,


84
00:05:44,278 --> 00:05:49,283 line:-2
since AVAssetWriter
does not write a file but outputs data.


85
00:05:49,349 --> 00:05:52,252 line:-2
You just have to specify
output content type.


86
00:05:52,319 --> 00:05:55,856 line:-1
Fragmented MP4 conforms to the MP4 family,


87
00:05:55,923 --> 00:06:01,562 line:-2
so you should always specify UTType
with AVFileType.MP4.


88
00:06:01,628 --> 00:06:03,897 line:-1
You create AVAssetWriterInput.


89
00:06:04,831 --> 00:06:09,002 line:-2
In this example, compressionSettings,
which is a dictionary,


90
00:06:09,069 --> 00:06:11,972 line:-1
is provided to encode the media samples.


91
00:06:12,039 --> 00:06:16,009 line:-2
Alternatively,
you can set nil to the outputSettings,


92
00:06:16,076 --> 00:06:18,145 line:-1
which is called passthrough mode.


93
00:06:18,212 --> 00:06:23,750 line:-2
In passthrough mode, AVAssetWriter
just writes the samples as it is given


94
00:06:23,817 --> 00:06:27,988 line:-2
and then adds the input
to the AVAssetWriter.


95
00:06:28,689 --> 00:06:31,725 line:-1
Here is how to configure AVAssetWriter.


96
00:06:31,792 --> 00:06:34,728 line:-1
You have to specify outputFileTypeProfile.


97
00:06:36,196 --> 00:06:40,701 line:-2
You should specify Apple HLS
or CMAF compliant profile


98
00:06:40,767 --> 00:06:44,404 line:-2
to output the media data
in fragmented MP4 format.


99
00:06:46,740 --> 00:06:49,743 line:-2
You specify
preferredOutputSegmentInterval.


100
00:06:50,777 --> 00:06:54,681 line:-2
AVAssetWriter outputs the media data
at that interval.


101
00:06:54,748 --> 00:07:01,221 line:-2
This interval time can be a rational time,
but target segment duration for HLS


102
00:07:01,288 --> 00:07:06,760 line:-2
must be an integer in seconds,
so we are not using greater precision here


103
00:07:06,827 --> 00:07:09,162 line:-1
because this is the right choice for HLS.


104
00:07:10,063 --> 00:07:13,267 line:-2
You also specify
initialSegmentStartTime...


105
00:07:14,334 --> 00:07:16,803 line:-2
which is the starting point
of the interval.


106
00:07:18,338 --> 00:07:23,844 line:-2
Then you specify a delegate object
that implements a designated protocol.


107
00:07:24,845 --> 00:07:27,548 line:-1
I will talk about the protocol later.


108
00:07:28,182 --> 00:07:33,487 line:-2
After this, the same process
as normal file writing continues.


109
00:07:34,721 --> 00:07:38,825 line:-2
You start writing, then append the samples
using AVAssetWriterInput.


110
00:07:40,093 --> 00:07:44,031 line:-2
I'm not actually going to talk
about this anymore in this session,


111
00:07:44,097 --> 00:07:50,771 line:0
but if you watch the WWDC session
"Working with Media in AV Foundation"


112
00:07:50,838 --> 00:07:56,310 line:0
in 2011, you will see
how you perform those processes.


113
00:07:56,376 --> 00:07:59,313 line:0
You can find the session video
on Apple's Developer site.


114
00:08:00,214 --> 00:08:03,016 line:-2
Here is the protocol
for the delegate methods.


115
00:08:03,083 --> 00:08:07,020 line:-2
There are two methods.
Both delegate methods deliver NSData


116
00:08:07,087 --> 00:08:08,856 line:-1
of a fragmented media data.


117
00:08:10,224 --> 00:08:13,126 line:-1
They also deliver AVAssetSegmentType...


118
00:08:14,161 --> 00:08:17,931 line:-2
that indicates a type
of the fragmented media data.


119
00:08:19,132 --> 00:08:21,435 line:-1
You should implement one of them.


120
00:08:21,502 --> 00:08:23,804 line:-1
The difference is that the second one


121
00:08:23,871 --> 00:08:27,207 line:-1
delivers an AVAssetSegmentReport object.


122
00:08:28,041 --> 00:08:32,279 line:-2
That object contains the information
about the fragmented media data.


123
00:08:32,346 --> 00:08:36,683 line:-2
If you do not need such information,
you should implement the first one.


124
00:08:37,784 --> 00:08:40,654 line:-1
AVAssetSegmentType has two types...


125
00:08:41,621 --> 00:08:43,857 line:-1
initialization and separable.


126
00:08:43,924 --> 00:08:48,028 line:-2
Fragmented media data
with type of initialization


127
00:08:48,095 --> 00:08:51,465 line:-2
contains the file type box
and the movie box.


128
00:08:52,199 --> 00:08:56,570 line:-2
Fragmented media data
with type of separable contains


129
00:08:56,637 --> 00:08:59,673 line:-2
one movie fragment box
and one media data box.


130
00:09:00,774 --> 00:09:05,445 line:-2
You will receive data
with a type of separable successfully.


131
00:09:12,319 --> 00:09:16,957 line:-2
You package the media data you received
into what is called a segment


132
00:09:17,024 --> 00:09:18,825 line:-1
and write the segment to files.


133
00:09:20,194 --> 00:09:22,663 line:-2
A pair of file type boxes
and the movie box...


134
00:09:23,797 --> 00:09:25,999 line:-1
should be an initialization segment.


135
00:09:27,201 --> 00:09:30,704 line:-2
And the pair of movie fragment boxes
and the media data box...


136
00:09:31,805 --> 00:09:33,340 line:-1
can be one segment.


137
00:09:35,576 --> 00:09:41,081 line:-2
For HLS, a single file that contains
all of those segments


138
00:09:41,148 --> 00:09:42,950 line:-1
can be used for streaming.


139
00:09:46,420 --> 00:09:50,591 line:-2
Or each segment can be divided
into multiple segment files.


140
00:09:52,926 --> 00:09:56,330 line:0
Moreover,
multiple pairs of movie fragment boxes


141
00:09:56,396 --> 00:09:59,700 line:0
and the media data boxes
can be one segment


142
00:09:59,766 --> 00:10:02,069 line:0
and stored in one segment file.


143
00:10:02,636 --> 00:10:04,738 line:-1
HLS uses Playlist.


144
00:10:04,805 --> 00:10:09,943 line:-2
Playlist is the centerpiece of how
the client finds out about the segments.


145
00:10:11,078 --> 00:10:13,046 line:-1
Here is an example of Playlist.


146
00:10:13,113 --> 00:10:16,950 line:-2
There are several tags
that indicate information about segments,


147
00:10:17,017 --> 00:10:19,353 line:-1
such as URL and the duration.


148
00:10:19,419 --> 00:10:22,289 line:-1
AVAssetWriter does not write the playlist.


149
00:10:22,356 --> 00:10:23,957 line:-1
You should write the playlist.


150
00:10:24,458 --> 00:10:29,663 line:-2
That is why one of the delegate methods
delivers AVAssetSegmentReport.


151
00:10:30,597 --> 00:10:34,434 line:-2
You can create the playlist
and the I-frame playlist


152
00:10:34,501 --> 00:10:38,372 line:-2
based on the information
AVAssetSegmentReport provides.


153
00:10:40,040 --> 00:10:43,777 line:-2
If you look at our sample code
called fmp4Writer,


154
00:10:43,844 --> 00:10:46,346 line:-1
you will see how to create the playlist.


155
00:10:47,114 --> 00:10:49,249 line:0
Also, the document about Playlist


156
00:10:49,316 --> 00:10:52,653 line:0
can be found here
at the Developer streaming page.


157
00:10:53,187 --> 00:10:57,524 line:-2
In passthrough mode, which is a mode
where a sample is not encoded,


158
00:10:57,591 --> 00:11:02,896 line:-2
AVAssetWriter outputs media data
that includes all the samples


159
00:11:02,963 --> 00:11:05,766 line:-1
just prior to the next sync sample


160
00:11:05,832 --> 00:11:10,237 line:-2
after the preferred output segment
interval has been reached or exceeded.


161
00:11:13,106 --> 00:11:18,245 line:-2
The sync sample here is a sample
that does not depend on another sample.


162
00:11:19,046 --> 00:11:25,285 line:-2
This is because CMAF requires that
every segment starts with a sync sample,


163
00:11:25,352 --> 00:11:28,555 line:-1
and Apple HLS also prefers this.


164
00:11:29,523 --> 00:11:32,659 line:-1
This rule applies not only to video,


165
00:11:32,726 --> 00:11:39,333 line:-2
but also to audio that has
sample dependencies, such as USAC audio.


166
00:11:39,833 --> 00:11:44,771 line:-2
Therefore, for passthrough,
only one AVAssetWriterInput can be added.


167
00:11:44,838 --> 00:11:48,609 line:-2
If sync samples are not laid out
near to the interval,


168
00:11:48,675 --> 00:11:50,878 line:-1
the media data may be output


169
00:11:50,944 --> 00:11:55,516 line:-2
in a considerably longer duration
than what is specified.


170
00:11:55,582 --> 00:11:59,286 line:-2
As a result,
it will be unsuitable for HLS.


171
00:11:59,353 --> 00:12:03,056 line:-2
One of the solutions is
to encode video samples at the same time.


172
00:12:03,957 --> 00:12:08,896 line:0
As I mentioned earlier, if you specify
output settings for compression


173
00:12:08,962 --> 00:12:12,566 line:0
in AVAssetWriterInput,
samples will be encoded.


174
00:12:12,633 --> 00:12:16,803 line:0
In encoding mode,
a video sample that just reaches


175
00:12:16,870 --> 00:12:20,307 line:0
or exceeds
the preferred output segment interval


176
00:12:20,374 --> 00:12:24,111 line:0
will be forced to be encoded
as a sync sample.


177
00:12:25,879 --> 00:12:29,750 line:0
In other words,
sync samples are automatically generated


178
00:12:29,816 --> 00:12:31,952 line:0
so that the fragmented media data


179
00:12:32,019 --> 00:12:36,323 line:0
will be output at or very close to
the specified interval.


180
00:12:36,390 --> 00:12:39,426 line:0
If you encode both audio and video,


181
00:12:39,493 --> 00:12:43,463 line:0
one AVAssetWriterInput for each media
can be added.


182
00:12:43,931 --> 00:12:46,900 line:-1
Earlier, I said that for passthrough,


183
00:12:46,967 --> 00:12:49,870 line:-2
only one AVAssetWriterInput
can be added.


184
00:12:50,370 --> 00:12:56,510 line:-2
But you can deliver video and audio as
separate streams using a master playlist.


185
00:12:56,977 --> 00:13:01,081 line:-2
You can deliver not just one pair
of audio and video streams,


186
00:13:01,148 --> 00:13:05,719 line:-2
but also streams at various bit rates
or in different languages.


187
00:13:07,254 --> 00:13:10,190 line:-2
You need to create multiple instances
of AVAssetWriter


188
00:13:10,257 --> 00:13:11,859 line:-1
to create multiple streams.


189
00:13:11,925 --> 00:13:15,295 line:-1
Again, the documents about master playlist


190
00:13:15,362 --> 00:13:19,199 line:-2
and recommendations
for encoding video and audio for HLS


191
00:13:19,266 --> 00:13:22,603 line:-2
can be found here
at the Developer streaming page.


192
00:13:23,704 --> 00:13:28,976 line:-2
This is an advanced use case,
but if you require different segmentation


193
00:13:29,042 --> 00:13:32,212 line:-1
than preferredOutputSegmentInterval does,


194
00:13:32,279 --> 00:13:34,414 line:-1
you can do it on your own.


195
00:13:34,481 --> 00:13:37,851 line:-2
For example,
you may want to output media data


196
00:13:37,918 --> 00:13:42,489 line:-2
not at a sync sample
after an interval has been reached,


197
00:13:42,556 --> 00:13:45,259 line:-1
but before the interval has been reached.


198
00:13:45,325 --> 00:13:50,631 line:-2
Here is the code I showed you earlier
that set properties of AVAssetWriter.


199
00:13:51,365 --> 00:13:53,567 line:-1
To signal custom segmentation,


200
00:13:53,634 --> 00:13:58,005 line:-2
set preferredOutputSegmentInterval
to CMTime indefinite.


201
00:13:58,071 --> 00:14:01,808 line:-2
It is not necessary
to set initialSegmentStartTime


202
00:14:01,875 --> 00:14:03,710 line:-1
since there is not an interval.


203
00:14:03,777 --> 00:14:05,345 line:-1
Other settings are the same


204
00:14:05,412 --> 00:14:09,349 line:-2
as the settings that are used
for preferredOutputSegmentInterval.


205
00:14:09,416 --> 00:14:11,251 line:-1
This is only for passthrough.


206
00:14:11,318 --> 00:14:13,887 line:0
You cannot encode
with custom segmentation.


207
00:14:13,954 --> 00:14:16,723 line:0
So when you create AVAssetWriterInput...


208
00:14:17,724 --> 00:14:20,360 line:0
you have to set nil
to the output settings.


209
00:14:21,395 --> 00:14:25,432 line:-2
You call the flushSegment method
to output media data.


210
00:14:27,134 --> 00:14:30,137 line:-1
FlushSegment outputs a media data


211
00:14:30,204 --> 00:14:34,441 line:-2
that includes all samples appended
since the previous call.


212
00:14:37,911 --> 00:14:41,782 line:-2
You must call flushSegment
prior to a sync sample


213
00:14:41,849 --> 00:14:46,753 line:-2
so that the next fragmented media data
can start with the sync sample.


214
00:14:48,789 --> 00:14:51,959 line:-1
Otherwise, an error that indicates


215
00:14:52,025 --> 00:14:57,130 line:-2
that a fragmented media data started
with nonsync sample occurs.


216
00:14:58,832 --> 00:15:01,869 line:-1
In this mode, you can mux audio and video.


217
00:15:02,803 --> 00:15:07,107 line:-2
But if both audio and video
have sample dependencies,


218
00:15:07,174 --> 00:15:11,078 line:-2
it would make it difficult
to align both media evenly.


219
00:15:11,545 --> 00:15:14,581 line:-2
In that case,
you should consider to package


220
00:15:14,648 --> 00:15:16,917 line:-1
video and audio as separate streams.


221
00:15:18,752 --> 00:15:21,321 line:-1
AVAssetTrack now has a new property


222
00:15:21,388 --> 00:15:25,859 line:-2
that indicates whether the audio track
has sample dependencies.


223
00:15:27,027 --> 00:15:30,664 line:-2
You can check if a track
has sample dependencies in advance


224
00:15:30,731 --> 00:15:32,299 line:-1
using this property.


225
00:15:32,366 --> 00:15:33,867 line:-1
I mentioned earlier


226
00:15:33,934 --> 00:15:38,005 line:-2
that you should specify
Apple HLS or CMAF compliant profile


227
00:15:38,071 --> 00:15:41,308 line:-2
to output media data
in fragmented MP4 format.


228
00:15:42,876 --> 00:15:49,416 line:-2
CMAF is a set of constraints for how
you construct fragmented MP4 segments.


229
00:15:51,118 --> 00:15:55,556 line:-2
It is a standard
for using fragmented MP4 for streaming.


230
00:15:57,090 --> 00:16:01,361 line:-2
It is supported by multiple players,
including Apple HLS.


231
00:16:02,095 --> 00:16:07,668 line:-2
Some of the constraints are not required
if you target just Apple HLS.


232
00:16:07,734 --> 00:16:12,139 line:-2
But if you wanted a broader audience
for your media library,


233
00:16:12,206 --> 00:16:14,208 line:-1
then you could consider CMAF.


234
00:16:16,677 --> 00:16:21,648 line:-2
Now I will talk
about how HLS deals with audio priming.


235
00:16:21,715 --> 00:16:26,220 line:-2
This diagram represents
audio waveform of AAC audio.


236
00:16:27,321 --> 00:16:34,294 line:-2
AAC audio codecs require data
beyond the source PCM audio samples


237
00:16:34,361 --> 00:16:37,865 line:-2
to correctly encode
and decode audio samples


238
00:16:37,931 --> 00:16:40,767 line:-2
due to the nature
of the encoding algorithm.


239
00:16:41,568 --> 00:16:48,108 line:-2
For this reason, the encoders add silence
before the first true audio sample.


240
00:16:49,877 --> 00:16:52,045 line:-1
This is called priming.


241
00:16:52,546 --> 00:16:58,051 line:-2
The most common priming for AAC
is 2,112 audio samples,


242
00:16:58,118 --> 00:17:01,588 line:-1
which is just about 48 milliseconds,


243
00:17:01,655 --> 00:17:05,224 line:-1
presuming that that sample rate is 44,100.


244
00:17:06,425 --> 00:17:11,498 line:0
When audio and video are written
in separate files as separate streams,


245
00:17:11,565 --> 00:17:14,067 line:0
the media data are laid out like this...


246
00:17:15,202 --> 00:17:19,071 line:0
presuming that both media start
at time zero.


247
00:17:19,705 --> 00:17:23,710 line:0
As you may have noticed, if audio
and video start at the same time...


248
00:17:24,711 --> 00:17:27,915 line:0
the audio will be delayed by the priming


249
00:17:27,981 --> 00:17:30,784 line:0
so that the audio and the video
will be slightly off.


250
00:17:31,752 --> 00:17:36,924 line:0
CMAF adopts edit list box in audio track
to compensate for this issue.


251
00:17:36,990 --> 00:17:39,326 line:0
The priming is edited out...


252
00:17:40,427 --> 00:17:43,830 line:0
so that start time of audio
and video match.


253
00:17:47,100 --> 00:17:51,271 line:-2
Historically,
Apple HLS hasn't used edit list,


254
00:17:51,338 --> 00:17:54,541 line:-1
so if you specify Apple HLS profile,


255
00:17:54,608 --> 00:18:00,047 line:-2
the edit list box is not used
for compatibility with older players.


256
00:18:00,113 --> 00:18:03,550 line:-2
Instead,
baseMediaDecodeTime of audio media


257
00:18:03,617 --> 00:18:07,554 line:-1
is shifted the amount of priming backward.


258
00:18:09,189 --> 00:18:14,761 line:-2
However, the baseMediaDecodeTime
cannot be earlier than zero


259
00:18:14,828 --> 00:18:17,664 line:-2
since it is defined
as an unsigned integer.


260
00:18:18,966 --> 00:18:25,272 line:-2
One solution is to shift both audio
and video forward by same time offset.


261
00:18:26,206 --> 00:18:29,543 line:-2
This way,
the baseMediaDecodeTime of audio media


262
00:18:29,610 --> 00:18:33,046 line:-2
can be shifted
the amount of priming backward.


263
00:18:34,281 --> 00:18:36,984 line:-1
Even if start time is not zero,


264
00:18:37,050 --> 00:18:42,222 line:-2
in HLS, playback starts at the earliest
presentation time of video sample...


265
00:18:43,223 --> 00:18:47,694 line:-2
so playback starts immediately
without waiting for the time offset.


266
00:18:49,763 --> 00:18:54,168 line:-2
This is important,
not only for exact video and audio sync,


267
00:18:54,234 --> 00:18:58,005 line:-1
but also for exact match of time stamps


268
00:18:58,071 --> 00:19:01,642 line:-2
between audio variants
with different priming duration.


269
00:19:01,708 --> 00:19:05,379 line:-1
So we recommend you to shift media time


270
00:19:05,445 --> 00:19:08,815 line:-2
by adding a certain amount of time
to all samples


271
00:19:08,882 --> 00:19:10,984 line:-1
if you specify Apple HLS profile.


272
00:19:12,786 --> 00:19:15,789 line:0
Initial segment start time
should be shifted


273
00:19:15,856 --> 00:19:17,724 line:0
same time offset as well.


274
00:19:19,026 --> 00:19:23,664 line:0
As I said earlier, the most common
priming is less than one second,


275
00:19:23,730 --> 00:19:27,301 line:0
so a couple of seconds
is enough for the time offset.


276
00:19:27,367 --> 00:19:31,839 line:0
But Media File Segmenter,
which is Apple's segmenter tool,


277
00:19:31,905 --> 00:19:36,610 line:0
has a ten-second offset,
so you could choose the same ten seconds.


278
00:19:36,677 --> 00:19:40,914 line:0
Our sample code shows how to add
a time offset to all samples.


279
00:19:41,582 --> 00:19:45,752 line:-2
In this demo, I will use
our sample command line application


280
00:19:45,819 --> 00:19:49,456 line:-2
to create fragmented MP4
segment files and the playlist.


281
00:19:50,924 --> 00:19:54,528 line:-2
Then I'll play the resulting content
using Safari.


282
00:19:55,562 --> 00:19:59,566 line:-2
This demo scene is set up
as a local server


283
00:19:59,633 --> 00:20:02,102 line:-1
in advance to host the content.


284
00:20:04,972 --> 00:20:08,442 line:-2
I entered the command line application
in the terminal application.


285
00:20:10,010 --> 00:20:13,547 line:-2
This command line application
reads media data


286
00:20:13,614 --> 00:20:19,186 line:-2
from a premade source movie
and encodes the video and the audio.


287
00:20:19,253 --> 00:20:21,488 line:-1
This is the source movie.


288
00:20:21,889 --> 00:20:24,091 line:-1
The length is just 30 seconds.


289
00:20:24,725 --> 00:20:27,661 line:-2
This command line application
uses six seconds


290
00:20:27,728 --> 00:20:30,998 line:-1
as the preferred output segment interval.


291
00:20:31,532 --> 00:20:36,036 line:-2
The segment files and the playlist
will be created in this directory.


292
00:20:36,103 --> 00:20:37,571 line:-1
Let's get started.


293
00:20:37,638 --> 00:20:42,009 line:-2
The video frame rate of the source movie
is 30 frames per second.


294
00:20:42,075 --> 00:20:46,180 line:-2
You see five segment files
and a playlist was created.


295
00:20:46,246 --> 00:20:47,881 line:-1
I open the playlist.


296
00:20:49,416 --> 00:20:52,085 line:-1
Each segment's duration is six seconds.


297
00:20:57,191 --> 00:21:01,662 line:-2
I enter the local host as URL
to play it using Safari.


298
00:21:08,235 --> 00:21:09,736 line:-1
Let's play it.


299
00:21:11,205 --> 00:21:13,207 line:-1
[soft music playing]


300
00:21:19,146 --> 00:21:21,582 line:-1
HLS has several requirements.


301
00:21:22,216 --> 00:21:27,221 line:-2
It also has several recommendations
to improve the experience for users.


302
00:21:28,021 --> 00:21:32,693 line:-2
We recommend you to validate
segment files and playlist that you write


303
00:21:32,759 --> 00:21:36,663 line:-2
using AVAssetWriter
to make sure that those files


304
00:21:36,730 --> 00:21:39,299 line:-1
meet the requirements and recommendations.


305
00:21:39,366 --> 00:21:45,305 line:0
WWDC session
"Validating HTTP Live Streams" in 2016


306
00:21:45,372 --> 00:21:47,608 line:0
is a great source for this purpose.


307
00:21:47,674 --> 00:21:50,878 line:0
You can find the session video
in Apple's Developer site.


308
00:21:52,179 --> 00:21:57,985 line:-2
AVAssetWriter now outputs media data
in fragmented MP4 format for HLS.


309
00:21:58,785 --> 00:22:01,021 line:-1
Thank you for joining the session.


310
00:22:01,088 --> 00:22:02,990 line:-1
[speaking Japanese]

