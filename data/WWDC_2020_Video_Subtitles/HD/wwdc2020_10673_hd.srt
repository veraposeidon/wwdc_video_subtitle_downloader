1
00:00:03,904 --> 00:00:06,139 line:-1
Hello and welcome to WWDC.


2
00:00:09,042 --> 00:00:10,544 line:0
Welcome to WWDC.


3
00:00:10,943 --> 00:00:14,047 line:0
My name is Frank Doepke and together
with my colleague David Hayward,


4
00:00:14,114 --> 00:00:16,517 line:0
we're going to explore
Computer Vision APIs.


5
00:00:18,118 --> 00:00:20,187 line:-2
So why would you talk
about Computer Vision?


6
00:00:20,621 --> 00:00:23,357 line:-2
Computer Vision can really
enhance your application.


7
00:00:23,757 --> 00:00:25,993 line:-2
And even if it's not
at the core of your business,


8
00:00:26,059 --> 00:00:28,362 line:-2
it really brings something new
to your application.


9
00:00:29,530 --> 00:00:31,031 line:-1
Let me give you an example.


10
00:00:32,031 --> 00:00:34,801 line:-2
Banking applications allow
you to deposit checks.


11
00:00:35,068 --> 00:00:38,772 line:-2
They use Computer Vision for the camera
to actually read the check for you,


12
00:00:38,839 --> 00:00:41,241 line:-2
so you don't have to type
in the information anymore.


13
00:00:41,909 --> 00:00:45,546 line:-2
And clearly Computer Vision is not
at the core of the banking industry.


14
00:00:46,313 --> 00:00:50,484 line:-2
But by doing this you really can save
a lot of steps for your user.


15
00:00:50,551 --> 00:00:52,519 line:-2
They don't have to type
anything in anymore.


16
00:00:53,820 --> 00:00:57,658 line:-2
Another thing might be that you want to
just, for instance, read a QR code,


17
00:00:57,724 --> 00:00:59,159 line:-1
or when you read a receipt.


18
00:00:59,226 --> 00:01:02,563 line:-2
All of that may not be at the core
of what you wanna do for your application,


19
00:01:02,629 --> 00:01:06,200 line:-2
but it really makes it much easier for
your users to do this by using the camera.


20
00:01:07,234 --> 00:01:09,970 line:-2
So what APIs do we have available
for Computer Vision?


21
00:01:11,505 --> 00:01:14,474 line:-2
At the most high level part,
we have VisionKit.


22
00:01:14,541 --> 00:01:16,343 line:-1
It's the home of the VNDocumentCamera


23
00:01:16,410 --> 00:01:19,379 line:-2
that you might have seen
in Notes, or in Messages, or Mail


24
00:01:19,446 --> 00:01:21,281 line:-1
to actually scan the document.


25
00:01:22,082 --> 00:01:25,886 line:-2
Then we use Core Image to actually do
the image processing of images,


26
00:01:26,520 --> 00:01:29,089 line:-1
Vision for the analysis of images,


27
00:01:29,656 --> 00:01:33,160 line:-2
and last but not least, Core ML
to do the machine learning inference.


28
00:01:34,094 --> 00:01:37,264 line:-2
Today we're gonna focus just on
Core Image and Vision.


29
00:01:38,031 --> 00:01:40,334 line:-2
But I wanna make sure
that you don't just think of them


30
00:01:40,400 --> 00:01:42,436 line:-1
as pillars that stand side by side.


31
00:01:42,503 --> 00:01:44,571 line:-1
They can actually be nicely intertwined.


32
00:01:45,105 --> 00:01:47,841 line:-2
I might actually want to do
some image preprocessing,


33
00:01:47,908 --> 00:01:52,880 line:0
run it into Vision, take the results
from there, feed them into Core ML,


34
00:01:53,313 --> 00:01:56,683 line:0
or back into Core Image
to create some of the effects.


35
00:01:56,750 --> 00:01:59,253 line:-2
Now to talk about how we
want to use Core Image


36
00:01:59,319 --> 00:02:01,455 line:-1
to preprocess images for Computer Vision,


37
00:02:01,522 --> 00:02:04,258 line:-2
I would like to hand it over
to my colleague David Hayward.


38
00:02:05,926 --> 00:02:08,895 line:-2
Thank you, Frank. I'd like to take
this opportunity to describe


39
00:02:08,961 --> 00:02:12,833 line:-2
how you can improve your Computer Vision
algorithms using Core Image.


40
00:02:14,368 --> 00:02:16,103 line:-1
If you are unfamiliar with Core Image,


41
00:02:16,170 --> 00:02:20,674 line:-2
it is an optimized, easy-to-use image
processing framework built upon Metal.


42
00:02:21,241 --> 00:02:23,076 line:-1
For a deep dive on how it works,


43
00:02:23,143 --> 00:02:28,215 line:-2
I recommend you watch our WWDC
2017 presentation on the subject.


44
00:02:29,917 --> 00:02:33,720 line:-2
There are two primary reasons why your
app should use Core Image with Vision.


45
00:02:35,189 --> 00:02:38,025 line:-2
Using Core Image to preprocess
an input to Vision


46
00:02:38,091 --> 00:02:41,028 line:-2
can make your algorithms faster
and more robust.


47
00:02:42,896 --> 00:02:45,799 line:-2
Using Core Image to post-process
the outputs from Vision


48
00:02:45,866 --> 00:02:49,603 line:-2
can give your app new ways
to show those results to your users.


49
00:02:51,839 --> 00:02:53,707 line:-1
Also, Core Image is a great tool


50
00:02:53,774 --> 00:02:56,443 line:-2
to do Augmentation
for Machine Learning training.


51
00:02:56,810 --> 00:03:03,283 line:-2
There's some great examples of this
in our presentation from WWDC in 2018.


52
00:03:05,085 --> 00:03:07,421 line:-2
One of the best ways
to prepare an image for analysis


53
00:03:07,487 --> 00:03:09,823 line:-1
is to downscale it for best performance.


54
00:03:10,824 --> 00:03:14,828 line:-2
The scaler with the best overall quality
is CILanczosScale.


55
00:03:16,563 --> 00:03:19,166 line:-2
It is very easy to use this filter
in your code.


56
00:03:19,600 --> 00:03:23,170 line:-2
All you need to do is import
the CIFilterBuiltins header,


57
00:03:23,737 --> 00:03:27,174 line:-2
create a filter instance,
set the input properties,


58
00:03:27,241 --> 00:03:30,010 line:-2
and then get the outputImage.
It's that easy.


59
00:03:32,446 --> 00:03:35,649 line:-2
But that is just one of several
resampling filters in Core Image.


60
00:03:36,550 --> 00:03:37,885 line:-1
Depending on your algorithm,


61
00:03:37,951 --> 00:03:42,256 line:-2
it may be better to use the
linear interpolated CIAffineTransform.


62
00:03:45,192 --> 00:03:47,494 line:-2
Morphology operations
are a great technique


63
00:03:47,561 --> 00:03:50,230 line:-2
to make small features
in your image more prominent.


64
00:03:51,465 --> 00:03:55,035 line:-2
Performing Dilate using
CIMorphologyRectangleMaximum


65
00:03:55,102 --> 00:03:57,838 line:-2
will make brighter areas
of the image larger.


66
00:04:01,542 --> 00:04:05,078 line:-2
Performing Erode using
CIMorphologyRectangleMinimum


67
00:04:05,145 --> 00:04:07,347 line:-1
will make those areas smaller.


68
00:04:09,383 --> 00:04:14,555 line:-2
Better still, is to perform Close using
CIMorphologyRectangleMinimum


69
00:04:14,621 --> 00:04:17,423 line:-1
followed by CIMorphologyRectangleMaximum.


70
00:04:17,757 --> 00:04:23,497 line:-2
And this is very useful for removing
small areas of noise from your image


71
00:04:23,564 --> 00:04:25,332 line:-1
that may affect the algorithm.


72
00:04:26,733 --> 00:04:29,269 line:-2
Some algorithms only need
monochrome inputs,


73
00:04:29,336 --> 00:04:33,440 line:-2
and for these, Vision will automatically
convert RGB to grayscale.


74
00:04:34,074 --> 00:04:36,743 line:-2
If you have domain knowledge
about your input images,


75
00:04:36,810 --> 00:04:40,280 line:-2
you might get better results
using Core Image to convert to gray.


76
00:04:42,182 --> 00:04:44,184 line:-1
With CIColorMatrix you can specify


77
00:04:44,251 --> 00:04:46,320 line:-2
any weighting you want
for this conversion.


78
00:04:47,955 --> 00:04:50,224 line:-1
Or with CIMaximumComponent,


79
00:04:50,290 --> 00:04:52,693 line:-2
the channel with the greatest signal
will be used.


80
00:04:55,262 --> 00:04:58,932 line:-2
Noise reductions before image analysis
is also worth consideration.


81
00:05:00,501 --> 00:05:05,839 line:-2
A couple passes of CIMedianFilter can
reduce noise without softening the edges.


82
00:05:07,674 --> 00:05:12,579 line:-2
CIGaussianBlur and CIBoxBlur
are also a fast way to reduce noise.


83
00:05:14,848 --> 00:05:17,918 line:-2
And consider using
the CINoiseReduction filter too.


84
00:05:19,152 --> 00:05:22,956 line:-2
Core Image also has a variety
of edge detection filters.


85
00:05:24,691 --> 00:05:28,862 line:-2
For a Sobel edge detector,
you can use CIConvolution3X3.


86
00:05:31,198 --> 00:05:34,101 line:-1
Even better is to use CIGaborGradients,


87
00:05:34,168 --> 00:05:38,572 line:-2
which will produce a 2D gradient vector
that is also more tolerant of noise.


88
00:05:40,641 --> 00:05:44,311 line:-2
Enhancing the contrast of an image
can aid in object detection.


89
00:05:45,679 --> 00:05:48,081 line:-1
CIColorPolynomial allows you to specify


90
00:05:48,148 --> 00:05:50,784 line:-1
an arbitrary 3rd degree contrast function.


91
00:05:51,752 --> 00:05:55,589 line:-2
CIColorControls provides
a linear contrast parameter.


92
00:05:57,257 --> 00:05:59,660 line:-2
Core Image also has
some new filters this year


93
00:05:59,726 --> 00:06:02,763 line:-2
that can convert your image
to just black and white.


94
00:06:03,931 --> 00:06:05,732 line:-1
For example, CIColorThreshold


95
00:06:05,799 --> 00:06:09,203 line:-2
allows you to set the threshold value
in your application code,


96
00:06:09,670 --> 00:06:12,773 line:-2
while CIColorThresholdOtsu
will automatically determine


97
00:06:12,840 --> 00:06:16,009 line:-2
the best threshold value
based on the image's histogram.


98
00:06:18,345 --> 00:06:22,182 line:-2
Core Image also has filters
for comparing two images.


99
00:06:22,249 --> 00:06:26,720 line:-2
This can be useful to prepare for
detecting motion between frames of video.


100
00:06:27,955 --> 00:06:31,158 line:-1
For example, CIColorAbsoluteDifference


101
00:06:31,225 --> 00:06:34,194 line:-2
is a new filter this year
that can help with this.


102
00:06:36,496 --> 00:06:40,801 line:-2
Also, the CILabDeltaE
will compare two images


103
00:06:40,868 --> 00:06:44,605 line:-2
using a formula designed to match
human perception of color.


104
00:06:45,973 --> 00:06:50,377 line:-2
These are just a sampling of the more
than 200 filters built into Core Image.


105
00:06:52,679 --> 00:06:55,182 line:-1
To help you use these built-in filters,


106
00:06:55,249 --> 00:06:57,985 line:-2
this documentation includes
parameter descriptions,


107
00:06:58,051 --> 00:07:00,587 line:-1
sample images, and even sample code.


108
00:07:02,890 --> 00:07:04,892 line:-2
And if none of these filters
suit your needs,


109
00:07:04,958 --> 00:07:08,262 line:-2
then you can easily write your own
using Metal Core Image.


110
00:07:09,062 --> 00:07:11,365 line:0
And we recommend that you
see our session on that


111
00:07:11,431 --> 00:07:13,367 line:0
that we also made available this year.


112
00:07:14,902 --> 00:07:16,970 line:-1
With image processing and Computer Vision,


113
00:07:17,037 --> 00:07:19,106 line:-1
it is important to be aware that images


114
00:07:19,173 --> 00:07:21,842 line:-2
can come in a wide variety
of color spaces.


115
00:07:23,310 --> 00:07:25,846 line:-1
Your app may receive images in spaces


116
00:07:25,913 --> 00:07:30,584 line:-2
ranging from the traditional sRGB,
to wide gamut P3,


117
00:07:31,518 --> 00:07:34,588 line:-2
even to HDR color spaces,
which are now supported.


118
00:07:36,290 --> 00:07:39,760 line:-2
Your app should be prepared
for this variety of color spaces,


119
00:07:40,260 --> 00:07:43,263 line:-2
and the good news is that Core Image
makes this very easy.


120
00:07:44,298 --> 00:07:47,968 line:-2
Core Image automatically converts inputs
to its working space,


121
00:07:48,535 --> 00:07:52,806 line:-2
which is Unclamped,
Linear, BT.709 primaries.


122
00:07:55,108 --> 00:07:59,446 line:-2
Your algorithm might want images
in a different color space though.


123
00:08:00,080 --> 00:08:01,849 line:-1
In that case, you should do the following.


124
00:08:02,883 --> 00:08:05,686 line:-2
You will want to get a variable
for the color space


125
00:08:05,752 --> 00:08:08,021 line:-1
that you want to use from CGColorSpace.


126
00:08:08,822 --> 00:08:11,892 line:-2
And you will call
image.matchedFromWorkingSpace.


127
00:08:13,493 --> 00:08:16,063 line:-1
Apply your algorithm in that space,


128
00:08:16,964 --> 00:08:20,033 line:-1
and then call image.matchedToWorkingSpace.


129
00:08:21,168 --> 00:08:22,703 line:-1
That's all you need to do.


130
00:08:23,604 --> 00:08:26,173 line:-2
My last topic today
will be using Core Image


131
00:08:26,240 --> 00:08:28,442 line:-1
to post-process the outputs from Vision.


132
00:08:29,109 --> 00:08:31,245 line:-1
One example of this is using Core Image


133
00:08:31,311 --> 00:08:35,315 line:-2
to regenerate a barcodeImage
from a Vision BarcodeObservation.


134
00:08:36,850 --> 00:08:40,654 line:-2
All you need to do in your code
is create the filter instance...


135
00:08:41,688 --> 00:08:45,559 line:-2
set its barcodeDescriptor property
to be that of the Vision observation,


136
00:08:46,527 --> 00:08:48,662 line:-1
and lastly, get the outputImage.


137
00:08:49,296 --> 00:08:51,365 line:-1
And the result looks just like this.


138
00:08:53,166 --> 00:08:57,504 line:-2
Similarly, your app can apply filters
based on Vision face observations.


139
00:08:59,706 --> 00:09:04,945 line:-2
As an example, you can use
a vignette effect very easily using this.


140
00:09:07,881 --> 00:09:09,550 line:-1
The code is actually very simple.


141
00:09:09,616 --> 00:09:11,385 line:-1
One thing you need to be aware of


142
00:09:11,451 --> 00:09:15,622 line:-2
is that you will need to convert
from Vision's normalized coordinate system


143
00:09:15,689 --> 00:09:18,859 line:-2
to Core Image's
Cartesian coordinate system.


144
00:09:20,460 --> 00:09:23,096 line:0
And once you create the vignette filter,


145
00:09:23,163 --> 00:09:27,501 line:0
you can then put that vignette
over the image using compositing over.


146
00:09:31,305 --> 00:09:34,808 line:-2
You can also use Core Image
to visualize vector fields,


147
00:09:34,875 --> 00:09:38,045 line:-2
which Frank
will be demonstrating later on.


148
00:09:39,580 --> 00:09:41,682 line:-2
That concludes
my part of this presentation.


149
00:09:41,748 --> 00:09:43,817 line:-1
Here's Frank to talk more about Vision.


150
00:09:45,485 --> 00:09:47,020 line:-1
All right. Thank you, David.


151
00:09:47,621 --> 00:09:52,092 line:-2
So, now I'm gonna talk about how we
can understand images by using Vision.


152
00:09:53,527 --> 00:09:58,065 line:-2
We have a task, the machinery,
and the results.


153
00:09:58,632 --> 00:10:00,501 line:-1
The task is what you wanna do.


154
00:10:00,567 --> 00:10:03,070 line:-2
The machinery is what actually
performs the work.


155
00:10:03,136 --> 00:10:05,672 line:-2
And the results is, of course,
what you're looking for--


156
00:10:05,739 --> 00:10:07,140 line:-1
what you want to get back.


157
00:10:08,242 --> 00:10:12,079 line:-2
The task could be in our compiler,
the VNRequests.


158
00:10:12,145 --> 00:10:14,615 line:-1
Like a VNDetectFaceRectanglesRequest.


159
00:10:15,182 --> 00:10:17,751 line:-1
The machinery is one of two.


160
00:10:18,552 --> 00:10:21,889 line:-2
We have an ImageRequestHandler
or a SequenceRequestHandler.


161
00:10:22,956 --> 00:10:26,560 line:-2
And the results that we get back
is what we call VNObservation.


162
00:10:27,294 --> 00:10:29,396 line:-2
And these depend
on which task you performed,


163
00:10:29,463 --> 00:10:33,200 line:-2
like a VNRectangleObservation
for detected rectangles.


164
00:10:35,836 --> 00:10:39,640 line:-2
We first perform the request
on the ImageRequestHandler.


165
00:10:40,541 --> 00:10:43,043 line:-1
And from there, we get our observations.


166
00:10:43,510 --> 00:10:45,179 line:-1
Let's look at a concrete example.


167
00:10:47,114 --> 00:10:50,684 line:-2
We want to read text, so we use
the VNRecognizeTextRequest.


168
00:10:53,020 --> 00:10:55,923 line:0
Then I create an ImageRequestHandler
with my image.


169
00:10:57,257 --> 00:11:02,663 line:0
And out of that, I now get my
observations, which is just a plain text.


170
00:11:04,631 --> 00:11:07,434 line:-1
So, what do we have new in 2020 in Vision?


171
00:11:08,902 --> 00:11:11,371 line:0
First, we have Hand and Body Pose.


172
00:11:12,072 --> 00:11:15,876 line:0
To see more about that, please look
at the "Hand and Body Pose" session.


173
00:11:18,579 --> 00:11:21,181 line:-2
Then you might have seen
our Trajectory Detection.


174
00:11:21,882 --> 00:11:23,650 line:0
And more about that, you can see


175
00:11:23,717 --> 00:11:26,320 line:0
in the "Exploring the Action
and Vision Application."


176
00:11:28,889 --> 00:11:33,594 line:-2
Today, we're just going to focus on the
Contour Detection and on Optical Flow.


177
00:11:34,962 --> 00:11:36,597 line:-1
What is Contour Detection?


178
00:11:36,930 --> 00:11:40,334 line:-2
With Contour Detection,
I can find edges in my image.


179
00:11:41,969 --> 00:11:46,773 line:-2
As we saw here, the red lines now show the
contours that we found in this graphic.


180
00:11:49,743 --> 00:11:54,548 line:-2
So we start with an image, and then we
create our VNDetectContourRequest.


181
00:11:58,652 --> 00:12:01,522 line:-1
We can now set the contrast on the image


182
00:12:01,588 --> 00:12:05,058 line:-2
to enhance, for instance,
how some of the contrast may come out.


183
00:12:05,425 --> 00:12:08,061 line:-2
We can switch between,
do we want to run it


184
00:12:08,128 --> 00:12:09,930 line:-2
on a dark background
with this light background,


185
00:12:09,997 --> 00:12:12,566 line:-2
which may separate the foreground
versus background?


186
00:12:13,634 --> 00:12:16,503 line:-2
Last but not least, we can insert
the maximumImageDimension.


187
00:12:17,037 --> 00:12:20,874 line:-2
That allows you to trade off
the performance versus the accuracy.


188
00:12:22,209 --> 00:12:25,279 line:-2
That means,
for instance, if you look at it


189
00:12:25,345 --> 00:12:28,081 line:-2
at a lower resolution
you will still get your contours


190
00:12:28,148 --> 00:12:30,517 line:-2
but they might not follow the edge
as closely,


191
00:12:30,584 --> 00:12:33,921 line:-2
but it runs much faster
because it can run at a lower resolution.


192
00:12:34,755 --> 00:12:37,925 line:-2
In comparison,
when we use a higher resolution,


193
00:12:37,991 --> 00:12:40,427 line:-2
which you might want to do
in some post-processing,


194
00:12:40,494 --> 00:12:42,262 line:-2
we actually get
much more accurate contours


195
00:12:42,329 --> 00:12:45,532 line:-2
but it's gonna take a little bit longer
because it has to do more work.


196
00:12:47,901 --> 00:12:50,003 line:-2
Let's look at the observation
that we get back.


197
00:12:51,238 --> 00:12:55,809 line:-2
Here we have a very simple image
of two squares with a circle in it.


198
00:12:57,311 --> 00:12:59,980 line:-2
We are getting back a
VNContoursObservation.


199
00:13:01,448 --> 00:13:05,352 line:-2
The topLevelContours
are our two rectangles that we see.


200
00:13:07,154 --> 00:13:09,923 line:-1
Inside of those we have childContours.


201
00:13:09,990 --> 00:13:12,326 line:-1
They are nested and those are the circles.


202
00:13:14,194 --> 00:13:17,197 line:-1
Then we get back the contourCount


203
00:13:17,264 --> 00:13:20,834 line:-2
which I can actually use
to walk through all of my contours.


204
00:13:21,902 --> 00:13:25,706 line:-2
But it's much more easier,
for instance, to use the index path.


205
00:13:25,772 --> 00:13:28,008 line:-2
As you can see,
they are nested in each other


206
00:13:28,075 --> 00:13:30,210 line:-1
and I can now traverse my graph.


207
00:13:32,346 --> 00:13:35,115 line:-2
Last but not least,
I also get the normalizedPath.


208
00:13:35,182 --> 00:13:38,418 line:-2
And this is a CGPath I can use easily
for rendering.


209
00:13:41,054 --> 00:13:42,923 line:-1
Now, what is a VNContour?


210
00:13:43,891 --> 00:13:46,560 line:-1
In our example we get a VNContour here...


211
00:13:47,561 --> 00:13:50,731 line:-2
and that is the most Outer Contour,
our Parent.


212
00:13:51,532 --> 00:13:56,270 line:-2
Nested inside of it are childContours.
These are the Inner Contours.


213
00:13:58,372 --> 00:14:01,308 line:-1
My contour has an index path


214
00:14:01,875 --> 00:14:05,078 line:-2
and, of course, with that
every childContour has the index path,


215
00:14:05,145 --> 00:14:07,814 line:-2
which I can use again
to traverse my graph.


216
00:14:09,149 --> 00:14:12,452 line:-2
Then I get the normalizedPoints
in the pointCount.


217
00:14:12,519 --> 00:14:14,588 line:-2
Now, that is actually the real meat
of the contour


218
00:14:14,655 --> 00:14:17,624 line:-2
because it describes each
of the line segments that we discover.


219
00:14:17,691 --> 00:14:21,628 line:-2
Because we didn't just discover pixels,
we really get a contour which is a path.


220
00:14:23,530 --> 00:14:25,999 line:0
We also have an aspectRatio.
I'm gonna talk about that


221
00:14:26,066 --> 00:14:27,301 line:0
on the next slide.


222
00:14:28,735 --> 00:14:31,572 line:0
And then we have the normalizedPath
to render.


223
00:14:32,105 --> 00:14:34,374 line:-1
When we want to work with contours,


224
00:14:34,441 --> 00:14:36,877 line:-2
there's a few things
we need to keep in mind.


225
00:14:36,944 --> 00:14:39,046 line:-2
Let's look at this image
that we have here.


226
00:14:40,314 --> 00:14:42,850 line:-1
It is 1920 by 1080 pixels,


227
00:14:42,916 --> 00:14:49,756 line:-2
and we have a circle in the middle
that is exactly 1080 pixels high and wide.


228
00:14:50,858 --> 00:14:53,660 line:-2
But Vision
uses a normalized coordinate space.


229
00:14:54,595 --> 00:14:58,732 line:-1
So, our image is 1.0 high and 1.0 wide.


230
00:14:59,499 --> 00:15:04,238 line:-2
Therefore, the circle now has a height
of 1.0,


231
00:15:04,838 --> 00:15:08,442 line:-1
but a width of 0.5625.


232
00:15:09,343 --> 00:15:14,248 line:-2
So, if you wanna take the geometry of the
shapes that you've detected into account,


233
00:15:14,314 --> 00:15:16,350 line:-1
you need to look at the aspectRatio


234
00:15:17,184 --> 00:15:19,753 line:-2
of the original image
from which it was computed on.


235
00:15:21,288 --> 00:15:24,491 line:-2
Now, contours really get interesting
when we can analyze them,


236
00:15:24,558 --> 00:15:27,594 line:-2
and we have a few utilities
for that available for you.


237
00:15:29,496 --> 00:15:32,499 line:-1
The VNGeometryUtils provides some API.


238
00:15:32,566 --> 00:15:35,002 line:-1
For instance, we have the boundingCircle


239
00:15:35,068 --> 00:15:38,572 line:-2
which is the smallest circle
that completely encapsulates


240
00:15:38,639 --> 00:15:40,507 line:-1
the contour that you detected.


241
00:15:40,841 --> 00:15:43,410 line:-2
It's great for comparing contours
with each other.


242
00:15:44,611 --> 00:15:46,513 line:-1
Then we can calculate the area.


243
00:15:47,381 --> 00:15:49,650 line:-1
And we can calculate the perimeter.


244
00:15:50,684 --> 00:15:52,920 line:-2
Now, the next part that
you might want to do with contours


245
00:15:52,986 --> 00:15:54,521 line:-1
is actually simplify them.


246
00:15:55,422 --> 00:15:58,192 line:-2
When we get contours from an image
they tend to be noisy.


247
00:15:58,825 --> 00:16:00,494 line:-1
Let's look at our example here.


248
00:16:02,062 --> 00:16:04,164 line:-2
We have a rectangle
that we've photographed.


249
00:16:05,165 --> 00:16:07,801 line:-2
But it has little kinks in it
and as you can see,


250
00:16:07,868 --> 00:16:10,571 line:-2
the contour actually followed
those little kinks.


251
00:16:11,505 --> 00:16:14,842 line:-2
So, now I actually
do not have all the points


252
00:16:15,576 --> 00:16:18,312 line:-2
on just the corners,
but even, like, on the middle.


253
00:16:20,113 --> 00:16:24,952 line:-2
I can now use the approximation
of a polygon by using the Epsilon.


254
00:16:25,018 --> 00:16:29,623 line:-2
Now, Epsilon means I can filter out
all the little noise parts around an edge,


255
00:16:29,690 --> 00:16:32,860 line:-2
so that only the strong contour edges
will actually stay.


256
00:16:34,194 --> 00:16:36,730 line:-2
And now, I get, again,
a perfect rectangle.


257
00:16:37,431 --> 00:16:42,069 line:-2
And with that I just have four points.
So, if I need to analyze shapes


258
00:16:42,135 --> 00:16:43,504 line:-1
it's very simple for me,


259
00:16:43,570 --> 00:16:46,740 line:-2
because I can simply say, "If
it has four points, it's a quadrilateral,"


260
00:16:46,807 --> 00:16:49,009 line:-1
and I detected what kind of shape I have.


261
00:16:51,111 --> 00:16:54,414 line:-2
Let's look at a concrete example
of how we can use all of this.


262
00:16:57,050 --> 00:16:59,119 line:-1
Let's say we need to save the world


263
00:16:59,186 --> 00:17:02,956 line:-2
by resurrecting very old computer code
that is done on punch cards.


264
00:17:04,657 --> 00:17:08,395 line:-2
So, our task is to identify the dimples
on the punch card


265
00:17:08,462 --> 00:17:10,797 line:-2
because nobody has
a punch card reader anymore.


266
00:17:12,499 --> 00:17:13,534 line:-1
So, we search the web


267
00:17:13,599 --> 00:17:17,037 line:-2
and find a Computer Vision blog post
that talks about how to do this.


268
00:17:17,538 --> 00:17:20,207 line:-1
But it's written in Python.


269
00:17:21,141 --> 00:17:23,010 line:-1
Our task, of course,


270
00:17:23,076 --> 00:17:25,512 line:-1
is to bring it natively onto our platform,


271
00:17:26,079 --> 00:17:28,448 line:-2
so that we can run it
in the best way possible.


272
00:17:31,618 --> 00:17:34,121 line:-2
So, now we have here
a section of Python code.


273
00:17:34,188 --> 00:17:35,856 line:-2
Don't worry
if you don't understand Python.


274
00:17:35,923 --> 00:17:38,258 line:-2
I'm just going
to walk you quickly through it.


275
00:17:38,325 --> 00:17:42,396 line:-2
The concept is very often always the same.
We do some image processing first...


276
00:17:43,697 --> 00:17:45,532 line:-1
then we do some image analysis...


277
00:17:47,801 --> 00:17:50,804 line:0
and we get some results back
that we need to visualize.


278
00:17:51,572 --> 00:17:54,041 line:-2
Now, there's one part,
even if you don't understand Python


279
00:17:54,107 --> 00:17:56,310 line:-2
that I would like to highlight
here in the very beginning,


280
00:17:56,376 --> 00:17:58,178 line:-1
the first three lines that you see.


281
00:17:58,846 --> 00:18:01,682 line:-2
You see that we actually needed
to import a few libraries.


282
00:18:02,216 --> 00:18:03,851 line:-1
Now, they don't come with Python.


283
00:18:03,917 --> 00:18:07,187 line:-2
These are third-party libraries
that you need to actually include.


284
00:18:09,423 --> 00:18:11,091 line:-1
So, how do we do this natively?


285
00:18:12,426 --> 00:18:14,962 line:-2
For the image processing part,
we need to load the image.


286
00:18:15,028 --> 00:18:18,165 line:-2
And you know how to do that already.
You use CGImageSource,


287
00:18:18,232 --> 00:18:21,068 line:-2
get a UI Image from it,
load it into CIImage...


288
00:18:21,869 --> 00:18:23,403 line:-1
You name it.


289
00:18:23,470 --> 00:18:26,240 line:-1
Then you have the way of using Core Image


290
00:18:26,306 --> 00:18:28,542 line:-1
to process the image by using CIFilters,


291
00:18:28,609 --> 00:18:30,143 line:-1
like in the CIAbsoluteThreshold


292
00:18:30,210 --> 00:18:32,880 line:-2
or many others
as David has already explained.


293
00:18:36,149 --> 00:18:38,085 line:-1
Now we're doing the image analysis.


294
00:18:38,151 --> 00:18:41,088 line:-1
For that we create our ImageRequestHandler


295
00:18:41,154 --> 00:18:43,257 line:-1
from the CIImage that we just processed,


296
00:18:44,024 --> 00:18:47,361 line:-2
and then we perform our request
like the VNDetectContourRequest.


297
00:18:48,095 --> 00:18:49,763 line:-1
Now, the beauty of this request is


298
00:18:49,830 --> 00:18:52,332 line:-2
we might not even have
to preprocess our image.


299
00:18:53,700 --> 00:18:55,269 line:-1
And then we visualize our results.


300
00:18:55,335 --> 00:18:57,771 line:-1
Again, we can use Core Image to do this,


301
00:18:57,838 --> 00:19:01,542 line:-2
which allows us to composite it right over
the image that we actually have


302
00:19:01,608 --> 00:19:04,978 line:-2
right into the same context.
You might use the CIMeshGenerator,


303
00:19:05,045 --> 00:19:06,580 line:-1
or the CITextGenerator.


304
00:19:08,015 --> 00:19:10,250 line:-1
But I can also use CoreGraphics


305
00:19:10,317 --> 00:19:13,554 line:-2
or UIKit to render it into a layer
on top of my image.


306
00:19:14,521 --> 00:19:17,758 line:-2
All right, now, after all these slides,
let's look at a real demo.


307
00:19:18,258 --> 00:19:19,993 line:-1
Let me go over to my demo machine.


308
00:19:21,495 --> 00:19:23,797 line:-2
What I've prepared here
is a little playground.


309
00:19:24,398 --> 00:19:27,634 line:-1
And what you see is I've loaded my image.


310
00:19:30,270 --> 00:19:32,072 line:-1
I created my contourRequest...


311
00:19:34,374 --> 00:19:35,976 line:-1
and then I simply perform it.


312
00:19:36,410 --> 00:19:37,578 line:-1
And there, voilÃ .


313
00:19:37,911 --> 00:19:42,216 line:-2
I can see all the contours, including
the dimples that I was looking for.


314
00:19:43,083 --> 00:19:46,353 line:-1
Now, notice that I found 387 contours.


315
00:19:46,720 --> 00:19:49,189 line:-2
So, this may be a bit more
than actually I want.


316
00:19:50,224 --> 00:19:53,827 line:-2
So, we need to filter out
all these contours.


317
00:19:54,862 --> 00:19:58,899 line:-2
Well, I was a little bit prepared here,
and I've hidden a little bit of code.


318
00:19:59,633 --> 00:20:01,768 line:-1
Let me uncover this piece of code.


319
00:20:01,835 --> 00:20:03,103 line:-1
And all that is...


320
00:20:03,770 --> 00:20:05,639 line:-1
I use my domain knowledge


321
00:20:05,706 --> 00:20:10,377 line:-2
of knowing that my contours are actually
on a blue background.


322
00:20:11,245 --> 00:20:14,715 line:-2
So, I use now some CIFiltering
to first blur out all the noise...


323
00:20:16,416 --> 00:20:20,721 line:-2
then I use my color controls
to really bring out the contrast.


324
00:20:21,455 --> 00:20:23,690 line:-2
And then I use
my filtered image afterwards


325
00:20:23,757 --> 00:20:26,426 line:-1
and run it through my Contour Detection.


326
00:20:27,160 --> 00:20:29,830 line:-1
And you see now, I only find 32 contours,


327
00:20:29,897 --> 00:20:33,100 line:-2
which is really just the dimples
that I care about in the first place.


328
00:20:34,067 --> 00:20:37,004 line:-1
All right, let's go back to our slides.


329
00:20:38,972 --> 00:20:41,441 line:-2
Normally, I would talk about
what I did in my demo,


330
00:20:41,508 --> 00:20:44,411 line:-2
but it's more important, actually,
what I didn't have to do.


331
00:20:45,612 --> 00:20:48,382 line:-2
You noticed, I did not load
any third-party packages,


332
00:20:48,448 --> 00:20:50,284 line:-1
because this is all part of OS.


333
00:20:50,350 --> 00:20:54,688 line:-2
All I used was UIKit,
Core Image, and Vision.


334
00:20:57,090 --> 00:21:00,427 line:-1
I also never left our image pipeline


335
00:21:00,494 --> 00:21:05,065 line:-2
while using our most optimal processing
path because I stayed in our pipeline.


336
00:21:06,300 --> 00:21:09,369 line:-2
There was no conversion of the images
into matrices,


337
00:21:09,436 --> 00:21:12,706 line:-2
and with that,
I really saved all my memory,


338
00:21:12,773 --> 00:21:14,942 line:-1
and also a lot of computational cost.


339
00:21:17,711 --> 00:21:19,713 line:-1
So that was Contour Detection.


340
00:21:20,414 --> 00:21:22,950 line:-1
Next, let's go to Optical Flow.


341
00:21:23,684 --> 00:21:25,352 line:-1
So, what is Optical Flow?


342
00:21:26,653 --> 00:21:29,423 line:-2
We want to analyze
the movement between two frames.


343
00:21:31,024 --> 00:21:33,760 line:-2
Traditionally,
we might have used registration.


344
00:21:34,528 --> 00:21:37,431 line:-2
That has been a part of Vision
for quite a while.


345
00:21:37,497 --> 00:21:40,100 line:-2
It gives me the alignment
of the whole image.


346
00:21:40,167 --> 00:21:41,835 line:-1
Let's look at an example here.


347
00:21:42,936 --> 00:21:44,137 line:-1
We have these two dots,


348
00:21:44,204 --> 00:21:46,840 line:-2
and let's see if we take this
as a picture with our camera,


349
00:21:46,907 --> 00:21:48,509 line:-1
and then we shift our camera.


350
00:21:49,610 --> 00:21:53,347 line:-2
Now, these two dots
moved to the top and to the right.


351
00:21:55,482 --> 00:21:58,552 line:-2
The registration will give me
the alignment between the two images


352
00:21:58,619 --> 00:22:02,356 line:-2
by telling me how much the image
has moved up and to the right.


353
00:22:04,258 --> 00:22:06,827 line:-2
Optical Flow, on the other hand,
is different.


354
00:22:06,894 --> 00:22:09,496 line:-2
It gives me a per pixel flow
between X and Y,


355
00:22:09,563 --> 00:22:11,398 line:-1
and that is new in Vision this year.


356
00:22:12,566 --> 00:22:15,002 line:-2
In our example, again,
we have our two dots...


357
00:22:16,503 --> 00:22:18,405 line:-1
but now, they've actually moved apart.


358
00:22:19,940 --> 00:22:23,377 line:-2
So, the image registration
is not going to pick this up correctly.


359
00:22:23,443 --> 00:22:25,379 line:-1
But I can use the Optical Flow,


360
00:22:25,445 --> 00:22:28,582 line:-2
because it's going to tell me,
for each pixel, how they have moved.


361
00:22:29,683 --> 00:22:31,818 line:-1
Let's look at the results of Optical Flow.


362
00:22:34,154 --> 00:22:37,724 line:-2
From the Optical Flow,
I get the VNPixelBufferObservation.


363
00:22:37,791 --> 00:22:39,726 line:-1
It's a floating point image.


364
00:22:39,793 --> 00:22:42,362 line:-1
It has the interleaved X and Y movement.


365
00:22:44,631 --> 00:22:47,267 line:-1
So, when we have a video like this,


366
00:22:47,334 --> 00:22:50,838 line:-2
you can imagine that perhaps
just looking at these values on its own


367
00:22:50,904 --> 00:22:53,607 line:-2
will be really hard to visualize
what's going on.


368
00:22:53,674 --> 00:22:57,311 line:-2
Because they're really just meant
for processing in later algorithms.


369
00:22:58,178 --> 00:22:59,847 line:-1
But if I want to check it out,


370
00:22:59,913 --> 00:23:03,317 line:-2
I can actually use Core Image
to visualize our results.


371
00:23:03,383 --> 00:23:06,553 line:-2
And as David was teasing on
earlier in our session,


372
00:23:06,620 --> 00:23:08,989 line:-1
there is a way of doing this.


373
00:23:09,056 --> 00:23:14,428 line:-2
We created a little custom kernel,
and now, you can see how everything moves.


374
00:23:14,494 --> 00:23:17,664 line:-2
I have a color-coding that shows me
the intensity of the movement


375
00:23:17,731 --> 00:23:21,068 line:-2
and the little triangles actually show me
the direction of the movement.


376
00:23:22,736 --> 00:23:24,605 line:-1
Let me quickly show you how we did this.


377
00:23:25,372 --> 00:23:27,641 line:-1
So, we wrote a custom filter.


378
00:23:27,708 --> 00:23:29,276 line:-1
I need to load the kernel,


379
00:23:29,343 --> 00:23:32,312 line:-2
which we'll make available
in the slide attachments to you,


380
00:23:32,379 --> 00:23:34,915 line:-2
and then, all I have to do
is basically apply this kernel


381
00:23:34,982 --> 00:23:38,185 line:-2
with the parameters for the size
of the arrow that I want,


382
00:23:38,252 --> 00:23:40,187 line:-1
and run it as a filter.


383
00:23:40,254 --> 00:23:43,323 line:-2
Now, in my Vision code,
all I'm going to do is,


384
00:23:43,390 --> 00:23:46,994 line:-1
I run my VNGenerateOpticalFlowRequest,


385
00:23:47,060 --> 00:23:53,166 line:-2
I get my observations to pixelBuffer,
which I can just now wrap into a CIImage,


386
00:23:53,233 --> 00:23:55,936 line:-2
and then,
I simply feed that into my filter,


387
00:23:56,003 --> 00:23:57,638 line:-1
and get the output image back.


388
00:23:58,906 --> 00:24:01,208 line:-2
So, let's wrap up
what we've talked about today.


389
00:24:02,176 --> 00:24:04,878 line:-1
Computer Vision doesn't have to be hard,


390
00:24:04,945 --> 00:24:07,881 line:-1
and it really enhances your application.


391
00:24:07,948 --> 00:24:11,618 line:-2
Our native APIs
make it fast and easy to adopt.


392
00:24:11,685 --> 00:24:13,720 line:-1
And by combining these things together,


393
00:24:13,787 --> 00:24:16,423 line:-2
you really can create
something interesting.


394
00:24:17,124 --> 00:24:19,359 line:-2
I'm looking forward
to all the great applications


395
00:24:19,426 --> 00:24:21,328 line:-2
and great innovations
that you're going to bring out.


396
00:24:22,129 --> 00:24:25,966 line:-2
Thank you for attending our session,
and have a great rest of the WWDC.

