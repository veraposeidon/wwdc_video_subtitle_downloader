1
00:00:03,904 --> 00:00:07,040 line:-1
Hello and welcome to WWDC.


2
00:00:08,876 --> 00:00:12,746 line:0
I am Yuxin,
an engineer on the Create ML team.


3
00:00:13,313 --> 00:00:15,983 line:-1
Today, my colleague, Alex, and I


4
00:00:16,049 --> 00:00:19,520 line:-2
are very excited
to introduce a new template:


5
00:00:20,020 --> 00:00:23,357 line:-1
Action Classification in Create ML.


6
00:00:24,024 --> 00:00:28,495 line:0
Last year,
we introduced Activity Classification,


7
00:00:28,562 --> 00:00:32,698 line:0
which allows you to create a classifier
using motion data.


8
00:00:33,834 --> 00:00:38,639 line:-2
But what if you wanted it to classify
actions from videos?


9
00:00:39,173 --> 00:00:41,675 line:-1
Cameras are ubiquitous today,


10
00:00:42,209 --> 00:00:47,681 line:-2
and it's become so easy to record
our own videos with our phones.


11
00:00:48,115 --> 00:00:50,817 line:-1
Whether we are in a gym or at home,


12
00:00:51,318 --> 00:00:55,289 line:-2
a self-guided workout
could easily brighten our days.


13
00:00:57,257 --> 00:01:00,727 line:-1
Other activities, such as gymnastics,


14
00:01:00,794 --> 00:01:04,431 line:-2
reveal the complexity
of human body movements.


15
00:01:05,098 --> 00:01:08,068 line:-1
Automatic understanding of these actions


16
00:01:08,135 --> 00:01:12,139 line:-2
could be helpful
for athletes' training and competition.


17
00:01:13,740 --> 00:01:19,813 line:-2
This year, we built Action Classification
to learn from human body poses.


18
00:01:20,314 --> 00:01:23,350 line:-2
Look at the sequence
of this amazing dance.


19
00:01:24,051 --> 00:01:27,321 line:-2
It would be interesting
to recognize these moves


20
00:01:27,387 --> 00:01:30,357 line:-2
and even build
an entertainment app around it.


21
00:01:31,358 --> 00:01:34,328 line:-1
So, what is Action Classification?


22
00:01:35,429 --> 00:01:38,565 line:-1
First, it's a standard classification task


23
00:01:38,632 --> 00:01:41,869 line:-2
that has the goal
to assign a label of action


24
00:01:41,935 --> 00:01:44,705 line:-1
from a list of your predefined classes.


25
00:01:46,406 --> 00:01:51,645 line:-2
This year, the model is powered by
Vision's human body pose estimation.


26
00:01:52,679 --> 00:01:57,184 line:-2
And therefore,
it works best with human body actions


27
00:01:57,251 --> 00:02:01,088 line:-2
but not with actions
from animals or objects.


28
00:02:03,123 --> 00:02:08,428 line:-2
To recognize an action over time,
a single image won't be enough.


29
00:02:09,562 --> 00:02:13,567 line:-2
Instead,
the prediction happens to a time window


30
00:02:13,634 --> 00:02:16,837 line:-1
that consists of a sequence of frames.


31
00:02:18,639 --> 00:02:20,574 line:-1
For a camera or video stream,


32
00:02:21,108 --> 00:02:25,445 line:-2
predictions could be made,
window by window, continuously.


33
00:02:26,780 --> 00:02:29,516 line:0
For more details about poses from Vision,


34
00:02:30,050 --> 00:02:33,921 line:0
please check our session
"Body and Hand Pose Estimation."


35
00:02:35,322 --> 00:02:39,259 line:-2
Now, with Create ML,
let's see how it works.


36
00:02:40,894 --> 00:02:45,165 line:-2
Assume we'd like to create
a fitness classifier


37
00:02:45,232 --> 00:02:48,135 line:-1
to recognize a person's workout routines,


38
00:02:48,202 --> 00:02:52,472 line:-2
such as jumping jacks, lunges, squats,
and a few others.


39
00:02:53,574 --> 00:02:57,744 line:-2
First, I need to prepare some video clips
for each class,


40
00:02:58,545 --> 00:03:00,614 line:-1
then launch Create ML for training.


41
00:03:01,815 --> 00:03:05,285 line:-2
And finally,
save the Fitness Classifier model


42
00:03:05,352 --> 00:03:07,621 line:-1
and build our fitness training app.


43
00:03:09,022 --> 00:03:11,592 line:-1
Now, let's dive in to each step.


44
00:03:12,125 --> 00:03:16,096 line:-2
My colleague, Alex,
will first talk about data and training


45
00:03:16,163 --> 00:03:17,798 line:-1
for Action Classification.


46
00:03:19,399 --> 00:03:20,467 line:-1
Thanks, Yuxin.


47
00:03:21,034 --> 00:03:23,403 line:-2
Since I can't get to my gym class
right now,


48
00:03:23,470 --> 00:03:26,640 line:-2
an exercise app which can respond
to how I do workouts


49
00:03:26,707 --> 00:03:28,509 line:-1
seems like a good way to keep fit.


50
00:03:29,176 --> 00:03:32,446 line:-2
My name is Alex,
and I'm an engineer on the Create ML team.


51
00:03:33,347 --> 00:03:36,416 line:0
Today I'm going to show you
how to capture action videos


52
00:03:36,483 --> 00:03:40,420 line:0
and train a new Action Classifier model
using the Create ML app.


53
00:03:41,355 --> 00:03:44,491 line:-2
Let's start by capturing the actions
on video.


54
00:03:45,325 --> 00:03:48,028 line:-2
We want our model to be able to tell
when we're exercising


55
00:03:48,095 --> 00:03:49,830 line:-1
and which workout we're doing.


56
00:03:49,897 --> 00:03:52,432 line:-2
We need to collect videos
of each of the five workouts:


57
00:03:53,166 --> 00:03:56,470 line:-1
jumping jacks, squats, and three others.


58
00:03:57,538 --> 00:04:00,741 line:-2
Each video should contain
just one of the action types


59
00:04:00,807 --> 00:04:02,809 line:-1
and should have just one human subject.


60
00:04:03,544 --> 00:04:04,945 line:-1
After taking your video,


61
00:04:05,012 --> 00:04:08,315 line:-2
if there's extra movement
at the start or end of your video,


62
00:04:08,382 --> 00:04:11,051 line:-2
you might want to trim these off
in your photo app.


63
00:04:12,319 --> 00:04:15,022 line:-1
We identify actions using the whole body,


64
00:04:15,088 --> 00:04:17,891 line:-2
so make sure the camera can see
the arms and legs


65
00:04:17,958 --> 00:04:19,726 line:-1
throughout the range of motion.


66
00:04:19,793 --> 00:04:21,795 line:-1
You might need to step back a bit.


67
00:04:23,830 --> 00:04:26,934 line:-2
Now,
make a folder to hold each action type.


68
00:04:27,701 --> 00:04:31,405 line:-2
The folder name is the label the model
will predict for actions like this.


69
00:04:33,207 --> 00:04:37,211 line:-2
If you collected some examples of resting,
stretching, and sitting down,


70
00:04:37,277 --> 00:04:40,981 line:-2
you could put all these together
in a single folder called "other."


71
00:04:42,616 --> 00:04:44,184 line:-1
We're now ready for training.


72
00:04:44,785 --> 00:04:47,654 line:-2
But I want to take a moment
to consider what to do


73
00:04:47,721 --> 00:04:49,756 line:-1
if we have a different sort of data.


74
00:04:50,324 --> 00:04:52,593 line:-2
Perhaps someone else
prepared the video for us,


75
00:04:52,659 --> 00:04:54,628 line:-1
or we downloaded it from the Internet.


76
00:04:55,128 --> 00:04:57,097 line:-1
In that case, we have a montage.


77
00:04:57,931 --> 00:05:02,135 line:-2
With a montage, a single video contains
multiple different actions we need,


78
00:05:02,202 --> 00:05:06,273 line:-2
as well as titles, establishing shots,
and maybe people just resting.


79
00:05:08,108 --> 00:05:12,079 line:-2
Looking at this video in sequence,
we see periods of no action


80
00:05:12,145 --> 00:05:14,915 line:-2
mixed with specific actions
we want to train for.


81
00:05:15,482 --> 00:05:16,884 line:-1
We have two options here.


82
00:05:17,684 --> 00:05:23,357 line:-2
We can use video editing software to trim
or split videos into the actions we need,


83
00:05:23,423 --> 00:05:26,860 line:-2
and then put them into folders
like before, or...


84
00:05:28,028 --> 00:05:31,465 line:-2
we can find the times in the video
where the actions start and stop


85
00:05:31,532 --> 00:05:35,903 line:-2
and record those in an annotation file
in CSV or JSON format.


86
00:05:35,969 --> 00:05:37,237 line:-1
Here's an example.


87
00:05:38,539 --> 00:05:39,907 line:-1
You can find out more


88
00:05:39,973 --> 00:05:44,011 line:-2
in the Create ML Documentation
on developer.apple.com.


89
00:05:45,279 --> 00:05:46,747 line:-1
Let's set that aside, though,


90
00:05:46,813 --> 00:05:50,150 line:-2
and keep going with the exercise videos
we already prepared.


91
00:05:52,286 --> 00:05:55,956 line:-2
On my Mac, I'm going to start up
the Create ML application.


92
00:05:58,692 --> 00:06:00,928 line:-1
If you are using Mac OS Big Sur,


93
00:06:01,862 --> 00:06:04,998 line:-2
it has a new template:
Action Classification.


94
00:06:06,733 --> 00:06:10,437 line:-2
Let's create an Action Classification
project for our exercise model.


95
00:06:13,540 --> 00:06:15,075 line:-1
We give it a suitable name.


96
00:06:17,811 --> 00:06:21,315 line:-2
And I'm going to add a description
so I can recognize it later on.


97
00:06:30,357 --> 00:06:35,062 line:-2
Our new Action Classification project
opens on the Model Source Settings page.


98
00:06:35,629 --> 00:06:39,099 line:-2
This is where we prepare for training,
by adding the data we captured


99
00:06:39,166 --> 00:06:41,802 line:-2
and making some decisions
about the training process.


100
00:06:42,870 --> 00:06:46,073 line:-2
We're going to need to drag the folder
of videos we already prepared


101
00:06:46,139 --> 00:06:47,708 line:-1
into the training data source.


102
00:06:48,609 --> 00:06:50,377 line:-1
Let's take a look at that now.


103
00:06:52,813 --> 00:06:56,250 line:-2
I've collected all the videos
in a "train" folder,


104
00:06:56,884 --> 00:07:00,821 line:-2
which has sub-folders named
for each of the actions we want to train.


105
00:07:02,122 --> 00:07:04,124 line:-1
Let's have a look in the "jacks" folder.


106
00:07:04,458 --> 00:07:06,994 line:-2
It contains all of the videos
of jumping jacks.


107
00:07:10,864 --> 00:07:14,001 line:-2
Let's drag that "train" folder
into the training data well.


108
00:07:15,402 --> 00:07:17,971 line:-2
The Create ML app
checks the data's in the right format


109
00:07:18,038 --> 00:07:19,740 line:-1
and tells us a little about it.


110
00:07:20,307 --> 00:07:22,409 line:-1
It says we've recorded seven classes.


111
00:07:23,010 --> 00:07:26,313 line:-2
That's the five exercises
plus two negative classes:


112
00:07:26,380 --> 00:07:27,714 line:-1
"other" and "none."


113
00:07:28,815 --> 00:07:33,453 line:-2
It also says we recorded 362 items.
Videos.


114
00:07:33,520 --> 00:07:36,056 line:-2
We can dive into this
using the data source view.


115
00:07:38,192 --> 00:07:41,662 line:-2
Here we can see that each action
has around 50 videos.


116
00:07:43,530 --> 00:07:46,266 line:-2
That's what you should aim for
when building your project.


117
00:07:49,736 --> 00:07:52,706 line:-2
Below the data inputs
are some parameters we can set.


118
00:07:52,773 --> 00:07:57,044 line:-2
There's two I want to talk about here:
Action Duration and Augmentations.


119
00:07:58,178 --> 00:08:00,948 line:-2
Action Duration
is a really important parameter.


120
00:08:01,014 --> 00:08:04,351 line:-2
It represents the length of the action
we're trying to recognize.


121
00:08:04,418 --> 00:08:07,855 line:-2
If we're working with a complex action,
like a dance move,


122
00:08:07,921 --> 00:08:10,490 line:-2
this needs to be long enough
to capture the whole motion,


123
00:08:10,557 --> 00:08:12,459 line:-1
which might be ten seconds.


124
00:08:14,628 --> 00:08:18,398 line:-2
But for short, repeating actions,
like our exercise reps,


125
00:08:18,465 --> 00:08:20,968 line:-2
we should set the window
to around two seconds.


126
00:08:22,870 --> 00:08:25,873 line:-2
The length of the action,
also known as prediction window,


127
00:08:25,939 --> 00:08:27,774 line:-1
is actually measured in frames,


128
00:08:28,342 --> 00:08:32,578 line:-2
so Create ML automatically calculates
the number of frames, 60,


129
00:08:33,313 --> 00:08:36,216 line:-2
based upon the frame rate
and action duration.


130
00:08:39,219 --> 00:08:42,655 line:-2
Augmentations are a way
to boost your training data,


131
00:08:42,722 --> 00:08:45,025 line:-2
without going out
and recording more videos,


132
00:08:45,092 --> 00:08:47,160 line:-1
by transforming the ones you already have


133
00:08:47,227 --> 00:08:49,730 line:-2
in ways which represent
real-world scenarios.


134
00:08:50,831 --> 00:08:54,067 line:-2
If all of our videos were taken
with a person facing to the left,


135
00:08:54,134 --> 00:08:57,437 line:-2
then "horizontal flip" generates
a mirror-image video


136
00:08:57,504 --> 00:09:00,607 line:-2
that make the model work better
in both orientations.


137
00:09:01,475 --> 00:09:02,676 line:-1
Let's turn it on.


138
00:09:04,745 --> 00:09:08,148 line:-2
You'll have noticed two other boxes
you can add data to.


139
00:09:08,215 --> 00:09:10,450 line:-1
These are Validation and Testing.


140
00:09:11,318 --> 00:09:14,221 line:-2
If you have extra data set aside
to test your model,


141
00:09:14,288 --> 00:09:16,790 line:-2
you can add that now
to the testing data well.


142
00:09:17,558 --> 00:09:19,960 line:-2
Create ML will automatically
perform tests on it


143
00:09:20,027 --> 00:09:21,695 line:-1
when the model is done training.


144
00:09:23,297 --> 00:09:27,301 line:-2
Machine learning veterans might like
to choose their own validation data,


145
00:09:27,367 --> 00:09:28,936 line:-1
but, by default,


146
00:09:29,002 --> 00:09:32,206 line:-2
Create ML will automatically use
some of your training data for this.


147
00:09:33,373 --> 00:09:36,143 line:-2
Let's hit the "train" button
to begin making our model.


148
00:09:40,047 --> 00:09:43,483 line:-2
When your Mac is training
an Action Classifier from videos,


149
00:09:43,550 --> 00:09:45,385 line:-1
it does this in two parts.


150
00:09:45,786 --> 00:09:48,021 line:-1
First, it does feature extraction,


151
00:09:48,088 --> 00:09:51,124 line:-2
which takes the videos
and works out the human poses.


152
00:09:51,758 --> 00:09:53,927 line:-2
Once that's complete,
it can train the model.


153
00:09:58,365 --> 00:10:01,602 line:-2
Feature extraction is a big task
and can take a while,


154
00:10:01,668 --> 00:10:03,470 line:-1
so let's dig a little deeper.


155
00:10:05,739 --> 00:10:10,077 line:-2
Using the power of the Vision API,
we look at every frame of our video


156
00:10:10,143 --> 00:10:12,880 line:-2
and encode the position of 18 landmarks
on the body,


157
00:10:12,946 --> 00:10:16,917 line:-2
including hands, legs, hips, eyes,
etcetera.


158
00:10:17,985 --> 00:10:21,021 line:-2
For each landmark,
it records x and y coordinates,


159
00:10:21,088 --> 00:10:22,389 line:-1
plus a confidence,


160
00:10:22,456 --> 00:10:24,825 line:-2
and these form the feature
that we use for training.


161
00:10:26,093 --> 00:10:28,362 line:-2
Let's go back and find out
how it's getting on.


162
00:10:29,763 --> 00:10:32,733 line:-2
The feature extraction is going to take
about half an hour,


163
00:10:32,799 --> 00:10:35,636 line:-2
and we don't want to wait,
so I'm going to stop this training process


164
00:10:35,702 --> 00:10:38,772 line:-2
and open a project
with a model that we made earlier.


165
00:10:39,907 --> 00:10:41,375 line:-1
[clicking]


166
00:10:44,378 --> 00:10:48,415 line:-2
This model's already finished training
on the same data that we used before.


167
00:10:48,482 --> 00:10:50,384 line:-1
I also added some validation data.


168
00:10:51,919 --> 00:10:55,522 line:-2
You can see the final report
in the training and evaluation tabs.


169
00:10:57,758 --> 00:11:01,328 line:-2
We can see how the model performance
improved over time.


170
00:11:01,395 --> 00:11:04,531 line:-2
It looks like 80 iterations
was a reasonable choice.


171
00:11:04,598 --> 00:11:08,035 line:-2
The line going up and to the right
and then flattening out


172
00:11:08,101 --> 00:11:10,470 line:-2
means that training has reached
a stable state.


173
00:11:12,606 --> 00:11:16,476 line:-2
I can review the performance per class
using the evaluation tab.


174
00:11:17,277 --> 00:11:20,113 line:-2
This lets me check that the model
will perform equally well


175
00:11:20,180 --> 00:11:22,616 line:-2
across each kind of action
we want to recognize.


176
00:11:23,450 --> 00:11:28,222 line:-2
If we added validation or test data,
you could see the results here.


177
00:11:30,524 --> 00:11:33,660 line:-2
But what I really want
is to see the model in action.


178
00:11:34,261 --> 00:11:35,762 line:-1
Let's go to the preview tab,


179
00:11:35,829 --> 00:11:39,600 line:-2
where we can try the model out on
new videos we haven't used for training.


180
00:11:40,534 --> 00:11:43,303 line:-2
Here's one I recorded
in my garden yesterday.


181
00:11:46,974 --> 00:11:49,009 line:-1
The video is processed to find the poses,


182
00:11:49,076 --> 00:11:51,945 line:-2
and then the model classifies these
into actions.


183
00:11:52,813 --> 00:11:55,716 line:-2
Right now,
it's classifying the whole video up front,


184
00:11:55,782 --> 00:11:58,719 line:-2
but when you make your own app,
you can choose to stream it


185
00:11:58,785 --> 00:12:02,456 line:-2
if you're working with live videos
or want a responsive experience.


186
00:12:03,524 --> 00:12:06,493 line:-2
Let's press "play"
to see what it's detected.


187
00:12:07,628 --> 00:12:09,730 line:-1
Look out for the label on the video.


188
00:12:09,796 --> 00:12:13,967 line:-2
You can see the classification change
as the action in the video unfolds.


189
00:12:15,669 --> 00:12:18,272 line:-1
Let's see it one more time.


190
00:12:19,740 --> 00:12:23,877 line:-2
You can see that pose skeleton
superimposed over the video.


191
00:12:24,378 --> 00:12:25,646 line:-1
We can turn that off.


192
00:12:30,184 --> 00:12:34,922 line:-2
Or we can just watch the pose skeleton
exercising in the dark.


193
00:12:37,824 --> 00:12:40,294 line:0
Below the video, we can see the timeline,


194
00:12:40,894 --> 00:12:43,964 line:0
which has divided the video
into two-second windows.


195
00:12:44,031 --> 00:12:45,599 line:0
Remember when we chose that?


196
00:12:45,666 --> 00:12:48,602 line:0
And it shows the best prediction
for each window underneath,


197
00:12:49,570 --> 00:12:52,172 line:0
as well as other predictions
with lower probability.


198
00:12:54,007 --> 00:12:56,944 line:-2
I think we're ready to make a great app
using this model.


199
00:12:57,010 --> 00:12:59,913 line:-2
For this, I need to export the model
from the project.


200
00:12:59,980 --> 00:13:02,249 line:-1
And we can do this on the output tab.


201
00:13:02,950 --> 00:13:06,019 line:-2
On the output tab, we can also see
some facts about the model,


202
00:13:06,086 --> 00:13:07,554 line:-1
including the size,


203
00:13:07,621 --> 00:13:10,958 line:-2
which is an important consideration
for assets in mobile apps,


204
00:13:11,024 --> 00:13:15,395 line:-2
and you can find out what operating system
versions this model is supported on.


205
00:13:16,630 --> 00:13:19,733 line:-2
Let's save the model
by dragging the icon into the finder.


206
00:13:22,302 --> 00:13:24,338 line:-1
FitnessClassifier.MLmodel.


207
00:13:27,207 --> 00:13:29,376 line:-1
And now we can share it with Yuxin,


208
00:13:29,443 --> 00:13:32,880 line:-2
who's going to show us how to build
an awesome iOS fitness app.


209
00:13:33,347 --> 00:13:38,819 line:-2
Now we've got a classifier from Alex
to drive our fitness training app.


210
00:13:39,686 --> 00:13:43,857 line:-2
Let's first check out
how to use the model to make a prediction.


211
00:13:44,892 --> 00:13:46,393 line:-1
For example,


212
00:13:46,460 --> 00:13:52,099 line:-2
we'd like to recognize jumping jacks
either from a camera or a video file.


213
00:13:53,901 --> 00:13:59,306 line:-2
The model, however, takes poses,
rather than a video, as the input.


214
00:14:00,274 --> 00:14:06,813 line:-2
To extract poses, we use Vision's API:
VNDetectHumanBodyPoseRequest.


215
00:14:08,382 --> 00:14:10,951 line:-1
If we are working with a video URL,


216
00:14:11,685 --> 00:14:16,790 line:-2
VNVideoProcessor could handle
the pose request for an entire video,


217
00:14:17,624 --> 00:14:22,829 line:-2
and pose results for every frame
can be obtained in a completion handler.


218
00:14:24,765 --> 00:14:28,569 line:-2
Alternatively,
when working with a camera stream,


219
00:14:28,635 --> 00:14:32,005 line:-1
we could use VNImageRequestHandler


220
00:14:32,072 --> 00:14:35,909 line:-2
to perform the same pose request
for each captured image.


221
00:14:37,845 --> 00:14:41,648 line:-2
Regardless of how we get poses
from each frame,


222
00:14:41,715 --> 00:14:45,385 line:-2
we need to aggregate them
into a prediction window


223
00:14:45,452 --> 00:14:49,022 line:-2
in a three-dimensional array
as the model input.


224
00:14:51,625 --> 00:14:53,727 line:-1
This looks complicated,


225
00:14:53,794 --> 00:14:58,732 line:-2
but if we use the convenient
keypointsMultiArray API from Vision,


226
00:14:58,799 --> 00:15:01,268 line:-1
we don't have to deal with the details.


227
00:15:02,302 --> 00:15:06,073 line:0
Since our fitness model's window size
is 60,


228
00:15:06,373 --> 00:15:10,711 line:0
we could simply concatenate poses
from 60 frames


229
00:15:10,777 --> 00:15:12,679 line:0
to make one prediction window.


230
00:15:14,648 --> 00:15:16,583 line:-1
With such a window prepared,


231
00:15:16,650 --> 00:15:19,653 line:-2
it can be passed to our model
as the input.


232
00:15:20,621 --> 00:15:24,157 line:-2
And finally,
the model output result includes


233
00:15:24,224 --> 00:15:27,928 line:-2
a top predicted action name
and a confidence score.


234
00:15:29,162 --> 00:15:33,133 line:-2
Now let's take a tour of these steps
in Xcode.


235
00:15:35,936 --> 00:15:38,472 line:-1
This is our fitness training app.


236
00:15:38,972 --> 00:15:40,240 line:-1
Let's open it.


237
00:15:42,843 --> 00:15:46,480 line:-2
And this is the Fitness Classifier
we've just trained.


238
00:15:47,514 --> 00:15:53,053 line:-2
This Metadata page shows us
relevant user and model information,


239
00:15:53,120 --> 00:15:57,558 line:-2
such as the author, description,
class label names,


240
00:15:57,624 --> 00:15:59,927 line:-1
and even layer distributions.


241
00:16:03,330 --> 00:16:09,536 line:-2
The Predictions page shows us detailed
model input and output information,


242
00:16:10,204 --> 00:16:14,107 line:-2
such as the required input
multi-array dimensions,


243
00:16:14,374 --> 00:16:17,911 line:-2
as well as the names and types
for the output.


244
00:16:19,279 --> 00:16:24,651 line:-2
Now let's skip the other app logic
and jump right to the model prediction.


245
00:16:28,822 --> 00:16:30,457 line:-1
In my predictor,


246
00:16:31,124 --> 00:16:35,629 line:-2
I first initialize
the Fitness Classifier model,


247
00:16:35,696 --> 00:16:38,799 line:-1
as well as the Vision body pose request.


248
00:16:39,199 --> 00:16:41,435 line:-1
It's better to do this only once.


249
00:16:43,604 --> 00:16:46,874 line:-2
Since the model takes a prediction window
as the input,


250
00:16:47,441 --> 00:16:49,676 line:-1
I also maintain a window buffer


251
00:16:49,743 --> 00:16:53,914 line:-2
to save the last 60 poses
from the past two seconds.


252
00:16:54,781 --> 00:16:59,853 line:-2
Sixty is the model's prediction
window size we have used for training.


253
00:17:07,361 --> 00:17:13,032 line:-2
When a frame is received from the camera,
we need to extract the poses.


254
00:17:16,770 --> 00:17:19,373 line:-1
This involves calling Vision APIs


255
00:17:19,439 --> 00:17:24,044 line:-2
and performing pose requests
that we have already seen on the slides.


256
00:17:25,244 --> 00:17:29,082 line:-2
Before we add the extracted poses
to our window...


257
00:17:30,484 --> 00:17:34,788 line:-2
remember that Action Classifier
takes only a single person,


258
00:17:35,222 --> 00:17:38,659 line:-2
so we need to implement some
person-selection logic


259
00:17:38,725 --> 00:17:41,228 line:-1
if Vision detects multiple people.


260
00:17:41,862 --> 00:17:43,363 line:-1
Here in this app,


261
00:17:43,430 --> 00:17:47,801 line:-2
I simply choose the most prominent person
based on their size,


262
00:17:47,868 --> 00:17:50,370 line:-1
and you may choose to do it in other ways.


263
00:17:51,471 --> 00:17:54,141 line:-1
Now, let's add the pose to the window.


264
00:17:57,711 --> 00:18:01,949 line:-2
And once the window is full,
we can start making predictions.


265
00:18:02,916 --> 00:18:06,520 line:-2
This app makes a prediction
every half a second,


266
00:18:06,587 --> 00:18:12,593 line:-2
but you can decide a different interval
or trigger based on your own use cases.


267
00:18:13,527 --> 00:18:16,230 line:-2
Now let's move on
to make a model prediction.


268
00:18:18,098 --> 00:18:19,967 line:-1
To prepare the model input,


269
00:18:20,267 --> 00:18:25,172 line:-2
I need to convert each pose
from the window into a multi-array


270
00:18:26,106 --> 00:18:30,077 line:-2
using Vision's convenient API,
keypointsMultiArray.


271
00:18:30,844 --> 00:18:35,616 line:-2
And if no person is detected,
simply pad zeros.


272
00:18:38,151 --> 00:18:43,724 line:-2
Then we concatenate 60
of these multi-arrays into a single array,


273
00:18:43,790 --> 00:18:45,759 line:-1
and that's our model input.


274
00:18:47,594 --> 00:18:51,932 line:-2
Finally, it's just the one line of code
to make a prediction.


275
00:18:54,301 --> 00:18:58,972 line:-2
The output includes
a top-predicted label name


276
00:18:59,039 --> 00:19:03,110 line:-2
and the confidences in a dictionary
for all the classes.


277
00:19:03,810 --> 00:19:07,681 line:-2
In the end,
don't forget to reset the pose window


278
00:19:07,748 --> 00:19:12,486 line:-2
so that once it is filled again,
we're ready to make another prediction.


279
00:19:14,588 --> 00:19:19,826 line:-2
And that's everything about making
predictions with an Action Classifier.


280
00:19:19,893 --> 00:19:23,897 line:-2
Now let's try out this app
and see how it works in action.


281
00:19:25,465 --> 00:19:27,768 line:-1
I feel some extra energy today


282
00:19:27,835 --> 00:19:31,038 line:-2
and would like to challenge myself
to a little workout.


283
00:19:32,539 --> 00:19:33,874 line:-1
Let's open the app...


284
00:19:35,042 --> 00:19:38,011 line:-2
and hit the "start" button
to begin a workout.


285
00:19:39,546 --> 00:19:43,851 line:-2
Now my poses are extracted
from every incoming frame,


286
00:19:43,917 --> 00:19:46,753 line:-1
and predictions are made continuously,


287
00:19:47,120 --> 00:19:50,157 line:-1
as displayed at the bottom of the app.


288
00:19:50,958 --> 00:19:54,561 line:-2
That's our Debugging View:
confidence and the labels.


289
00:19:55,395 --> 00:19:57,231 line:-1
Now, let me get started.


290
00:20:05,906 --> 00:20:08,342 line:-1
Wow. I finished a five-second challenge.


291
00:20:09,243 --> 00:20:14,381 line:-2
As you may have seen,
as soon as the model recognized my action,


292
00:20:14,915 --> 00:20:16,149 line:-1
the timer starts,


293
00:20:16,817 --> 00:20:18,886 line:-1
and as soon as I stopped,


294
00:20:18,952 --> 00:20:22,990 line:-2
the model recognized my action
as "other action" class now


295
00:20:23,690 --> 00:20:26,994 line:-2
and starts to wait for next challenge,
lunges.


296
00:20:27,594 --> 00:20:28,595 line:-1
I'm ready.


297
00:20:37,971 --> 00:20:41,909 line:-2
[sighs] I finished lunges,
but I am not in a hurry yet


298
00:20:41,975 --> 00:20:45,312 line:-2
and would like to take a rest
and drink some water.


299
00:20:50,784 --> 00:20:53,353 line:-1
Everything happens interactively...


300
00:20:54,421 --> 00:20:58,592 line:-2
and I don't have to get back
and operate a device,


301
00:20:58,659 --> 00:21:02,329 line:-2
which is very convenient
for home exercises.


302
00:21:04,631 --> 00:21:07,000 line:-1
Lastly, let's try squats.


303
00:21:15,742 --> 00:21:18,612 line:-1
Now all three challenges are finished,


304
00:21:18,679 --> 00:21:22,082 line:-2
and here is the summary
of my time spent for each.


305
00:21:22,649 --> 00:21:25,252 line:-1
Wow. That's a lot of exercise for today.


306
00:21:27,221 --> 00:21:33,393 line:-2
Now we have covered how to train an
Action Classifier in making predictions.


307
00:21:33,894 --> 00:21:39,433 line:-2
Let's invite Alex back and wrap up
this session with some best practices.


308
00:21:40,234 --> 00:21:41,235 line:-1
Thanks, Yuxin.


309
00:21:41,602 --> 00:21:44,771 line:-2
I love the way this application
waits for me to start my workout.


310
00:21:44,838 --> 00:21:47,374 line:-2
I often need time to remember
how an exercise is done


311
00:21:47,441 --> 00:21:49,810 line:-1
and maybe which arm or leg is moved first.


312
00:21:50,410 --> 00:21:53,080 line:-2
Building great features like that
needs great data,


313
00:21:53,146 --> 00:21:55,449 line:-2
so let's make sure
we get the best performance


314
00:21:55,516 --> 00:21:59,520 line:-2
out of the athletes, gamers and kids
who bring our videos to life.


315
00:22:00,888 --> 00:22:05,359 line:-2
Your model will train best
if it's exposed to repetition and variety.


316
00:22:07,060 --> 00:22:08,529 line:-1
Earlier,


317
00:22:08,595 --> 00:22:13,000 line:-2
I made sure I had around 50 videos
for each exercise I wanted to classify,


318
00:22:13,066 --> 00:22:14,935 line:-1
and you should, too, in your apps.


319
00:22:15,869 --> 00:22:18,505 line:-2
Make sure you train
with a variety of different people.


320
00:22:18,572 --> 00:22:21,475 line:-2
They'll bring different styles,
abilities and speeds,


321
00:22:21,542 --> 00:22:23,544 line:-1
which your model needs to recognize.


322
00:22:24,912 --> 00:22:27,114 line:-2
If you think your users
will move around a lot,


323
00:22:27,181 --> 00:22:31,251 line:-2
consider capturing the action from
the sides and back as well as the front.


324
00:22:33,420 --> 00:22:35,989 line:-2
The model does need
to understand exercises,


325
00:22:36,056 --> 00:22:39,126 line:-2
but it also needs to understand
when we're not exercising,


326
00:22:39,193 --> 00:22:40,661 line:-1
or not moving at all.


327
00:22:41,094 --> 00:22:46,033 line:-2
You could create two extra folders:
one for walking around and stretching,


328
00:22:46,099 --> 00:22:49,369 line:-2
and one for sitting
or doing nothing quietly.


329
00:22:53,173 --> 00:22:57,311 line:-2
Let's consider how to capture
great videos for the Action Classifier.


330
00:22:58,579 --> 00:23:00,480 line:-1
Any motion from the camera person


331
00:23:00,547 --> 00:23:03,217 line:-2
might be interpreted
as the subject moving around,


332
00:23:03,283 --> 00:23:08,222 line:-2
so let's keep the camera stable using
a tripod or resting it somewhere solid.


333
00:23:10,390 --> 00:23:14,027 line:-2
The pose detector needs to clearly see
the parts of the body,


334
00:23:14,094 --> 00:23:17,831 line:-2
so if your outfit blends into
the background, it won't work as well,


335
00:23:17,898 --> 00:23:21,835 line:-2
and flowing clothing might conceal
the movement you're trying to detect.


336
00:23:24,838 --> 00:23:28,308 line:-2
Now you know how to take the best videos
for your Action Classifier.


337
00:23:28,809 --> 00:23:32,813 line:-2
Yuxin, can you tell us how to get the most
out of training in Create ML?


338
00:23:35,015 --> 00:23:36,149 line:-1
Sure, Alex.


339
00:23:36,717 --> 00:23:42,022 line:-2
Once you have the data, take a moment
to configure your training parameters.


340
00:23:42,422 --> 00:23:46,793 line:-2
One key parameter is action duration,
in seconds,


341
00:23:46,860 --> 00:23:52,199 line:-2
or prediction window size, which is
number of frames in your time window.


342
00:23:52,933 --> 00:23:57,004 line:-2
The length should match the action length
in your videos,


343
00:23:57,070 --> 00:24:01,108 line:-2
and try to make all actions roughly
at the same duration.


344
00:24:03,410 --> 00:24:05,445 line:-1
Frame rate in your video


345
00:24:05,512 --> 00:24:09,383 line:-2
affects the effective length
of the prediction window...


346
00:24:10,384 --> 00:24:14,421 line:-2
so it's important to keep
the average frame rate consistent


347
00:24:14,488 --> 00:24:19,293 line:-2
between your training and testing videos
to get accurate results.


348
00:24:21,695 --> 00:24:25,699 line:-2
When it comes to using the model
in your applications,


349
00:24:25,766 --> 00:24:28,902 line:-1
make sure to only select a single person.


350
00:24:29,770 --> 00:24:32,005 line:-1
Your app may remind users


351
00:24:32,072 --> 00:24:36,210 line:-2
to keep only one person in view
when multiple people are detected,


352
00:24:36,643 --> 00:24:41,448 line:-2
or you can implement your own
selection logic to choose a person


353
00:24:41,515 --> 00:24:44,685 line:-2
based on their size
or location within the frame,


354
00:24:45,519 --> 00:24:50,424 line:-2
and this can be achieved by using
the coordinates from pose landmarks.


355
00:24:52,492 --> 00:24:56,430 line:-2
If you'd like to count the repetitions
of your actions,


356
00:24:56,496 --> 00:24:58,899 line:-2
you may need to prepare
each training video


357
00:24:58,966 --> 00:25:02,402 line:-2
to be just one repetition
of a single action.


358
00:25:03,270 --> 00:25:06,306 line:-1
And when you make predictions in your app,


359
00:25:06,373 --> 00:25:11,144 line:-2
you should find a reliable trigger
to start a prediction at the right time


360
00:25:11,912 --> 00:25:16,717 line:-2
or implement some smoothing logic
to properly update the counter.


361
00:25:18,919 --> 00:25:22,022 line:-1
Finally, you can use an Action Classifier


362
00:25:22,089 --> 00:25:25,259 line:-2
to score or judge
the quality of an action.


363
00:25:25,993 --> 00:25:30,831 line:-2
For example, your app could use
a prediction's confidence value


364
00:25:30,898 --> 00:25:33,767 line:-1
to score the quality of an action


365
00:25:33,834 --> 00:25:37,371 line:-2
compared to the example actions
in your training videos.


366
00:25:40,841 --> 00:25:45,779 line:-2
So, that's Action Classification
in Create ML.


367
00:25:46,146 --> 00:25:51,218 line:-2
We can't wait to see the amazing apps
you are going to create using it.


368
00:25:52,052 --> 00:25:57,424 line:-2
Thanks for joining us today,
and enjoy the rest of WWDC.

