1
00:00:03,836 --> 00:00:06,573 line:-1
Hello and welcome to WWDC.


2
00:00:07,541 --> 00:00:10,410 line:0
Hello, everybody.
I'm Rishi Verma from the Core Data team.


3
00:00:10,477 --> 00:00:12,646 line:0
This session,
we'll show you how to harness Core Data


4
00:00:12,713 --> 00:00:14,314 line:0
to best fit the needs of an application.


5
00:00:14,748 --> 00:00:16,950 line:-2
To start,
we'll investigate how to populate


6
00:00:17,017 --> 00:00:21,121 line:-2
and maintain your persistent store quickly
and efficiently with batch operations.


7
00:00:22,022 --> 00:00:24,291 line:-2
Then we'll go over
how to tailor a fetch request


8
00:00:24,358 --> 00:00:26,159 line:-1
to match the needs of an application.


9
00:00:26,827 --> 00:00:30,330 line:-2
Lastly, a few tips and tricks
on how an application could react


10
00:00:30,397 --> 00:00:32,432 line:-1
to changes in the persistent store.


11
00:00:32,900 --> 00:00:35,802 line:-2
Let's start with a look at our sample,
Earthquakes.


12
00:00:35,869 --> 00:00:39,907 line:-2
It is a Swift application
that has a view context to drive the UI


13
00:00:39,973 --> 00:00:42,109 line:-2
and a background context
to ingest the data


14
00:00:42,176 --> 00:00:44,978 line:-1
provided by the US Geological Survey.


15
00:00:45,045 --> 00:00:47,648 line:-2
Our sample has a local container
for our application


16
00:00:47,714 --> 00:00:50,784 line:-2
and gathers quake data
from the USGS's JSON feed.


17
00:00:51,985 --> 00:00:54,855 line:-2
Here, we send the JSON feed
to our JSON parser,


18
00:00:55,822 --> 00:00:59,426 line:-2
which in turn sends the data over
to our background context


19
00:00:59,493 --> 00:01:03,197 line:-2
to be turned into quake managed objects
and saved to our local store.


20
00:01:03,931 --> 00:01:07,401 line:-2
Our view context then merges changes
to magically update our UI.


21
00:01:08,769 --> 00:01:10,771 line:-2
Now, our background context
could be working


22
00:01:10,838 --> 00:01:12,706 line:-1
with a large number of managed objects


23
00:01:12,773 --> 00:01:16,543 line:-2
that are created or fetched only to be
discarded shortly after the save.


24
00:01:16,610 --> 00:01:18,979 line:-1
But this is where batch operations shine.


25
00:01:19,546 --> 00:01:21,014 line:-1
Batch operations allow developers


26
00:01:21,081 --> 00:01:24,351 line:-2
to do inserts,
updates and deletes with ease,


27
00:01:24,418 --> 00:01:26,587 line:-1
while being as minimal as possible.


28
00:01:26,653 --> 00:01:30,290 line:-2
Due to the minimalist nature
of these operations, a few caveats.


29
00:01:30,757 --> 00:01:32,826 line:-1
There are no save notifications posted


30
00:01:32,893 --> 00:01:35,195 line:-2
because, well,
obviously we're not doing a save here.


31
00:01:35,863 --> 00:01:38,498 line:-2
And because we did not manifest
the managed objects,


32
00:01:38,565 --> 00:01:41,969 line:-2
we do not get any callback
or accessor logic for our changes.


33
00:01:42,369 --> 00:01:43,370 line:-1
But wait.


34
00:01:43,437 --> 00:01:46,440 line:-2
You can work around these two caveats
with persistent history.


35
00:01:47,207 --> 00:01:50,744 line:-2
Enable persistent history,
and your batch operations are captured


36
00:01:50,811 --> 00:01:53,380 line:-2
so that a notification
can easily be acquired,


37
00:01:53,447 --> 00:01:56,316 line:-2
and now we've worked around
that first caveat of the batch operations.


38
00:01:56,383 --> 00:01:58,986 line:-1
As for the callbacks and accessor logic,


39
00:01:59,052 --> 00:02:01,989 line:-2
we can accommodate that
by parsing our persistent history


40
00:02:02,055 --> 00:02:05,559 line:-2
for the relevant changes
to our application's current view.


41
00:02:05,626 --> 00:02:08,395 line:-2
How about we dive deeper
into batch operations?


42
00:02:08,461 --> 00:02:11,098 line:-2
The first thing
our applications generally do


43
00:02:11,164 --> 00:02:15,369 line:-2
is load data into the persistent store,
which in turn drives our UI.


44
00:02:15,435 --> 00:02:18,272 line:-1
When we introduced NSBatchInsertRequest,


45
00:02:18,338 --> 00:02:20,841 line:-2
it gave developers
the power of a batch operation


46
00:02:20,908 --> 00:02:22,342 line:-1
for data ingestion.


47
00:02:22,409 --> 00:02:25,913 line:-2
It streamlined the ability
to ingest a ton of data,


48
00:02:25,979 --> 00:02:29,416 line:-2
and now we have expanded the abilities
of NSBatchInsertRequest.


49
00:02:30,384 --> 00:02:32,853 line:-1
Initially, we gave developers the ability


50
00:02:32,920 --> 00:02:35,856 line:-2
to pass an array of dictionaries
for a batch insert.


51
00:02:35,923 --> 00:02:39,226 line:-2
The array of dictionaries represented
the objects to create


52
00:02:39,293 --> 00:02:42,329 line:-2
with the keys as attribute names
and their assigned values.


53
00:02:43,163 --> 00:02:46,533 line:0
And we have a new addition,
an initializer that allows developers


54
00:02:46,600 --> 00:02:50,370 line:0
to give a block to fill out
a given dictionary or managed object.


55
00:02:50,437 --> 00:02:53,040 line:-2
This greatly reduces the peak memory
of an ingestion


56
00:02:53,106 --> 00:02:56,777 line:-2
while also reducing the number
of objects allocations even further.


57
00:02:57,477 --> 00:02:58,879 line:-1
How about we see an example?


58
00:03:00,013 --> 00:03:03,217 line:-2
In this snippet of code, we're creating
managed objects for our quake...


59
00:03:04,551 --> 00:03:08,422 line:-2
and populating them with values
for our quake data provided by the USGS.


60
00:03:09,590 --> 00:03:11,225 line:-1
Then we save.


61
00:03:11,291 --> 00:03:14,394 line:-2
Let's see how this code would look
if we adopted NSBatchInsertRequest.


62
00:03:15,329 --> 00:03:17,764 line:-2
First, we gather a dictionary
of all the quake data


63
00:03:17,831 --> 00:03:19,132 line:-1
and add it to our array.


64
00:03:20,234 --> 00:03:23,036 line:-2
We create a batch insert request
and execute.


65
00:03:24,071 --> 00:03:27,774 line:-2
Simple. Now let's see this block variant
of batch insert request.


66
00:03:27,841 --> 00:03:30,210 line:-2
We create the batch insert request
with a block


67
00:03:30,277 --> 00:03:32,546 line:-2
that will assign values
to a given dictionary.


68
00:03:33,247 --> 00:03:34,815 line:-1
We then execute the request,


69
00:03:34,882 --> 00:03:37,150 line:0
and this block is called
until it returns "true"


70
00:03:37,217 --> 00:03:40,087 line:0
as the indicator to stop and save.


71
00:03:40,153 --> 00:03:42,322 line:-2
But how do these three different ways
perform?


72
00:03:42,389 --> 00:03:44,258 line:-1
Let's look at how our application performs


73
00:03:44,324 --> 00:03:46,493 line:-2
when we ingest
a large number of earthquakes.


74
00:03:46,560 --> 00:03:48,829 line:-2
When we save the context
with managed objects,


75
00:03:48,896 --> 00:03:50,397 line:-1
we see it takes over one minute


76
00:03:50,464 --> 00:03:53,267 line:-2
and idles out
with roughly 30 megs of memory usage.


77
00:03:54,001 --> 00:03:56,336 line:-1
The initial peak is the JSON data,


78
00:03:56,403 --> 00:03:59,373 line:-2
and most of that first minute
isn't actually the ingestion,


79
00:03:59,439 --> 00:04:01,708 line:-1
but the merging of change notifications


80
00:04:01,775 --> 00:04:04,444 line:-2
since we did a huge number
of transactions.


81
00:04:04,511 --> 00:04:07,347 line:-2
But we don't have that issue
with batch inserts.


82
00:04:07,414 --> 00:04:10,951 line:-2
When we use the NSBatchInsert
with an array of dictionaries,


83
00:04:11,018 --> 00:04:14,655 line:-2
we finish the same operation
with 25 megs of idle memory


84
00:04:14,721 --> 00:04:18,625 line:-2
and are able to save the same number
of objects in only 13 seconds,


85
00:04:18,692 --> 00:04:21,995 line:-2
a fraction of the time needed
for a traditional save.


86
00:04:22,496 --> 00:04:24,031 line:-1
But we can do even better.


87
00:04:24,097 --> 00:04:25,799 line:-1
Let's use the new block ingestion,


88
00:04:25,866 --> 00:04:29,002 line:-2
and we're able to ingest
all of the objects in 11 seconds.


89
00:04:29,069 --> 00:04:31,438 line:-2
Now that we've optimized
our data population,


90
00:04:31,505 --> 00:04:33,640 line:-2
let's learn a neat trick
about batch inserts.


91
00:04:33,707 --> 00:04:37,444 line:-2
Here, we have our managed-object model
from the Earthquakes Sample,


92
00:04:37,511 --> 00:04:39,713 line:-1
and I have the quake entity selected.


93
00:04:39,780 --> 00:04:42,049 line:-2
On the right-hand side
is the data model inspector


94
00:04:42,115 --> 00:04:43,383 line:-1
for our quake entity.


95
00:04:43,450 --> 00:04:45,452 line:-1
Let's take a closer look.


96
00:04:45,519 --> 00:04:48,722 line:-2
We can see that the attribute "code"
is a unique constraint.


97
00:04:48,789 --> 00:04:52,159 line:-2
This means that only one object
can have a specific value for "code"


98
00:04:52,226 --> 00:04:53,727 line:-1
in the persistent store.


99
00:04:53,794 --> 00:04:56,163 line:-2
No other quakes
can have the same value for "code."


100
00:04:56,630 --> 00:04:58,932 line:-2
How does this come into play
with our Earthquakes Sample?


101
00:04:59,533 --> 00:05:03,737 line:0
Well, our JSON feed gives us
all of the quakes for the last 30 days,


102
00:05:03,804 --> 00:05:06,073 line:0
and every time
our user hits the reload button,


103
00:05:06,139 --> 00:05:08,809 line:0
we ingest all of the data
from the JSON feed,


104
00:05:08,876 --> 00:05:10,077 line:0
which is mostly the same data


105
00:05:10,143 --> 00:05:12,679 line:0
except for a few quakes
that have happened recently,


106
00:05:12,746 --> 00:05:15,315 line:0
and updated data on past quakes.


107
00:05:15,382 --> 00:05:20,587 line:0
Here we have a quake with code 42
coming from our JSON feed into our parser.


108
00:05:21,255 --> 00:05:23,590 line:0
Then it is passed
to our background context,


109
00:05:23,657 --> 00:05:25,859 line:0
which saves quake 42 into the store.


110
00:05:26,426 --> 00:05:29,763 line:0
The first time it is ingested,
we create a new row in our store.


111
00:05:29,830 --> 00:05:32,699 line:0
However, on subsequent attempts
to insert the same quake,


112
00:05:32,766 --> 00:05:35,602 line:0
we don't want to delete the old one
and insert the new one.


113
00:05:35,669 --> 00:05:38,739 line:-2
We would really like to update
any data that has changed,


114
00:05:38,805 --> 00:05:40,674 line:-1
and in SQL that's called an UPSERT.


115
00:05:41,642 --> 00:05:44,478 line:-2
UPSERT is an SQL term
that is easier to understand


116
00:05:44,545 --> 00:05:46,747 line:-1
if you see the SQL at the same time.


117
00:05:46,813 --> 00:05:50,450 line:-2
So, here we have our insert
of our quake object,


118
00:05:50,517 --> 00:05:53,787 line:-2
and while inserting,
if we have a conflict on code,


119
00:05:53,854 --> 00:05:56,456 line:-2
update these properties
instead of inserting.


120
00:05:56,523 --> 00:05:58,292 line:-1
How do you get this behavior?


121
00:05:58,358 --> 00:06:01,128 line:-1
Simply set the mergePolicy on the context,


122
00:06:01,195 --> 00:06:03,530 line:-1
executing the batch insert request


123
00:06:03,597 --> 00:06:07,568 line:-2
to NSMergeByPropertyObject-
TrumpMergePolicy.


124
00:06:08,335 --> 00:06:10,270 line:-1
But there is a simpler way to do updates,


125
00:06:10,337 --> 00:06:12,840 line:-2
and it couldn't be simpler
with batch updates.


126
00:06:13,307 --> 00:06:15,242 line:-1
With the NSBatchUpdateRequest,


127
00:06:15,309 --> 00:06:17,177 line:-2
there is no need
to execute the fetch request


128
00:06:17,244 --> 00:06:19,813 line:-2
only to update the managed objects
and save.


129
00:06:19,880 --> 00:06:22,983 line:-2
An NSBatchUpdateRequest
can quickly and efficiently


130
00:06:23,050 --> 00:06:26,253 line:-2
update the properties in our objects
that meet the search criteria


131
00:06:26,320 --> 00:06:27,888 line:-1
defined in the fetch request.


132
00:06:28,422 --> 00:06:29,990 line:-1
Let's check out a quick example.


133
00:06:30,624 --> 00:06:32,526 line:-1
Using our Earthquakes Sample app,


134
00:06:32,593 --> 00:06:34,895 line:-2
we could mark all of the quakes
as validated


135
00:06:34,962 --> 00:06:37,731 line:-2
if we were able to confirm them
with another source.


136
00:06:37,798 --> 00:06:40,601 line:-2
Well, let's imagine our source
only validates earthquakes


137
00:06:40,667 --> 00:06:42,936 line:-1
if they have a magnitude greater than 2.5.


138
00:06:44,104 --> 00:06:46,607 line:-2
Well, we can build
a batch update for this.


139
00:06:46,673 --> 00:06:50,310 line:-2
This code creates a batch update request
for "Quake"


140
00:06:50,377 --> 00:06:54,214 line:-2
and then sets the propertiesToUpdate
"validated" equals "true"


141
00:06:54,281 --> 00:06:57,551 line:-2
and sets our predicate,
magnitude greater than 2.5.


142
00:06:58,719 --> 00:07:01,488 line:-2
Then we execute our batch update,
and we're all done.


143
00:07:01,555 --> 00:07:02,956 line:-1
It's just that simple.


144
00:07:03,023 --> 00:07:06,126 line:-2
So we've covered inserts and updates.
Let's do batch deletes.


145
00:07:06,960 --> 00:07:08,595 line:-1
Batch deletes are very powerful


146
00:07:08,662 --> 00:07:12,299 line:-2
and can be used to easily delete
large portions of the object graph.


147
00:07:12,366 --> 00:07:13,967 line:-1
Relationship rules are observed,


148
00:07:14,034 --> 00:07:17,004 line:-2
so deletes will cascade
and relationships will be nullified.


149
00:07:17,070 --> 00:07:19,773 line:-2
The general use case we have seen
is from expiration code


150
00:07:19,840 --> 00:07:22,242 line:-2
that determines the time to live
for objects


151
00:07:22,309 --> 00:07:24,912 line:-2
and cleans up those objects
that have reached their expiration.


152
00:07:25,445 --> 00:07:27,414 line:-1
This is a great use of this API.


153
00:07:27,481 --> 00:07:31,818 line:-2
However, what seems like a simple
operation can have complex ramifications.


154
00:07:31,885 --> 00:07:33,153 line:-1
How about an example?


155
00:07:33,220 --> 00:07:36,023 line:-2
Here, we have an example
where our earthquake data


156
00:07:36,089 --> 00:07:37,791 line:-1
expires after 30 days.


157
00:07:38,358 --> 00:07:40,394 line:-1
And since this is a clean-up task,


158
00:07:40,460 --> 00:07:43,897 line:-2
we have it dispatched asynchronously
with a background priority.


159
00:07:43,964 --> 00:07:46,700 line:-2
Our block kicks off
on the managed object context


160
00:07:46,767 --> 00:07:48,936 line:-1
and determines the expiration date.


161
00:07:49,002 --> 00:07:52,606 line:-2
Then we build our fetch request
and set our expiration predicate...


162
00:07:53,874 --> 00:07:57,511 line:-2
create a batch delete request
with our new fetch request, and execute.


163
00:07:58,579 --> 00:08:01,114 line:-2
Hmm. But what if we had
a large number of objects


164
00:08:01,181 --> 00:08:03,450 line:-2
that matched
our batch delete search criteria?


165
00:08:04,117 --> 00:08:07,020 line:-2
The request would take
a write lock on our store for,


166
00:08:07,087 --> 00:08:10,057 line:-2
uh-oh, possibly some
unbounded amount of time.


167
00:08:11,024 --> 00:08:12,793 line:-1
But wait. We can fix this.


168
00:08:13,460 --> 00:08:15,462 line:-1
Let's set a fetch limit.


169
00:08:15,529 --> 00:08:17,731 line:-1
This way our task doesn't go unbounded,


170
00:08:17,798 --> 00:08:20,400 line:-2
and we've saved our user
from a lot of frustration.


171
00:08:21,168 --> 00:08:22,903 line:-1
Now that we've avoided this pitfall,


172
00:08:22,970 --> 00:08:25,639 line:-2
let's see how we can improve
how to fetch data.


173
00:08:25,706 --> 00:08:27,841 line:-1
Now that we have this rich object graph,


174
00:08:27,908 --> 00:08:31,211 line:-2
we need to investigate and display
what we have in our store.


175
00:08:31,278 --> 00:08:35,349 line:-2
When fetching data, the data we get back
can drive a lot of views and computations.


176
00:08:35,414 --> 00:08:37,751 line:-1
But do we always need that much data?


177
00:08:37,818 --> 00:08:39,820 line:-2
And how can we do
these complex computations


178
00:08:39,886 --> 00:08:41,421 line:-1
without the entire object graph?


179
00:08:41,955 --> 00:08:46,527 line:-2
Well, first, the managedObjectResultType
provides the easiest way


180
00:08:46,593 --> 00:08:49,196 line:-2
to traverse the object graph
to the fullest extent.


181
00:08:49,897 --> 00:08:54,168 line:-2
This is great when we're using the results
to batch a fetchResultsController.


182
00:08:54,234 --> 00:08:56,069 line:-1
As our managed objects are updated,


183
00:08:56,136 --> 00:08:59,907 line:-2
our fetchResultsController
magically responds and applies the diffs.


184
00:08:59,973 --> 00:09:01,542 line:-1
Let's see this in action.


185
00:09:01,608 --> 00:09:05,212 line:-2
Here's our Earthquakes Sample
without any data, and then we fetch.


186
00:09:05,679 --> 00:09:09,416 line:-2
As the objects are fetched,
our fetchResultsController is adding rows.


187
00:09:10,050 --> 00:09:13,820 line:-2
And as we update these objects,
our view is also updated.


188
00:09:13,887 --> 00:09:17,925 line:-2
But if you notice here,
our view only shows about 15 earthquakes.


189
00:09:17,991 --> 00:09:19,760 line:-1
We fetched a lot more than that.


190
00:09:19,826 --> 00:09:21,562 line:-1
We have some room for improvement here.


191
00:09:21,628 --> 00:09:24,364 line:0
By setting the batch size
on our fetch request,


192
00:09:24,431 --> 00:09:27,467 line:0
the results will only have the first batch
of a given number of objects


193
00:09:27,534 --> 00:09:29,236 line:0
fully hydrated with data.


194
00:09:29,303 --> 00:09:32,706 line:0
And when our user scrolls,
the remaining objects will be hydrated,


195
00:09:32,773 --> 00:09:34,641 line:0
but only as far as the user needs.


196
00:09:35,309 --> 00:09:36,844 line:-1
The batched array is special


197
00:09:36,910 --> 00:09:39,213 line:-2
and behaves differently
than a traditional array.


198
00:09:39,279 --> 00:09:40,280 line:-1
Let me show you.


199
00:09:42,049 --> 00:09:44,084 line:0
Here, we have a regular result array.


200
00:09:44,151 --> 00:09:47,521 line:0
All our managed objects are hydrated,
and as we iterate over them,


201
00:09:47,588 --> 00:09:48,989 line:0
all is as expected.


202
00:09:49,623 --> 00:09:51,291 line:0
Now we have a batched array.


203
00:09:51,358 --> 00:09:53,360 line:0
Notice that we do not have managed objects


204
00:09:53,427 --> 00:09:55,863 line:0
but ObjectIDs
that will turn into managed objects


205
00:09:55,929 --> 00:09:58,298 line:0
upon iterating over the array.


206
00:09:58,365 --> 00:10:01,235 line:0
And as I move on, the batches are released


207
00:10:01,301 --> 00:10:04,104 line:0
and only the ObjectID is retained
for the results.


208
00:10:04,972 --> 00:10:06,373 line:-1
Let's see this in action.


209
00:10:06,440 --> 00:10:09,843 line:-2
Here's our Earthquakes Sample
fetching data,


210
00:10:09,910 --> 00:10:13,280 line:-2
and it shows the idle memory
at roughly 17 megs.


211
00:10:14,181 --> 00:10:16,216 line:-1
Well, what if we turn on batch fetch?


212
00:10:18,151 --> 00:10:19,653 line:-1
When we set the batch size,


213
00:10:19,720 --> 00:10:23,357 line:-2
we only use roughly 12 megs of memory
to do the same task,


214
00:10:24,157 --> 00:10:25,759 line:-1
and we've saved almost five megs,


215
00:10:25,826 --> 00:10:28,428 line:-2
nearly a third
of our application's memory usage.


216
00:10:29,329 --> 00:10:31,365 line:-1
How else can we improve our fetches?


217
00:10:32,566 --> 00:10:35,702 line:-2
We can trim down what data
the fetch fetches.


218
00:10:35,769 --> 00:10:38,939 line:-2
If we know the result will need
certain attributes or relationships,


219
00:10:39,006 --> 00:10:41,275 line:-2
we can tailor our fetch
to these requirements.


220
00:10:41,775 --> 00:10:44,244 line:-2
For the known attributes
that will be accessed,


221
00:10:44,311 --> 00:10:46,180 line:-1
we can set propertiesToFetch.


222
00:10:46,246 --> 00:10:47,981 line:-1
When we work with managed objects,


223
00:10:48,048 --> 00:10:51,351 line:-2
the default behavior is to have
the relationship set as "false,"


224
00:10:51,418 --> 00:10:53,420 line:-1
and the first traversal of a relationship


225
00:10:53,487 --> 00:10:55,956 line:-2
will trigger the fetch
of the related object.


226
00:10:56,023 --> 00:10:59,626 line:-2
This is great if few relationships
are traversed, or none at all.


227
00:10:59,693 --> 00:11:03,764 line:-2
But if it is known that a relationship
is highly likely to be traversed,


228
00:11:03,830 --> 00:11:06,967 line:-2
we recommend setting that key path
to be prefetched,


229
00:11:07,601 --> 00:11:10,771 line:-2
thus avoiding having to fetch the data
at a later time


230
00:11:10,838 --> 00:11:14,274 line:-2
and inefficiently, as the faults
are loaded on each traversal.


231
00:11:14,808 --> 00:11:18,645 line:-2
Here is our baseline fetch,
which sits idle at 17.6 megs.


232
00:11:19,313 --> 00:11:21,014 line:-1
But if we set the propertiesToFetch


233
00:11:21,081 --> 00:11:23,917 line:-2
to only those attributes
visible in our UI,


234
00:11:23,984 --> 00:11:27,087 line:-2
we can reduce our idle memory
to 16.4 megs.


235
00:11:28,121 --> 00:11:30,057 line:-1
Now let's talk about ObjectIDs.


236
00:11:31,191 --> 00:11:33,827 line:-2
Managed objects are large
and rich with data,


237
00:11:33,894 --> 00:11:36,463 line:-2
but these are not ideal
to pass between threads,


238
00:11:36,530 --> 00:11:39,132 line:-1
so ObjectIDResultType comes in handy.


239
00:11:39,199 --> 00:11:41,268 line:-2
When we want to do the work
and identify the objects


240
00:11:41,335 --> 00:11:42,803 line:-1
that meet a certain criteria,


241
00:11:42,870 --> 00:11:46,974 line:-2
these simple identifiers can be passed
to other threads for further processing,


242
00:11:47,040 --> 00:11:49,743 line:-2
avoiding that lookup cost
on the processing thread.


243
00:11:49,810 --> 00:11:54,081 line:-2
But what if we need something in between
a full managed object and ObjectIDs?


244
00:11:54,147 --> 00:11:56,383 line:0
Like dictionary results,


245
00:11:56,950 --> 00:12:00,787 line:0
which are very handy as they provide
a lightweight, read-only data set


246
00:12:00,854 --> 00:12:02,589 line:0
that can be passed to other threads.


247
00:12:03,056 --> 00:12:06,460 line:-2
Dictionary results can also be tailored
to do complex data aggregation


248
00:12:06,527 --> 00:12:08,428 line:-1
that can help reduce large computations


249
00:12:08,495 --> 00:12:11,431 line:-2
that would normally require
pulling in the relevant object graph...


250
00:12:12,699 --> 00:12:16,470 line:-2
such as groupBy with aggregate functions
on the entities and their properties.


251
00:12:16,537 --> 00:12:17,871 line:-1
Let's look at an example.


252
00:12:18,405 --> 00:12:20,507 line:-1
Average magnitude grouped by place.


253
00:12:20,574 --> 00:12:24,011 line:-2
First, we determine
our key path expression for magnitude,


254
00:12:24,077 --> 00:12:26,446 line:-2
then our function expression
to get the average.


255
00:12:27,347 --> 00:12:30,884 line:-2
Then we make an expression description
of our average magnitude.


256
00:12:32,186 --> 00:12:36,323 line:-2
And lastly, we set up our fetch request
for quakes with properties to fetch,


257
00:12:36,390 --> 00:12:40,627 line:-2
our expression description and the place,
grouped by the place,


258
00:12:40,694 --> 00:12:42,563 line:-1
and set the result to "dictionary."


259
00:12:42,629 --> 00:12:44,898 line:-1
And this leads to these results,


260
00:12:44,965 --> 00:12:48,869 line:-2
which show us the average magnitude
for earthquakes in the designated places.


261
00:12:48,936 --> 00:12:52,673 line:-2
Our last result type to cover
is the countResultType.


262
00:12:52,739 --> 00:12:56,210 line:-1
It's simple, elegant and optimized.


263
00:12:56,777 --> 00:12:58,011 line:-1
Need I say more?


264
00:12:58,946 --> 00:13:01,281 line:-1
We've optimized ingestion and fetching.


265
00:13:01,348 --> 00:13:04,318 line:-2
Now let's look at how we can improve
our application's reactions


266
00:13:04,384 --> 00:13:06,353 line:-1
to changes in our persistent store.


267
00:13:06,420 --> 00:13:08,589 line:-1
Core Data is rich with notifications


268
00:13:08,655 --> 00:13:11,258 line:-2
that let you know when stores
have been added or removed,


269
00:13:11,325 --> 00:13:12,960 line:-1
or objects have been saved or changed.


270
00:13:13,026 --> 00:13:16,430 line:-2
But we want to focus on two in particular
that are really useful.


271
00:13:16,496 --> 00:13:19,466 line:-2
This year,
we've introduced ObjectID notifications


272
00:13:19,533 --> 00:13:22,936 line:-2
that are available in addition
to the traditional save notification.


273
00:13:23,003 --> 00:13:25,739 line:-2
The ObjectID notification
is a lightweight counterpart


274
00:13:25,806 --> 00:13:29,142 line:-2
to the managed object save notification
you're already familiar with,


275
00:13:29,209 --> 00:13:33,113 line:-2
and this convenient notification is vended
from persistent history transactions.


276
00:13:33,514 --> 00:13:35,616 line:-2
Let's look at these new additions
in Swift.


277
00:13:35,682 --> 00:13:39,920 line:-2
The managed object context now has
modernized notifications for Swift.


278
00:13:39,987 --> 00:13:43,156 line:-2
We've updated some oldies but goodies
to be friendlier in Swift


279
00:13:43,223 --> 00:13:45,492 line:-1
and added these two new notifications


280
00:13:45,559 --> 00:13:48,262 line:-2
that allow you to drive your application
with ObjectIDs


281
00:13:48,328 --> 00:13:49,663 line:-1
rather than managed objects.


282
00:13:49,730 --> 00:13:52,299 line:-1
But that's not all that we've modernized.


283
00:13:52,366 --> 00:13:56,637 line:-2
As part of our efforts to modernize,
we've also added notification keys


284
00:13:56,703 --> 00:13:59,673 line:-2
that make notification processing
much easier in Swift.


285
00:14:00,140 --> 00:14:03,844 line:0
We've also added some new keys
to go with the ObjectID notifications.


286
00:14:03,911 --> 00:14:06,013 line:-2
And the other notification
we want to discuss


287
00:14:06,079 --> 00:14:07,915 line:-1
is the remote change notification.


288
00:14:09,149 --> 00:14:11,985 line:-2
Remote change notifications
are very informative


289
00:14:12,052 --> 00:14:17,357 line:-2
because they're posted for all operations
that are done in and out of your process


290
00:14:17,424 --> 00:14:19,059 line:-1
by any Core Data client.


291
00:14:19,593 --> 00:14:22,529 line:-2
This allows your application
to avoid polling for changes


292
00:14:22,596 --> 00:14:25,699 line:-2
and be able to drive that same logic
with notifications.


293
00:14:26,333 --> 00:14:28,235 line:-1
And when persistent history is enabled,


294
00:14:28,302 --> 00:14:30,971 line:-2
the userInfo
of a remote change notification


295
00:14:31,038 --> 00:14:33,473 line:-1
contains a persistent history token


296
00:14:33,540 --> 00:14:36,343 line:-2
which can be used to get
an ObjectID notification.


297
00:14:36,410 --> 00:14:38,145 line:-1
Let's see this in the works.


298
00:14:39,313 --> 00:14:42,216 line:-1
Here we have our container and application


299
00:14:42,282 --> 00:14:45,419 line:-2
and a table showing our persistent history
that has been captured so far.


300
00:14:46,220 --> 00:14:49,223 line:-1
The JSON feed from the USGS comes online,


301
00:14:49,289 --> 00:14:53,227 line:-2
and the background context ingests
the JSON data into the persistent store.


302
00:14:54,328 --> 00:14:57,531 line:-2
And persistent history captures
the operation with great detail,


303
00:14:57,598 --> 00:14:59,399 line:-1
much more than we can show you here.


304
00:15:00,267 --> 00:15:02,336 line:-2
Previously, though,
our application would need


305
00:15:02,402 --> 00:15:04,571 line:-1
to poll the store for new changes.


306
00:15:04,638 --> 00:15:06,907 line:-1
But not with remote change notifications.


307
00:15:06,974 --> 00:15:10,010 line:-2
Instead, we're notified
that changes have been made,


308
00:15:10,077 --> 00:15:13,480 line:-2
and the userInfo payload
for the remote change notification


309
00:15:13,547 --> 00:15:17,551 line:-2
has a history token so I can see
the exact operation in persistent history.


310
00:15:17,618 --> 00:15:20,153 line:-2
Let's see how this works
as our application evolves.


311
00:15:20,621 --> 00:15:22,656 line:0
Our application has some new additions:


312
00:15:22,723 --> 00:15:26,326 line:0
a share extension, a second application
that harnesses the same data,


313
00:15:26,393 --> 00:15:28,395 line:0
and a handy Photos extension.


314
00:15:28,462 --> 00:15:32,666 line:-2
When one of these new additions
makes a change to the persistent store,


315
00:15:32,733 --> 00:15:35,135 line:-2
the operation is recorded
in persistent history.


316
00:15:35,802 --> 00:15:38,772 line:0
However, when an application
comes to the foreground,


317
00:15:38,839 --> 00:15:41,808 line:0
it needs to poll for history,
which is really expensive.


318
00:15:43,010 --> 00:15:45,746 line:0
But if we have
remote change notifications enabled,


319
00:15:45,812 --> 00:15:48,715 line:0
we get notified when our app
is brought to the foreground...


320
00:15:50,651 --> 00:15:53,720 line:0
and on subsequent changes
by our Photos extension...


321
00:15:55,255 --> 00:15:57,191 line:0
and our second application...


322
00:15:58,192 --> 00:16:00,093 line:0
and our share extension.


323
00:16:01,495 --> 00:16:03,030 line:0
When our app relaunches,


324
00:16:03,096 --> 00:16:07,034 line:0
it is notified and can easily see
what has changed with persistent history.


325
00:16:07,601 --> 00:16:12,339 line:-2
These two features make it really easy
to know who, what, when, where,


326
00:16:12,406 --> 00:16:15,175 line:-1
and how the persistent store has changed.


327
00:16:15,242 --> 00:16:18,111 line:-1
Lastly, a quick tip on persistent history.


328
00:16:19,246 --> 00:16:21,882 line:0
A rather handy tip for persistent history


329
00:16:21,949 --> 00:16:24,518 line:0
is the same we told you earlier
for fetch requests.


330
00:16:24,585 --> 00:16:27,721 line:0
Make sure to tailor the request
to your application's needs.


331
00:16:28,222 --> 00:16:31,525 line:0
Here, we have an example
of how to tailor the change request


332
00:16:31,592 --> 00:16:36,630 line:0
so that we can find all the changes for
a specific ObjectID after a given date.


333
00:16:37,331 --> 00:16:40,167 line:0
First, we start off
by getting our entity description


334
00:16:40,234 --> 00:16:42,369 line:0
for the persistent history
change object...


335
00:16:43,871 --> 00:16:46,039 line:0
so that we can use it
to build our fetch request


336
00:16:46,106 --> 00:16:47,274 line:0
and set the entity.


337
00:16:48,175 --> 00:16:50,110 line:0
Then we set our predicate here,


338
00:16:50,177 --> 00:16:52,713 line:0
looking for changes
to a specific ObjectID,


339
00:16:53,547 --> 00:16:57,084 line:0
and then we create our history request,
and set the fetch request.


340
00:16:58,018 --> 00:16:59,686 line:0
And voilà, execute.


341
00:16:59,753 --> 00:17:02,956 line:0
Our results will only be those changes
for the given ObjectID


342
00:17:03,023 --> 00:17:04,657 line:0
after the specific date.


343
00:17:04,724 --> 00:17:06,527 line:-1
That's all we have for this session.


344
00:17:06,593 --> 00:17:07,761 line:-1
A quick recap:


345
00:17:07,828 --> 00:17:11,832 line:-2
batch when possible,
tailor your fetches to the intended use


346
00:17:11,898 --> 00:17:15,435 line:-2
and harness the power of notifications
and persistent history.


347
00:17:15,502 --> 00:17:17,538 line:-2
Thanks to all of you
and to the Core Data team.


348
00:17:17,604 --> 00:17:18,839 line:-1
It's been an honor.

