1
00:00:03,703 --> 00:00:06,939 line:-1
Hello and welcome to WWDC.


2
00:00:09,309 --> 00:00:11,912 line:0
Hello, everyone. My name is Brett Keating.


3
00:00:12,412 --> 00:00:15,315 line:0
I hope you are all enjoying WWDC 2020.


4
00:00:15,916 --> 00:00:17,251 line:-1
Let's continue your journey


5
00:00:17,317 --> 00:00:19,686 line:-2
and talk about some new APIs
in Vision this year.


6
00:00:20,120 --> 00:00:23,757 line:-2
Let's hear all about how to obtain Body
and Hand Pose using the Vision framework.


7
00:00:24,725 --> 00:00:27,027 line:-2
One of the major themes
of the Vision framework


8
00:00:27,461 --> 00:00:29,630 line:-2
is how it can be used
to help you understand people


9
00:00:29,696 --> 00:00:30,931 line:-1
in your visual data.


10
00:00:31,865 --> 00:00:34,168 line:-2
Back when Vision framework
was first introduced,


11
00:00:34,234 --> 00:00:38,405 line:-2
it came with a new Face Detector that
was based on deep learning technology.


12
00:00:39,006 --> 00:00:41,108 line:-2
And since that time,
it had been improved upon


13
00:00:41,175 --> 00:00:44,278 line:-2
with a second revision
that could detect upside-down faces.


14
00:00:45,445 --> 00:00:49,416 line:-2
Also arriving with the debut of Vision
was Face Landmark detection,


15
00:00:49,850 --> 00:00:52,352 line:-2
which has also seen improvements
with new revisions.


16
00:00:52,853 --> 00:00:56,023 line:-2
Last year, we started giving you a new,
richer set of landmarks


17
00:00:56,089 --> 00:00:57,991 line:-1
that infer pupil locations.


18
00:00:58,825 --> 00:01:01,728 line:-2
Also new last year
is human torso detection.


19
00:01:03,697 --> 00:01:06,733 line:-2
And now, I'm excited to show you
what's new in the People theme


20
00:01:06,800 --> 00:01:08,368 line:-1
for Vision framework this year.


21
00:01:08,969 --> 00:01:10,671 line:-1
We will be providing hand pose,


22
00:01:11,572 --> 00:01:15,242 line:-2
and we will be providing human body pose
from Vision framework.


23
00:01:16,510 --> 00:01:18,445 line:-1
Let's begin by talking about hand pose.


24
00:01:19,479 --> 00:01:22,449 line:-2
Hand pose has so many exciting
possible applications.


25
00:01:22,716 --> 00:01:23,817 line:-1
I hope you will agree


26
00:01:23,884 --> 00:01:26,320 line:-2
and find amazing ways
to take advantage of it.


27
00:01:27,287 --> 00:01:29,256 line:-2
Look at how well it's working
on this video


28
00:01:29,323 --> 00:01:30,891 line:-1
of a child playing the ukulele.


29
00:01:32,226 --> 00:01:35,495 line:-2
Let's go through a few examples
of what can be done with hand pose.


30
00:01:35,963 --> 00:01:39,600 line:-2
Perhaps your mission as a developer
is to rid the world of selfie sticks.


31
00:01:40,133 --> 00:01:43,170 line:-2
The Vision framework can now help
with hand pose estimation.


32
00:01:43,570 --> 00:01:46,373 line:0
If you develop your app
to look for specific gestures


33
00:01:46,440 --> 00:01:48,275 line:0
to trigger a photo capture,


34
00:01:48,342 --> 00:01:52,479 line:0
you can create an experience like the one
my colleague Xiaoxia Sun has done here.


35
00:01:53,480 --> 00:01:56,517 line:-2
Maybe you'd like to be able
to overlay fun graphics on hands,


36
00:01:56,583 --> 00:01:58,752 line:-2
like many developers
have already done with faces.


37
00:01:59,286 --> 00:02:02,556 line:-2
In your app, you can look at
the hand poses Vision gives you,


38
00:02:02,623 --> 00:02:05,792 line:-2
and if you write code to classify them,
you can overlay emojis


39
00:02:05,859 --> 00:02:07,961 line:-2
or any other kind
of graphic that you choose,


40
00:02:08,027 --> 00:02:09,763 line:-1
based on the gestures you find.


41
00:02:10,797 --> 00:02:12,799 line:-1
So how do you use Vision for this?


42
00:02:12,866 --> 00:02:14,535 line:-1
As we've promised since the start,


43
00:02:14,601 --> 00:02:18,372 line:-2
all of our algorithm requests generally
follow the same pattern as the others.


44
00:02:18,438 --> 00:02:21,074 line:-2
The first step is to
create a request handler.


45
00:02:21,375 --> 00:02:24,011 line:-2
Here, we are using
the image request handler.


46
00:02:24,611 --> 00:02:26,713 line:-1
The next step is to create the request.


47
00:02:27,014 --> 00:02:31,051 line:-2
In this case, use
a VNDetectHumanHandPoseRequest.


48
00:02:32,586 --> 00:02:35,422 line:-2
The next step is to provide the request
to the handler


49
00:02:35,489 --> 00:02:37,457 line:-1
via a call to performRequests.


50
00:02:38,292 --> 00:02:39,626 line:-1
Once that completes successfully,


51
00:02:39,693 --> 00:02:43,197 line:-2
you will have your observations
in the requests' results property.


52
00:02:43,263 --> 00:02:44,331 line:-1
In this case,


53
00:02:44,398 --> 00:02:47,501 line:-2
VNRecognizedPointsObservations
are returned.


54
00:02:49,703 --> 00:02:53,740 line:-2
The observations contain locations
for all the found landmarks for the hand.


55
00:02:54,308 --> 00:02:57,444 line:-2
These are given in new classes meant
to represent 2D points.


56
00:02:58,111 --> 00:03:00,814 line:-2
These classes form
an inheritance hierarchy.


57
00:03:02,115 --> 00:03:04,751 line:-2
If the algorithm you're using
only returns a location,


58
00:03:04,818 --> 00:03:06,453 line:-1
you will be given a VNPoint.


59
00:03:07,487 --> 00:03:09,923 line:-1
A VNPoint contains a CGPoint location,


60
00:03:09,990 --> 00:03:13,660 line:-2
and the x and y coordinates
of the locations can be accessed directly


61
00:03:13,727 --> 00:03:15,362 line:-1
if desired as well.


62
00:03:16,730 --> 00:03:20,801 line:-2
The coordinates use the same lower left
origin as other Vision algorithms


63
00:03:20,868 --> 00:03:22,970 line:-2
and are also returned
in normalized coordinates


64
00:03:23,036 --> 00:03:25,305 line:-2
relative to the pixel dimensions
of your image.


65
00:03:26,807 --> 00:03:30,344 line:-2
If the algorithm you are using
also has a confidence associated with it,


66
00:03:30,410 --> 00:03:33,247 line:-1
you will get VNDetectedPoint objects.


67
00:03:34,414 --> 00:03:37,618 line:-2
Finally, if the algorithm
is also labeling the points,


68
00:03:37,684 --> 00:03:40,120 line:-1
you will get VNRecognizedPoint objects.


69
00:03:40,787 --> 00:03:44,424 line:0
For Hand Pose,
VNRecognizedPoint objects are returned.


70
00:03:45,759 --> 00:03:48,762 line:-2
Here's how you access these points
from a Hand Pose observation.


71
00:03:50,030 --> 00:03:52,432 line:-2
First, you will request a dictionary
of the landmarks


72
00:03:52,499 --> 00:03:55,936 line:-2
by calling recognizedPoints(forGroupKey
on the observation.


73
00:03:56,403 --> 00:03:59,139 line:-2
I will go into more detail
about group keys in a minute,


74
00:03:59,206 --> 00:04:00,974 line:-1
but know that if you want all the points,


75
00:04:01,041 --> 00:04:03,844 line:-2
you will use
VNRecognizedPointGroupKeyAll.


76
00:04:05,012 --> 00:04:07,614 line:-2
We provide other
VNRecognizedPointGroupKeys,


77
00:04:07,681 --> 00:04:11,318 line:-2
such as for only those points
that are part of the index finger.


78
00:04:12,219 --> 00:04:14,922 line:-2
You can access the landmarks
by iterating over them,


79
00:04:14,988 --> 00:04:16,523 line:-1
or as in this example,


80
00:04:16,589 --> 00:04:18,926 line:-2
access a particular one
for the tip of the index finger


81
00:04:18,992 --> 00:04:21,562 line:-1
by specifying its VNRecognizedPointKey.


82
00:04:23,730 --> 00:04:27,601 line:-2
Here I am showing you a quick overview
of the hand landmarks that are returned.


83
00:04:27,668 --> 00:04:32,172 line:-2
There are four for each finger and thumb,
and one for the wrist,


84
00:04:32,239 --> 00:04:34,308 line:-1
for a total of 21 hand landmarks.


85
00:04:35,876 --> 00:04:37,144 line:-1
As I just mentioned,


86
00:04:37,211 --> 00:04:40,547 line:-2
we have a new type this year
called "VNRecognizedPointGroupKey."


87
00:04:41,215 --> 00:04:44,418 line:-2
Each of the hand landmarks belong
to at least one of these groups.


88
00:04:44,885 --> 00:04:48,055 line:-2
Here is the definition for the group key
for the index finger,


89
00:04:48,121 --> 00:04:50,424 line:-2
and here are the landmarks
that are contained in that group


90
00:04:50,490 --> 00:04:52,159 line:-1
displayed visually on the hand.


91
00:04:53,694 --> 00:04:56,296 line:-1
Also, in the code example shown before,


92
00:04:56,363 --> 00:04:59,199 line:-2
I showed you one
of the VNRecognizedPointKey


93
00:04:59,266 --> 00:05:02,236 line:-2
that you can use
to retrieve the tip of the index finger.


94
00:05:02,870 --> 00:05:05,472 line:-2
Let's see what this looks like
for the remaining parts of the finger.


95
00:05:06,206 --> 00:05:07,741 line:-1
Going down towards the hand,


96
00:05:07,808 --> 00:05:10,777 line:-2
the first joint
is the distal interphalangeal joint,


97
00:05:10,844 --> 00:05:13,180 line:-1
which we abbreviate as DIP.


98
00:05:13,580 --> 00:05:15,582 line:-2
Notice that
the VNRecognizedPointKey type


99
00:05:15,649 --> 00:05:17,918 line:-2
uses this abbreviation
to distinguish itself.


100
00:05:18,785 --> 00:05:22,856 line:-2
Continuing along, the next joint
is the proximal interphalangeal joint,


101
00:05:22,923 --> 00:05:25,359 line:-1
which we abbreviate as PIP.


102
00:05:25,425 --> 00:05:28,829 line:-2
Finally, at the base of the fingers
is the metacarpophalangeal joint,


103
00:05:28,896 --> 00:05:31,365 line:-1
which for fingers we abbreviate as MCP.


104
00:05:31,965 --> 00:05:33,567 line:-1
All these four points I mentioned


105
00:05:33,634 --> 00:05:36,203 line:-2
are retrievable
by the index finger's group key.


106
00:05:37,771 --> 00:05:39,940 line:-2
The pattern repeats for each
of the fingers.


107
00:05:40,007 --> 00:05:43,243 line:-2
As an example, here's the group key
and associated landmark keys


108
00:05:43,310 --> 00:05:44,645 line:-1
for the ring finger.


109
00:05:46,046 --> 00:05:47,948 line:0
The thumb is a bit different.


110
00:05:48,015 --> 00:05:49,983 line:0
The thumb also has a tip.


111
00:05:50,050 --> 00:05:52,486 line:0
The first joint
is the interphalangeal joint,


112
00:05:52,553 --> 00:05:54,221 line:0
which we abbreviate as IP.


113
00:05:54,721 --> 00:05:57,424 line:0
The next joint
is the metacarpophalangeal joint,


114
00:05:57,491 --> 00:05:59,893 line:0
which for thumbs we abbreviate as MP.


115
00:06:01,094 --> 00:06:04,598 line:0
The next joint for the thumb
is the carpometacarpal joint,


116
00:06:04,665 --> 00:06:06,533 line:0
abbreviated as CMC.


117
00:06:08,702 --> 00:06:10,103 line:0
Below, for reference,


118
00:06:10,170 --> 00:06:13,607 line:0
the corresponding group keys
and landmark keys for the thumb are shown


119
00:06:13,674 --> 00:06:15,976 line:0
to contrast against what we provide
for fingers.


120
00:06:17,644 --> 00:06:21,181 line:-2
And then, there's the base of the wrist,
which also has its own landmark.


121
00:06:21,648 --> 00:06:23,917 line:-2
The wrist landmark falls
on the center of the wrist


122
00:06:23,984 --> 00:06:26,587 line:-2
and is not part of any group
except for the "all" group.


123
00:06:26,653 --> 00:06:29,389 line:-2
In other words, it's not part
of any finger or thumb group.


124
00:06:30,090 --> 00:06:32,626 line:-2
Combined with the landmarks
for the fingers and thumbs,


125
00:06:32,693 --> 00:06:35,495 line:-2
this forms the set of landmarks
we identify for hands.


126
00:06:36,830 --> 00:06:40,000 line:-2
Now, let me show you
our sample application for hand pose.


127
00:06:43,003 --> 00:06:46,607 line:-2
So with this sample,
I'm using my hand to draw on the screen.


128
00:06:46,974 --> 00:06:48,675 line:-2
When my finger and thumb
are close together,


129
00:06:48,742 --> 00:06:49,843 line:-1
I'll start drawing.


130
00:06:51,912 --> 00:06:55,215 line:-2
And here I'm using that
to write the word "hello."


131
00:06:58,552 --> 00:06:59,553 line:-1
And that's it.


132
00:07:00,220 --> 00:07:02,389 line:-2
So I'll see what that looks like
in the code.


133
00:07:04,558 --> 00:07:06,760 line:-2
Here I'm going to start
in the camera view controller...


134
00:07:09,029 --> 00:07:12,432 line:-2
in our capture output where
we are receiving CMSampleBuffers


135
00:07:12,499 --> 00:07:13,800 line:-1
from the camera stream.


136
00:07:14,334 --> 00:07:15,435 line:-1
The first thing we do


137
00:07:15,502 --> 00:07:19,339 line:-2
is we create a VNImageRequestHandler
using that sample buffer.


138
00:07:20,607 --> 00:07:23,577 line:-2
Then, we use that handler
to perform our request.


139
00:07:24,144 --> 00:07:25,312 line:-1
Our request...


140
00:07:26,580 --> 00:07:29,249 line:-1
is a VNDetect human hand pose request.


141
00:07:31,218 --> 00:07:34,922 line:-2
If we find a hand,
we will be getting an observation back,


142
00:07:34,988 --> 00:07:37,691 line:-2
and from that observation
we can get the thumb points


143
00:07:37,758 --> 00:07:41,995 line:-2
and the index finger points by using
the VNRecognizedPointGroupKey


144
00:07:42,062 --> 00:07:44,464 line:-2
by calling
recognizedPoints(forGroupKey.


145
00:07:45,265 --> 00:07:46,466 line:-1
With those collections,


146
00:07:46,533 --> 00:07:49,169 line:-2
we can look for the fingertip
and the thumb tip points.


147
00:07:49,236 --> 00:07:50,737 line:-1
And we do that here.


148
00:07:51,839 --> 00:07:53,974 line:-1
We ignore any low confidence points.


149
00:07:55,175 --> 00:07:56,810 line:-1
And then, at the end of this section,


150
00:07:56,877 --> 00:07:58,745 line:-2
we convert the points
from Vision coordinates


151
00:07:58,812 --> 00:08:00,214 line:-1
to AVFoundation coordinates.


152
00:08:01,348 --> 00:08:04,351 line:-2
One thing I want to draw your attention to
up here is that we have set up


153
00:08:04,418 --> 00:08:06,887 line:-1
a separate queue to process these points.


154
00:08:07,521 --> 00:08:09,122 line:-1
So let's go into processPoints.


155
00:08:10,357 --> 00:08:14,228 line:-2
Here we're just checking to see
if nothing's going on. If so, we reset.


156
00:08:14,294 --> 00:08:17,564 line:-2
Otherwise, we convert our thumb
and index points


157
00:08:17,631 --> 00:08:19,600 line:-1
into AVFoundation coordinates here.


158
00:08:20,934 --> 00:08:22,469 line:-1
Then, we process these points.


159
00:08:22,703 --> 00:08:25,606 line:-2
We have another class
called "gestureProcessor."


160
00:08:25,672 --> 00:08:28,141 line:-1
And in that we call "processPointsPair."


161
00:08:29,843 --> 00:08:33,380 line:-2
So here, in processPointsPair,
we're looking at the distance between


162
00:08:33,447 --> 00:08:36,149 line:-2
the tip of the index finger
and the tip of the thumb.


163
00:08:36,650 --> 00:08:39,318 line:-2
If the distance
is less than a threshold...


164
00:08:40,419 --> 00:08:42,089 line:-1
then we start accumulating evidence


165
00:08:42,155 --> 00:08:44,224 line:-1
for whether or not we are in a pinch state


166
00:08:44,291 --> 00:08:45,759 line:-1
or a possible pinch state.


167
00:08:47,127 --> 00:08:49,463 line:-1
The threshold that we have is 40.


168
00:08:49,730 --> 00:08:54,768 line:-2
And how much evidence we require
is three frames of the same pinch state.


169
00:08:56,069 --> 00:08:58,906 line:-2
So once we've collected three frames
of the pinch state,


170
00:08:58,972 --> 00:09:02,009 line:-2
we move from a possible pinch state
to a pinch state.


171
00:09:02,843 --> 00:09:04,878 line:-1
We do the same thing for the apart state.


172
00:09:05,179 --> 00:09:07,181 line:-2
If the fingers
are not meeting the threshold,


173
00:09:07,247 --> 00:09:08,649 line:-1
we consider them apart.


174
00:09:09,416 --> 00:09:13,287 line:-2
Until we've accumulated enough evidence,
we are in the possible apart state.


175
00:09:13,353 --> 00:09:16,924 line:-2
Once we have enough evidence,
we transition to the apart state.


176
00:09:18,025 --> 00:09:20,761 line:-2
Going back
to the CounterViewController file,


177
00:09:21,161 --> 00:09:25,199 line:-2
these state changes are handled
in handleGestureStateChange.


178
00:09:25,799 --> 00:09:27,868 line:-1
Here we're looking at which case we're in.


179
00:09:27,935 --> 00:09:31,038 line:-2
If we're in a possible pinch
or a possible apart state,


180
00:09:31,104 --> 00:09:33,941 line:-2
we want to keep track of the points
that we have found


181
00:09:34,007 --> 00:09:36,376 line:-2
in order to decide later
if we want to draw them or not,


182
00:09:36,443 --> 00:09:38,045 line:-1
and we collect those here.


183
00:09:38,745 --> 00:09:40,948 line:-2
If, ultimately,
we end up in the pinch state,


184
00:09:41,815 --> 00:09:44,585 line:-1
then we go ahead and we draw those points.


185
00:09:44,651 --> 00:09:47,688 line:-2
Once we've drawn them, we can
remove them from our evidence buffer,


186
00:09:48,355 --> 00:09:50,357 line:-1
and then go ahead and continue drawing.


187
00:09:52,359 --> 00:09:55,462 line:-2
If we end up in the apart state,
we will not draw those points.


188
00:09:55,529 --> 00:09:58,799 line:-2
We will still remove all of those points
from our evidence buffer,


189
00:09:59,166 --> 00:10:01,768 line:-2
but then, we will update the path
with the final point


190
00:10:01,835 --> 00:10:05,539 line:-2
and indicate that with the Boolean
isLastPointsPair set to "True."


191
00:10:07,875 --> 00:10:10,177 line:-2
One last thing I'd like to mention
in the sample


192
00:10:10,611 --> 00:10:13,447 line:-2
is that we have also set up
a gesture handler


193
00:10:13,847 --> 00:10:16,583 line:-2
that looks for a double tap
in order to clear the screen.


194
00:10:17,618 --> 00:10:20,120 line:-2
And that is how you use
our sample for hand pose


195
00:10:20,187 --> 00:10:23,156 line:-2
to use Vision framework
to draw on the screen with your hand.


196
00:10:24,658 --> 00:10:28,061 line:-2
Vision provides a couple additional things
in the API that you should know about


197
00:10:28,128 --> 00:10:29,596 line:-1
which are meant to deal with the fact


198
00:10:29,663 --> 00:10:31,765 line:-2
that there could be many hands
in the scene.


199
00:10:32,466 --> 00:10:35,702 line:-2
Perhaps you're only interested in the one
or two biggest hands in the scene


200
00:10:35,769 --> 00:10:39,239 line:-2
and don't want results returned for any
smaller hands found in the background.


201
00:10:39,773 --> 00:10:44,011 line:-2
You may control this by specifying
maximum hand count on the request.


202
00:10:44,678 --> 00:10:46,346 line:-1
The default is two,


203
00:10:46,413 --> 00:10:49,950 line:-2
so if you do want more than two,
it's important to adjust this parameter.


204
00:10:50,584 --> 00:10:54,621 line:-2
Suppose you want all the detected hands
which aren't too blurry or occluded.


205
00:10:55,355 --> 00:10:57,191 line:-1
You may set this as high as you want,


206
00:10:57,357 --> 00:11:00,093 line:-2
but know that setting this parameter
to a large number


207
00:11:00,160 --> 00:11:01,929 line:-1
will have a latency impact,


208
00:11:02,229 --> 00:11:05,399 line:-2
because the pose will be generated
for every detected hand.


209
00:11:06,567 --> 00:11:09,136 line:-1
If the parameter is set to a lower number,


210
00:11:09,203 --> 00:11:11,939 line:-2
pose will not be computed
for any hands detected


211
00:11:12,005 --> 00:11:15,976 line:-2
beyond the maximum requested,
which can help with performance.


212
00:11:16,043 --> 00:11:18,378 line:-2
It is, therefore, recommended
that you tune this parameter


213
00:11:18,445 --> 00:11:21,415 line:-2
for your application needs
with performance in mind.


214
00:11:22,082 --> 00:11:23,750 line:-1
While using hand pose in Vision,


215
00:11:23,817 --> 00:11:26,753 line:-2
it might help to take advantage
of the VNTrackObjectRequest.


216
00:11:27,721 --> 00:11:31,825 line:-2
VNTrackObjectRequest is potentially
useful in hand analysis for two reasons.


217
00:11:33,193 --> 00:11:36,964 line:0
First, if all you want to do
is track the location of the hands


218
00:11:37,030 --> 00:11:38,765 line:0
and care less about the pose,


219
00:11:38,832 --> 00:11:41,735 line:0
you may use a hand pose request
to find the hands,


220
00:11:41,802 --> 00:11:45,339 line:0
and then use VNTrackObjectRequest
from that point forward


221
00:11:45,405 --> 00:11:47,241 line:0
to know where the hands are moving.


222
00:11:48,075 --> 00:11:52,112 line:-2
Second, if you want to be more robust
about which hand is which,


223
00:11:52,179 --> 00:11:53,947 line:-1
a tracking request may help you.


224
00:11:54,715 --> 00:11:57,417 line:-2
The Vision tracker is good
at maintaining object identifiers


225
00:11:57,484 --> 00:12:00,721 line:-2
as things move offscreen
or become occluded temporarily.


226
00:12:01,555 --> 00:12:03,991 line:0
For more information
about the object tracker,


227
00:12:04,057 --> 00:12:08,762 line:0
please have a look at our WWDC session
on the matter from 2018.


228
00:12:10,163 --> 00:12:12,666 line:-2
I'm sure you're completely sold
on hand pose by now


229
00:12:12,733 --> 00:12:16,003 line:-2
and can't wait to start implementing
the apps you've already begun dreaming up,


230
00:12:16,069 --> 00:12:17,237 line:-1
but before you do,


231
00:12:17,304 --> 00:12:19,840 line:-2
there are a few accuracy considerations
to keep in mind.


232
00:12:20,707 --> 00:12:24,044 line:-2
Hands near the edges of the screen
will be partially occluded,


233
00:12:24,278 --> 00:12:27,414 line:-2
and hand pose is not guaranteed
to work well in those situations.


234
00:12:27,748 --> 00:12:31,151 line:-2
Also, the algorithm may have difficulty
with hands that are parallel


235
00:12:31,218 --> 00:12:32,819 line:-1
to the camera viewing direction,


236
00:12:32,886 --> 00:12:35,455 line:-2
like those in the image
of this karate chopper.


237
00:12:36,623 --> 00:12:40,060 line:-2
Hands covered by gloves
may also be difficult at times.


238
00:12:41,195 --> 00:12:44,698 line:-2
Finally, the algorithm will sometimes
detect feet as hands.


239
00:12:45,232 --> 00:12:47,501 line:-2
It is good to keep all
of these caveats in mind


240
00:12:47,568 --> 00:12:48,869 line:-1
when designing your app,


241
00:12:48,936 --> 00:12:50,938 line:-2
which may include instructions
to your users


242
00:12:51,004 --> 00:12:53,907 line:-2
to make the best of the experience
you plan to provide them.


243
00:12:54,908 --> 00:12:56,076 line:-1
And that is hand pose.


244
00:12:56,143 --> 00:12:59,479 line:-2
Moving on, let's now discuss
human body pose in Vision.


245
00:13:00,647 --> 00:13:04,017 line:-2
New this year, Vision is providing you
with the capability


246
00:13:04,084 --> 00:13:07,955 line:-2
to analyze many people at once
for body pose as shown in this video.


247
00:13:09,857 --> 00:13:11,859 line:-1
So like we did for hand pose,


248
00:13:11,925 --> 00:13:15,495 line:-2
let's go over some exciting ideas for apps
you can develop with body pose.


249
00:13:16,396 --> 00:13:18,365 line:-1
How about taking better action shots?


250
00:13:18,832 --> 00:13:20,767 line:-1
If you have the body pose available,


251
00:13:20,834 --> 00:13:23,237 line:-2
you can identify interesting parts
of the action,


252
00:13:23,303 --> 00:13:24,905 line:-1
like the apex of this jump.


253
00:13:26,640 --> 00:13:29,576 line:-2
That is probably the most interesting shot
of this sequence.


254
00:13:32,145 --> 00:13:34,381 line:-1
Or creating Stromotion shots?


255
00:13:35,148 --> 00:13:37,518 line:-2
Here we use human body pose
to find the frames


256
00:13:37,584 --> 00:13:40,320 line:-2
where the basketball player
doesn't overlap with himself.


257
00:13:40,387 --> 00:13:42,456 line:-1
And we can create this cool-looking image.


258
00:13:43,524 --> 00:13:45,993 line:-2
Having a look at body pose
for work and safety applications


259
00:13:46,059 --> 00:13:48,395 line:-2
might be interesting
to your application domain.


260
00:13:49,062 --> 00:13:51,932 line:-2
Perhaps it can help with training
for proper ergonomics.


261
00:13:52,933 --> 00:13:56,670 line:-2
Human body pose can be used as part
of another new feature this year as well--


262
00:13:57,471 --> 00:14:00,240 line:-1
Action Classification through CreateML.


263
00:14:00,307 --> 00:14:02,409 line:-2
Maybe you'd like to create
some fitness apps


264
00:14:02,476 --> 00:14:05,445 line:-2
that can classify an athlete
as performing the action you expect,


265
00:14:05,512 --> 00:14:07,481 line:-1
like this man performing jumping jacks.


266
00:14:08,982 --> 00:14:12,085 line:0
Or perhaps you want to know if these kids
are having any success


267
00:14:12,152 --> 00:14:14,655 line:0
in attempting
what is properly considered "dancing".


268
00:14:15,956 --> 00:14:18,892 line:0
This technology was used
in the Action and Vision application.


269
00:14:19,560 --> 00:14:22,729 line:-2
So check out the session on that
for a more in-depth look


270
00:14:22,796 --> 00:14:25,632 line:-2
on how to bring everything together
into a really cool app.


271
00:14:26,900 --> 00:14:29,102 line:-1
To analyze images for human body pose,


272
00:14:29,169 --> 00:14:31,405 line:-2
the flow is very similar
to that of hand pose.


273
00:14:31,805 --> 00:14:33,473 line:-1
As with any use of Vision,


274
00:14:33,540 --> 00:14:36,143 line:-2
the first thing to do
is to create a request handler.


275
00:14:36,577 --> 00:14:40,347 line:-2
The next step is to create
a VNDetectHumanBodyPoseRequest.


276
00:14:40,781 --> 00:14:43,617 line:-2
Then, use the request handler
to perform the requests.


277
00:14:44,284 --> 00:14:47,221 line:-2
The difference between this example
and the hand pose example


278
00:14:47,287 --> 00:14:50,390 line:-2
is that we use the word "body"
instead of "hand", and that's it.


279
00:14:51,191 --> 00:14:53,393 line:-2
Looking at the landmark points
is also analogous


280
00:14:53,460 --> 00:14:54,928 line:-1
to what is done for hand pose.


281
00:14:55,662 --> 00:14:58,665 line:-2
Getting all the points is done
exactly the same way


282
00:14:58,999 --> 00:15:01,602 line:-2
by calling
the recognizedPoints(forGroupKey


283
00:15:01,668 --> 00:15:03,804 line:-1
with VNRecognizedPointGroupAll.


284
00:15:04,538 --> 00:15:07,374 line:-2
Or you can get a specific group
of landmarks


285
00:15:07,541 --> 00:15:09,142 line:-1
as we do for the left arm here.


286
00:15:09,710 --> 00:15:13,046 line:-2
And then, you may request
a particular landmark by key,


287
00:15:13,380 --> 00:15:15,983 line:-2
as we do here
to get the left wrist landmark.


288
00:15:19,186 --> 00:15:21,488 line:-2
Let's go over the landmarks
for the human body.


289
00:15:22,022 --> 00:15:25,025 line:-2
There are VNRecognizedPointGroupKeys
for each group.


290
00:15:25,459 --> 00:15:26,994 line:-1
Here we start with the face.


291
00:15:27,728 --> 00:15:31,632 line:-2
The VNRecognizedPointKey values
for the face include the nose,


292
00:15:31,698 --> 00:15:34,201 line:-2
the left and right eye,
and the left and right ear.


293
00:15:35,802 --> 00:15:38,005 line:-2
Let's have a look
at the right arm group next.


294
00:15:38,372 --> 00:15:40,607 line:-1
Note that this is the subject's right arm,


295
00:15:40,674 --> 00:15:42,809 line:-2
not the one on the right side
of the image.


296
00:15:43,410 --> 00:15:45,379 line:-1
There are three landmarks in this group,


297
00:15:45,445 --> 00:15:47,514 line:-1
for the shoulder, the elbow and the wrist.


298
00:15:48,148 --> 00:15:52,519 line:-2
The subject's left arm also has a group
for the shoulder, elbow and wrist,


299
00:15:52,586 --> 00:15:54,855 line:-1
with associated keys listed here.


300
00:15:56,523 --> 00:15:58,225 line:-1
The torso is next.


301
00:15:58,292 --> 00:16:00,394 line:-1
Note that is also contains the shoulders.


302
00:16:01,161 --> 00:16:04,164 line:-2
So then, the shoulder landmarks appear
in more than one group.


303
00:16:05,165 --> 00:16:07,734 line:-2
Also included is a neck point
between the shoulders,


304
00:16:07,801 --> 00:16:09,269 line:-1
the left and right hip joints,


305
00:16:09,336 --> 00:16:11,638 line:-2
and a root joint
between the two hip joints.


306
00:16:12,673 --> 00:16:14,608 line:0
The subject's right leg comes next.


307
00:16:14,675 --> 00:16:17,578 line:0
And note that the hip joints appear
in both the torso group


308
00:16:17,644 --> 00:16:18,879 line:0
and each leg group.


309
00:16:19,613 --> 00:16:22,716 line:0
Also in the leg group
are the knee and ankle landmarks.


310
00:16:23,717 --> 00:16:27,454 line:0
And finally, same with the left leg,
the hip, knee and ankle landmark.


311
00:16:28,655 --> 00:16:30,891 line:-2
There are some limitations
you ought to be aware of


312
00:16:30,958 --> 00:16:32,492 line:-1
regarding body pose in Vision.


313
00:16:33,193 --> 00:16:36,230 line:-2
If the people in the scene are bent over
or upside-down,


314
00:16:36,296 --> 00:16:38,866 line:-2
the body pose algorithm will not perform
as well.


315
00:16:38,932 --> 00:16:44,071 line:-2
Also, the pose might not be determinable
due to obstructive flowing clothing.


316
00:16:44,505 --> 00:16:47,174 line:-2
Also, as you may have noticed
in the dancing example,


317
00:16:47,541 --> 00:16:50,577 line:-2
if one person is partially occluding
another in the view,


318
00:16:50,644 --> 00:16:52,980 line:-2
it is possible for the algorithm
to get confused.


319
00:16:54,915 --> 00:16:57,784 line:-2
Similar to hand pose,
the results may get worse


320
00:16:57,851 --> 00:17:00,354 line:-2
if the subject is close
to the edges of the screen.


321
00:17:00,988 --> 00:17:04,657 line:-2
And finally, the same considerations
that applied to hand pose for tracking


322
00:17:04,724 --> 00:17:06,527 line:-1
also apply to body pose.


323
00:17:08,694 --> 00:17:10,030 line:0
As you may be aware,


324
00:17:10,097 --> 00:17:13,700 line:0
Vision is not the first framework
in our SDKs to offer body pose analysis.


325
00:17:14,568 --> 00:17:17,204 line:0
Since last year,
ARKit has been providing body pose


326
00:17:17,271 --> 00:17:20,307 line:0
to developers within the context
of an ARSession.


327
00:17:21,340 --> 00:17:22,976 line:0
Here, we contrast that to Vision,


328
00:17:23,042 --> 00:17:25,546 line:0
and when it makes sense
to use one versus the other.


329
00:17:25,779 --> 00:17:28,182 line:0
You will get the same set of landmarks
either way,


330
00:17:28,248 --> 00:17:30,784 line:0
but Vision provides a confidence value
per point,


331
00:17:30,851 --> 00:17:32,152 line:0
while ARKit does not.


332
00:17:33,687 --> 00:17:36,356 line:-2
In general, the Vision framework
is capable of being used


333
00:17:36,423 --> 00:17:38,425 line:-1
on still images or camera feeds,


334
00:17:38,492 --> 00:17:40,827 line:-2
so just like anything else
Vision provides,


335
00:17:40,894 --> 00:17:43,297 line:-2
human body pose from Vision
can be used offline


336
00:17:43,363 --> 00:17:45,365 line:-1
to analyze whole libraries of images.


337
00:17:45,999 --> 00:17:49,469 line:0
ARKit's solution is designed
for live motion capture applications.


338
00:17:50,137 --> 00:17:52,206 line:0
Also, due to its specific use case,


339
00:17:52,272 --> 00:17:55,709 line:0
ARKit body pose can only be used
with a rear-facing camera


340
00:17:55,776 --> 00:17:57,611 line:0
from within an ARSession


341
00:17:57,678 --> 00:18:00,280 line:0
on supported iOS and iPadOS devices.


342
00:18:00,981 --> 00:18:03,383 line:0
Vision's API can be used
on all supported platforms


343
00:18:03,450 --> 00:18:04,685 line:0
except the watch.


344
00:18:06,019 --> 00:18:08,422 line:-2
The idea behind providing this API
through Vision


345
00:18:08,689 --> 00:18:11,692 line:-2
is precisely to make available
Apple's body pose technology


346
00:18:11,758 --> 00:18:13,393 line:-1
outside of an ARKit session.


347
00:18:14,261 --> 00:18:17,931 line:-2
However, for most ARKit use cases,
especially motion capture,


348
00:18:17,998 --> 00:18:20,667 line:-2
you should be using ARKit
to get body pose information.


349
00:18:21,635 --> 00:18:23,804 line:-1
As mentioned earlier in this discussion,


350
00:18:23,871 --> 00:18:27,941 line:-2
you can use body pose in combination
with CreateML for action classification.


351
00:18:28,809 --> 00:18:32,246 line:-2
Let's go over a couple tips for how
to best train your action classifier


352
00:18:32,312 --> 00:18:33,514 line:-1
with body pose data.


353
00:18:34,381 --> 00:18:37,050 line:-1
If you use videos directly for training,


354
00:18:37,117 --> 00:18:39,419 line:-2
you should understand that Vision
will be used


355
00:18:39,486 --> 00:18:41,488 line:-1
to generate body poses on your behalf.


356
00:18:42,155 --> 00:18:44,791 line:-2
If there is more than one subject
in the video,


357
00:18:44,858 --> 00:18:47,127 line:-2
then by default,
the largest one will be used.


358
00:18:48,362 --> 00:18:51,732 line:-2
To avoid having this default behavior
applied to your training videos,


359
00:18:51,798 --> 00:18:55,002 line:-2
it is best to ensure only the subject
of interest is in the frame.


360
00:18:55,769 --> 00:18:58,038 line:-2
You can do this by cropping
your training videos


361
00:18:58,105 --> 00:19:00,073 line:-1
so that only a single actor is present.


362
00:19:01,308 --> 00:19:05,612 line:-2
If you are not using videos,
you may also use MLMultiArray buffers


363
00:19:05,913 --> 00:19:09,283 line:-2
retrieved from Vision's
keyPointsMultiArray method.


364
00:19:09,950 --> 00:19:13,687 line:-2
With this method, you exactly control
which body poses are used for training.


365
00:19:14,288 --> 00:19:17,991 line:-2
Since Vision is used during training,
it should also be used for inference.


366
00:19:18,625 --> 00:19:21,195 line:-1
Attempting to use ARKit body pose


367
00:19:21,261 --> 00:19:24,331 line:-2
as an input to a model trained
with Vision body poses


368
00:19:24,398 --> 00:19:26,233 line:-1
will produce undefined results.


369
00:19:27,734 --> 00:19:32,072 line:-2
Finally, please be aware that once you get
to the point of performing inference,


370
00:19:32,139 --> 00:19:35,175 line:-2
you will need to pay attention to latency
on older devices.


371
00:19:36,009 --> 00:19:37,611 line:-1
While the latest and greatest devices


372
00:19:37,678 --> 00:19:39,780 line:-2
might be able to keep up
with the camera stream


373
00:19:39,847 --> 00:19:42,049 line:-2
while performing inference
for action classification,


374
00:19:42,115 --> 00:19:43,684 line:-1
older devices will not.


375
00:19:44,318 --> 00:19:46,253 line:-1
Suppose this is your image sequence.


376
00:19:47,521 --> 00:19:50,090 line:0
You will still want to retrieve body pose
for every frame,


377
00:19:50,157 --> 00:19:54,328 line:0
because the classifiers will be expecting
the body poses to be sampled at that rate.


378
00:19:54,895 --> 00:19:58,098 line:0
But do be mindful of holding on
to camera buffers for too long,


379
00:19:58,165 --> 00:19:59,933 line:0
otherwise you may starve the stream.


380
00:20:00,968 --> 00:20:05,572 line:0
But your application will perform better,
especially on older model devices,


381
00:20:05,639 --> 00:20:08,575 line:0
if you space out
the classification inferences.


382
00:20:09,109 --> 00:20:12,179 line:0
Recognizing an action
within a fraction of a second


383
00:20:12,246 --> 00:20:14,314 line:0
is pretty quick for most use cases,


384
00:20:15,249 --> 00:20:18,452 line:0
so doing inferences only a few times
per second should be okay.


385
00:20:18,852 --> 00:20:21,855 line:0
The Action and Vision sample
is a great example


386
00:20:21,922 --> 00:20:25,359 line:0
of how to use Vision and CreateML
to classify human actions.


387
00:20:26,026 --> 00:20:30,063 line:0
Here is a short demonstration clip
of the Action and Vision sample in action.


388
00:20:31,198 --> 00:20:33,400 line:0
In this clip, we see my colleague Frank


389
00:20:33,467 --> 00:20:36,103 line:0
tossing some bean bags
at a cornhole board.


390
00:20:36,970 --> 00:20:39,907 line:0
He tosses the bags
using a few different throwing techniques,


391
00:20:39,973 --> 00:20:42,676 line:0
which are classified in the app
using CreateML.


392
00:20:43,644 --> 00:20:46,313 line:-2
So let's have a look at the code
in the Action and Vision sample


393
00:20:46,380 --> 00:20:47,781 line:-1
to see how this is done.


394
00:20:48,949 --> 00:20:51,485 line:-1
We'll start in the game view controller.


395
00:20:52,085 --> 00:20:53,353 line:-1
Here in the camera view controller,


396
00:20:53,420 --> 00:20:56,089 line:-2
we are getting our CMSampleBuffers
from the camera.


397
00:20:56,156 --> 00:20:59,193 line:-2
And we provide those
to an image request handler here.


398
00:20:59,259 --> 00:21:02,863 line:-2
Later on, we use that image request
handler to perform our request.


399
00:21:03,630 --> 00:21:07,267 line:-2
Here it is,
a VNDetectHumanBodyPose request.


400
00:21:08,302 --> 00:21:12,606 line:-2
If we get a result,
then we call humanBoundingBox.


401
00:21:13,173 --> 00:21:15,876 line:-2
Let's go into humanBoundingBox
to see what this is doing.


402
00:21:17,311 --> 00:21:18,645 line:-1
In humanBoundingBox,


403
00:21:18,712 --> 00:21:22,583 line:-2
we're trying to find
the humanBoundingBox around the person.


404
00:21:22,649 --> 00:21:25,252 line:-2
We'll start by getting all the points
out of the observation


405
00:21:25,319 --> 00:21:28,589 line:-2
by calling
recognizedPoints(forGroupKey: all.


406
00:21:28,655 --> 00:21:30,824 line:-2
Then we can iterate
over all the points here.


407
00:21:31,892 --> 00:21:34,928 line:-2
We look here to see
if the point has enough confidence.


408
00:21:34,995 --> 00:21:37,664 line:-2
And if it does, we add it
to our normalizedBoundingBox.


409
00:21:39,833 --> 00:21:42,669 line:-2
Here, we're doing something where
we're trying to find the body joints


410
00:21:42,736 --> 00:21:45,706 line:-2
for the right-hand side of the body
so we can overlay them on the screen.


411
00:21:45,772 --> 00:21:47,741 line:-2
But since we're talking about
action classification,


412
00:21:47,808 --> 00:21:49,109 line:-1
I'll skip that for now.


413
00:21:49,943 --> 00:21:53,347 line:-2
Later on, we're going
to be storing these observations.


414
00:21:53,413 --> 00:21:55,549 line:-2
Let's look at what's happening
in storeObservation,


415
00:21:55,616 --> 00:21:56,750 line:-1
and why we're doing it.


416
00:21:58,018 --> 00:22:01,788 line:-2
In storeObservation,
we have a buffer of observations


417
00:22:01,855 --> 00:22:03,690 line:-1
that we call "poseObservations."


418
00:22:04,558 --> 00:22:07,461 line:-2
What we're going to do is use that
as a ring buffer.


419
00:22:08,128 --> 00:22:11,632 line:-2
If this buffer is full,
we will remove the first one,


420
00:22:12,232 --> 00:22:14,635 line:-2
and either way we're going
to append the next one.


421
00:22:15,068 --> 00:22:18,038 line:-2
We use this later on
when a throw is detected.


422
00:22:18,305 --> 00:22:22,476 line:-2
A throw is detected when
the trajectory analysis detects a throw.


423
00:22:22,543 --> 00:22:25,379 line:-2
And then we call this function
"getLastThrowType"


424
00:22:25,445 --> 00:22:27,481 line:-1
to find out what kind of throw was made.


425
00:22:28,148 --> 00:22:30,350 line:-2
We're going to use our action classifier
for this,


426
00:22:30,417 --> 00:22:33,687 line:-2
but we need to put all of our observations
into the correct format.


427
00:22:34,288 --> 00:22:36,790 line:-2
So we will call
prepareInputWithObservations


428
00:22:36,857 --> 00:22:37,858 line:-1
in order to do that.


429
00:22:38,492 --> 00:22:40,527 line:-1
Let's look at what that function's doing.


430
00:22:41,328 --> 00:22:46,433 line:-2
In prepareInputWithObservations,
we see that we need 60 observations,


431
00:22:46,500 --> 00:22:49,036 line:-2
and we need to put them
in a multiArrayBuffer


432
00:22:49,102 --> 00:22:51,538 line:-1
of type MLMultiArray.


433
00:22:51,605 --> 00:22:52,773 line:-1
Here in this loop,


434
00:22:52,840 --> 00:22:56,009 line:-2
we iterate over all the observations
that we have in our buffer.


435
00:22:56,076 --> 00:23:00,013 line:-2
And for each observation
we call keypointsMultiArray.


436
00:23:00,080 --> 00:23:01,982 line:-1
And we put that into our buffer.


437
00:23:02,482 --> 00:23:06,153 line:-2
If we don't have 60,
then we'll pad with zeros.


438
00:23:06,220 --> 00:23:08,422 line:-1
Notice the MLMultiArray(shape


439
00:23:09,156 --> 00:23:11,358 line:-1
and data type that's expected.


440
00:23:12,726 --> 00:23:15,262 line:-2
And then we can catenate those
and return them.


441
00:23:16,163 --> 00:23:17,798 line:-1
Going back to getLastThrowType...


442
00:23:19,066 --> 00:23:22,603 line:-2
we provide that input
into PlayerActionClassifierInput,


443
00:23:22,669 --> 00:23:24,671 line:-1
which is our ML feature provider.


444
00:23:25,572 --> 00:23:28,141 line:-2
Once we have that,
we provide it to our action classifier


445
00:23:28,208 --> 00:23:30,544 line:-1
by calling prediction with the input.


446
00:23:31,111 --> 00:23:34,081 line:0
Then, once we have the output,
we look over all the probabilities


447
00:23:34,147 --> 00:23:37,084 line:0
to see which of the possibilities
are the most likely,


448
00:23:37,518 --> 00:23:39,386 line:0
and then, we return the Throw Type


449
00:23:39,453 --> 00:23:41,522 line:0
that corresponds
to that maximum probability.


450
00:23:42,556 --> 00:23:45,192 line:-2
And that is how you do
action classification


451
00:23:45,259 --> 00:23:47,594 line:-1
using body pose from Vision framework.


452
00:23:48,896 --> 00:23:50,964 line:-2
Now that you've had a closer look
at the code


453
00:23:51,031 --> 00:23:53,500 line:-2
in the Action and Vision application
for body pose,


454
00:23:53,567 --> 00:23:54,835 line:-1
congratulations!


455
00:23:54,902 --> 00:23:57,437 line:-2
Along with everything else
you've learned in this session,


456
00:23:57,504 --> 00:24:00,674 line:-2
you are prepared to go create
amazing applications.


457
00:24:00,741 --> 00:24:03,443 line:-2
Applications that do more
when it comes to analyzing people


458
00:24:03,510 --> 00:24:06,046 line:-2
with computer vision
with the Vision framework.


459
00:24:07,080 --> 00:24:09,416 line:-1
And that is hand and body pose in Vision.


460
00:24:09,483 --> 00:24:12,686 line:-2
Have a great time continuing
your WWDC journey.

