1
00:00:00,133 --> 00:00:02,169 line:-1
[chimes]


2
00:00:03,804 --> 00:00:06,306 line:-1
Hello and welcome to WWDC.


3
00:00:08,275 --> 00:00:12,279 line:0
Hi, my name is Aseem,
and I'm from the Core ML team.


4
00:00:12,346 --> 00:00:14,848 line:0
In this session, I want to share with you


5
00:00:14,915 --> 00:00:19,052 line:0
a few exciting new developments
in Core ML converters.


6
00:00:19,987 --> 00:00:23,557 line:-2
We have been working hard
on improving the experience


7
00:00:23,624 --> 00:00:25,759 line:-1
of converting models to Core ML


8
00:00:25,826 --> 00:00:29,162 line:-2
and have significantly updated
our conversion tools.


9
00:00:30,163 --> 00:00:35,068 line:-2
Let's first start by looking, though,
at why Core ML is such a great solution


10
00:00:35,135 --> 00:00:38,105 line:-2
for integrating machine learning
into your apps.


11
00:00:38,172 --> 00:00:41,308 line:-1
Since Core ML was launched in 2017,


12
00:00:41,375 --> 00:00:45,913 line:-2
our mission has always been
about making it as easy as possible


13
00:00:45,979 --> 00:00:49,383 line:-2
to deploy machine learning models
into your application


14
00:00:49,449 --> 00:00:52,553 line:-2
to create a wide range
of compelling experiences.


15
00:00:53,387 --> 00:00:57,157 line:0
With Core ML, the same model
can be conveniently deployed


16
00:00:57,224 --> 00:00:59,359 line:0
to all types of Apple devices,


17
00:00:59,426 --> 00:01:02,429 line:0
and the best compatibility and performance


18
00:01:02,496 --> 00:01:06,166 line:0
across OS and device generations
is guaranteed.


19
00:01:06,700 --> 00:01:10,938 line:0
Core ML models seamlessly leverage
all the hardware acceleration


20
00:01:11,004 --> 00:01:12,706 line:0
available on the device,


21
00:01:12,773 --> 00:01:17,711 line:0
be it the CPU, the GPU
or the Apple Neural Engine,


22
00:01:17,778 --> 00:01:21,849 line:0
which is specifically designed
for accelerating neural networks.


23
00:01:22,683 --> 00:01:25,118 line:-1
In addition, with each new release,


24
00:01:25,185 --> 00:01:27,988 line:-1
you get the best of Apple's ecosystem.


25
00:01:28,655 --> 00:01:34,161 line:0
For instance, this year we are introducing
Core ML model deployment


26
00:01:34,228 --> 00:01:37,698 line:0
to make it really easy
to update your models.


27
00:01:37,764 --> 00:01:41,902 line:0
Furthermore,
Core ML models can now be encrypted.


28
00:01:42,736 --> 00:01:45,239 line:0
For more details,
please check out our session


29
00:01:45,305 --> 00:01:49,009 line:0
on "Model Deployment and Security
with Core ML."


30
00:01:49,610 --> 00:01:53,080 line:-2
So really, the starting point
to unlock all this awesomeness


31
00:01:53,146 --> 00:01:54,948 line:-1
is the Core ML model.


32
00:01:55,015 --> 00:01:58,485 line:-2
And how it can be created
is the topic of this session.


33
00:01:59,453 --> 00:02:01,922 line:0
A variety of machine learning models,


34
00:02:01,989 --> 00:02:04,525 line:0
ranging from deep learning to tree based,


35
00:02:04,591 --> 00:02:06,994 line:0
can be expressed in Core ML.


36
00:02:07,060 --> 00:02:11,098 line:0
Of course, one of the best sources
is the Create ML app,


37
00:02:11,164 --> 00:02:14,334 line:0
but you can also easily create an ML model


38
00:02:14,401 --> 00:02:16,803 line:0
starting from your favorite framework


39
00:02:16,870 --> 00:02:20,474 line:0
using the Core ML Tools Python package.


40
00:02:21,275 --> 00:02:24,411 line:0
Over time, as the ML ecosystem grows,


41
00:02:24,478 --> 00:02:29,383 line:0
the Core ML converters continue
to extend support to more frameworks.


42
00:02:30,350 --> 00:02:33,153 line:0
This year,
we have some exciting announcements


43
00:02:33,220 --> 00:02:36,390 line:0
regarding our support
for neural network libraries.


44
00:02:37,457 --> 00:02:41,428 line:0
So far we have supported
the conversion of neural network models


45
00:02:41,495 --> 00:02:43,730 line:0
from one of these frameworks.


46
00:02:43,797 --> 00:02:48,602 line:0
This year, we have focused
on the two most commonly used libraries


47
00:02:48,669 --> 00:02:50,404 line:0
by the deep learning community,


48
00:02:50,470 --> 00:02:53,040 line:0
which are PyTorch and TensorFlow.


49
00:02:53,907 --> 00:02:55,709 line:0
Lets look at TensorFlow first.


50
00:02:56,977 --> 00:03:00,948 line:0
So far, if you wanted to convert
a TensorFlow model to Core ML,


51
00:03:01,014 --> 00:03:04,751 line:0
you would have to
additionally install tfcoreml


52
00:03:04,818 --> 00:03:06,486 line:0
and use its API,


53
00:03:06,553 --> 00:03:10,157 line:0
which internally depended
on the Core ML Tools package.


54
00:03:10,224 --> 00:03:12,159 line:0
Well, this has changed,


55
00:03:12,226 --> 00:03:15,762 line:0
and now all you need is Core ML Tools.


56
00:03:15,829 --> 00:03:19,433 line:0
We have now fully integrated
TensorFlow conversion


57
00:03:19,499 --> 00:03:21,435 line:0
within Core ML Tools.


58
00:03:21,502 --> 00:03:27,341 line:0
We are also very excited to announce
a much expanded support for TensorFlow 2.


59
00:03:27,407 --> 00:03:31,378 line:0
TensorFlow 1 has been supported
for a while now through tfcoreml,


60
00:03:31,445 --> 00:03:35,949 line:0
and we had added support for TF 2
convolutional models last year.


61
00:03:36,750 --> 00:03:41,355 line:0
This year, we significantly expanded
to include dynamic models


62
00:03:41,421 --> 00:03:44,591 line:0
such as LSTMs, transformers, et cetera.


63
00:03:45,859 --> 00:03:48,829 line:0
The new converter supports
all the different formats


64
00:03:48,896 --> 00:03:51,365 line:0
in which a TensorFlow model
can be exported.


65
00:03:52,266 --> 00:03:54,501 line:0
Let's look at conversion from PyTorch now.


66
00:03:55,302 --> 00:03:59,406 line:0
So far, the way to do this
is to use the PyTorch export tool


67
00:03:59,473 --> 00:04:01,475 line:0
to produce an ONNX model


68
00:04:01,542 --> 00:04:05,179 line:0
and then use ONNX Core ML
to get to ML model.


69
00:04:05,245 --> 00:04:10,050 line:0
However, many times
the first export step may fail


70
00:04:10,117 --> 00:04:14,655 line:0
since ONNX is an open standard
that evolves independently,


71
00:04:14,721 --> 00:04:17,757 line:0
so it may lack
a newly added feature to PyTorch,


72
00:04:17,824 --> 00:04:20,226 line:0
or the torch exporter hasn't been updated,


73
00:04:20,293 --> 00:04:22,229 line:0
or maybe it has a bug.


74
00:04:22,296 --> 00:04:26,533 line:0
Well, now this extra dependency
has been removed


75
00:04:26,600 --> 00:04:29,770 line:0
since we have implemented
a new PyTorch converter.


76
00:04:30,571 --> 00:04:35,342 line:0
It's now a one-step process
starting from a torch_script_model.


77
00:04:36,743 --> 00:04:41,148 line:0
You may have noticed that the API
to invoke the PyTorch converter


78
00:04:41,215 --> 00:04:44,818 line:0
is exactly the same
as was used with TensorFlow.


79
00:04:44,885 --> 00:04:47,721 line:0
That's because we have redesigned the API


80
00:04:47,788 --> 00:04:51,625 line:0
to keep a single call
to invoke all converters.


81
00:04:52,359 --> 00:04:53,360 line:0
It works,


82
00:04:53,427 --> 00:04:57,531 line:0
irrespective of which source framework
the model needs to be converted from.


83
00:04:58,932 --> 00:05:03,770 line:0
With these changes,
Core ML Tools is now the one-stop shop


84
00:05:03,837 --> 00:05:07,040 line:0
for converting models
from TensorFlow and PyTorch.


85
00:05:08,008 --> 00:05:10,811 line:0
And it's not just the API
that has changed.


86
00:05:10,878 --> 00:05:14,548 line:0
We haven't simply added
two new converter paths.


87
00:05:14,615 --> 00:05:17,951 line:0
Instead, we have undertaken a major effort


88
00:05:18,018 --> 00:05:20,721 line:0
to redesign the converter architecture


89
00:05:20,787 --> 00:05:25,092 line:0
to significantly improve
the experience and code quality.


90
00:05:25,859 --> 00:05:31,164 line:0
So, we have moved from having
separate converter pipelines,


91
00:05:31,231 --> 00:05:33,400 line:-1
that were built when different converters


92
00:05:33,467 --> 00:05:35,636 line:-1
were added at different points of time,


93
00:05:35,702 --> 00:05:40,174 line:0
to a single converter stack
with maximum code reuse.


94
00:05:40,841 --> 00:05:42,709 line:0
And to achieve this consolidation,


95
00:05:42,776 --> 00:05:46,713 line:0
we have introduced
a new in-memory representation


96
00:05:46,780 --> 00:05:51,685 line:0
called Model Intermediate Language,
or MIL for short.


97
00:05:52,586 --> 00:05:56,490 line:0
MIL has been designed
to streamline the conversion process


98
00:05:56,557 --> 00:06:00,127 line:0
and make it easy to add support
for new frameworks.


99
00:06:00,561 --> 00:06:04,164 line:0
It unifies the stack
by providing a common interface


100
00:06:04,231 --> 00:06:07,367 line:0
to capture information
from different frameworks.


101
00:06:08,635 --> 00:06:12,439 line:0
It has a set of operations,
optimization passes


102
00:06:12,506 --> 00:06:14,274 line:0
and a model builder API.


103
00:06:15,175 --> 00:06:18,612 line:-2
As an end user,
you generally don't interact with MIL,


104
00:06:18,679 --> 00:06:21,582 line:-2
but it can be really useful
in certain scenarios.


105
00:06:22,416 --> 00:06:24,985 line:-2
We will revisit MIL
a bit later in this talk.


106
00:06:25,052 --> 00:06:28,388 line:-2
Let's first see a few examples
of using the new converters.


107
00:06:29,690 --> 00:06:32,593 line:-2
Let's start with
a simple image classifier example


108
00:06:32,659 --> 00:06:35,762 line:-1
to get familiar with the new unified API.


109
00:06:35,829 --> 00:06:38,866 line:-2
Let me switch
to the Jupyter notebook for that.


110
00:06:39,333 --> 00:06:41,401 line:-1
Here I am in a Jupyter notebook,


111
00:06:41,468 --> 00:06:44,438 line:-1
and I've already imported Core ML Tools.


112
00:06:44,505 --> 00:06:48,775 line:-2
Let's convert from TensorFlow 2 first,
which I've imported as well.


113
00:06:49,710 --> 00:06:53,146 line:-2
I'll grab a model
from the TensorFlow 2 model zoo.


114
00:06:55,382 --> 00:06:57,351 line:-1
I'm using a MobileNet model,


115
00:06:57,417 --> 00:07:01,321 line:-2
which is a popular convolutional model
for image classification.


116
00:07:01,388 --> 00:07:02,723 line:-1
Let's load it.


117
00:07:04,091 --> 00:07:05,659 line:-1
And let's convert it.


118
00:07:06,660 --> 00:07:11,265 line:-1
To do that I simply type ct.convert...


119
00:07:13,000 --> 00:07:17,070 line:-2
and provide to it
the TensorFlow model object


120
00:07:17,137 --> 00:07:18,305 line:-1
and hit enter.


121
00:07:19,306 --> 00:07:22,309 line:-2
The converter automatically detects
the type of the model,


122
00:07:22,376 --> 00:07:25,145 line:-1
its input shapes, outputs, et cetera,


123
00:07:25,212 --> 00:07:27,714 line:-1
and continues to convert it through MIL.


124
00:07:28,549 --> 00:07:30,684 line:-1
Okay, it's done. That was easy!


125
00:07:30,751 --> 00:07:32,252 line:-1
Why don't we try it again?


126
00:07:32,886 --> 00:07:36,056 line:-1
This time let's use a model from PyTorch.


127
00:07:40,694 --> 00:07:45,032 line:-2
For that, let me go ahead
and import torch and torchvision.


128
00:07:45,098 --> 00:07:48,135 line:-2
And I'll grab a model this time
from torchvision,


129
00:07:48,202 --> 00:07:50,037 line:-1
the mobilenet v2 model.


130
00:07:51,238 --> 00:07:55,843 line:-2
Now, we need a TorchScript model
to convert to Core ML,


131
00:07:55,909 --> 00:07:59,813 line:-2
which can be obtained
by either scripting or tracing.


132
00:08:00,414 --> 00:08:02,149 line:-1
I'm gonna use tracing here.


133
00:08:07,554 --> 00:08:11,558 line:-2
Tracing can be done
by using functions provided by PyTorch.


134
00:08:12,059 --> 00:08:13,260 line:-1
Let's see how.


135
00:08:14,228 --> 00:08:18,365 line:0
We first get the model in inference mode
by using eval.


136
00:08:19,032 --> 00:08:22,436 line:0
And then invoke the jit.trace method,


137
00:08:22,503 --> 00:08:25,105 line:0
which needs an example input to work.


138
00:08:25,439 --> 00:08:26,673 line:0
Let's hit enter.


139
00:08:26,740 --> 00:08:28,575 line:0
So, we have the traced model,


140
00:08:28,642 --> 00:08:31,812 line:0
and now we can use Core ML Tools
to convert it.


141
00:08:33,280 --> 00:08:36,850 line:0
So, I'll again type ct.convert.


142
00:08:36,917 --> 00:08:40,120 line:0
And this time,
I provided the traced model.


143
00:08:40,187 --> 00:08:41,522 line:0
Now, one more thing.


144
00:08:41,587 --> 00:08:44,258 line:0
Generally,
the information about input shape


145
00:08:44,324 --> 00:08:46,994 line:0
is not present in the TorchScript model,


146
00:08:47,060 --> 00:08:49,329 line:0
but that's required for conversion.


147
00:08:49,396 --> 00:08:51,532 line:0
So let's give it to the converter.


148
00:08:51,598 --> 00:08:55,469 line:0
And this can be done
using the inputs argument


149
00:08:55,536 --> 00:08:59,673 line:0
to which I will provide
the type and shape of the input.


150
00:09:00,574 --> 00:09:04,178 line:0
And I can do that
by using the TensorType class,


151
00:09:05,145 --> 00:09:07,147 line:0
which takes a shaping.


152
00:09:10,851 --> 00:09:12,686 line:0
That's it. Let's hit enter.


153
00:09:13,954 --> 00:09:17,491 line:0
We see the familiar steps
of conversion to MIL,


154
00:09:17,558 --> 00:09:19,726 line:0
a few optimization passes


155
00:09:19,793 --> 00:09:22,095 line:0
and finally conversion to ML model.


156
00:09:22,162 --> 00:09:23,463 line:0
And that's done.


157
00:09:23,530 --> 00:09:28,569 line:0
Let's check out the model interface
by printing the ML model object.


158
00:09:30,070 --> 00:09:31,138 line:-1
Let's see.


159
00:09:31,205 --> 00:09:35,209 line:-2
So, we see
that there is an input called input.1,


160
00:09:35,275 --> 00:09:37,277 line:-1
and it's of type multiArray


161
00:09:37,344 --> 00:09:39,513 line:-1
since we provided a TensorType here.


162
00:09:40,180 --> 00:09:43,417 line:-1
And there is an output called 1648.


163
00:09:43,483 --> 00:09:45,586 line:-1
Hmm. That is a bit odd.


164
00:09:45,652 --> 00:09:47,154 line:-1
What really happened there


165
00:09:47,221 --> 00:09:51,692 line:-2
is that that's the name
of the output tensor in the torch model,


166
00:09:51,758 --> 00:09:54,761 line:-2
and the converter just automatically
picked it up from there.


167
00:09:55,696 --> 00:09:59,933 line:-2
Well, it's easy to rename it
to something more meaningful.


168
00:10:00,667 --> 00:10:01,902 line:-1
Let's see how.


169
00:10:03,170 --> 00:10:07,774 line:0
So I use the rename feature utility
from Core ML Tools


170
00:10:07,841 --> 00:10:11,912 line:0
to rename my inputs and outputs
to whatever I want.


171
00:10:11,979 --> 00:10:15,249 line:0
In this case, I use a placeholder name.


172
00:10:15,315 --> 00:10:16,817 line:0
So let's hit enter


173
00:10:16,884 --> 00:10:20,254 line:0
and then again print the ML model object.


174
00:10:20,320 --> 00:10:21,455 line:-1
Nice.


175
00:10:21,522 --> 00:10:25,759 line:-2
So we see that the name of inputs
and outputs has been updated


176
00:10:25,826 --> 00:10:27,561 line:-1
to the names that I provided.


177
00:10:28,896 --> 00:10:31,899 line:-1
Let me do one last conversion now,


178
00:10:31,965 --> 00:10:34,168 line:-1
this time from TensorFlow 1.


179
00:10:34,234 --> 00:10:37,137 line:-2
For that,
let me move to a different notebook,


180
00:10:37,204 --> 00:10:40,040 line:-2
where I already set up
my TensorFlow 1 environment.


181
00:10:40,107 --> 00:10:45,679 line:-2
I also pre-downloaded
the mobilenet TensorFlow 1 model,


182
00:10:45,746 --> 00:10:49,283 line:-1
which is in this protobuf pb format.


183
00:10:49,349 --> 00:10:53,120 line:-2
Along with this model came
the labels.txt file,


184
00:10:53,187 --> 00:10:57,424 line:-2
which contains the names of the classes
that this model was trained on.


185
00:10:57,925 --> 00:10:59,593 line:-1
So let's convert this model.


186
00:11:00,594 --> 00:11:03,664 line:-1
We'll invoke our familiar convert API,


187
00:11:03,730 --> 00:11:07,501 line:-2
and now we can provide
the protobuf pb file to it.


188
00:11:08,302 --> 00:11:09,469 line:-1
Now, this will work.


189
00:11:09,536 --> 00:11:12,739 line:-2
But let's make a nice Core ML model
this time


190
00:11:12,806 --> 00:11:15,309 line:-1
by doing a couple of extra things.


191
00:11:16,076 --> 00:11:20,314 line:-1
So, earlier we had used the TensorType,


192
00:11:20,380 --> 00:11:23,016 line:-2
but since this model really works
on an image, you know,


193
00:11:23,083 --> 00:11:25,452 line:-2
it would be nice
to let the converter know about it.


194
00:11:26,220 --> 00:11:31,491 line:-2
And I can do that
by using the ct.ImageType class,


195
00:11:32,426 --> 00:11:35,896 line:-2
to which I'll provide
a couple of pre-processing parameters.


196
00:11:36,597 --> 00:11:37,598 line:-1
Bias...


197
00:11:39,299 --> 00:11:42,636 line:-1
for each channel in an RGB image


198
00:11:42,703 --> 00:11:44,004 line:-1
and scale.


199
00:11:46,039 --> 00:11:50,077 line:-2
This will normalize the image
as expected by the mobilenet model.


200
00:11:52,079 --> 00:11:57,784 line:-2
Another thing that I'll change is that
since this model performs classification,


201
00:11:57,851 --> 00:12:02,489 line:-2
you know, probably it's a good idea
to generate a classifier Core ML model.


202
00:12:03,090 --> 00:12:08,395 line:-2
And this can be done
by using the ClassifierConfig class,


203
00:12:08,462 --> 00:12:12,566 line:-1
which takes the labels.txt file as is.


204
00:12:13,300 --> 00:12:15,035 line:-1
Isn't this great?


205
00:12:15,102 --> 00:12:16,637 line:-1
Let's hit enter.


206
00:12:20,340 --> 00:12:23,710 line:-2
And we are done. Let's go ahead
and save this model to disk.


207
00:12:23,777 --> 00:12:26,547 line:-2
Before that though,
I'll add some useful metadata


208
00:12:26,613 --> 00:12:29,283 line:-1
regarding license and author.


209
00:12:29,349 --> 00:12:33,854 line:0
And then I'll go ahead and save the model
by typing mlmodel.save.


210
00:12:35,189 --> 00:12:39,459 line:0
I'll name my model as mobilenet.mlmodel.


211
00:12:41,295 --> 00:12:44,231 line:0
And now I can see that it's on disk.


212
00:12:44,298 --> 00:12:47,067 line:0
Let's check out this model in Finder.


213
00:12:47,601 --> 00:12:51,104 line:-2
So we see the model is here.
Let me click to open it.


214
00:12:51,171 --> 00:12:53,407 line:-1
It automatically opens in Xcode.


215
00:12:53,473 --> 00:12:56,577 line:-1
This year, we have updated our Xcode UI.


216
00:12:57,144 --> 00:13:01,148 line:-2
For classifiers now,
the class labels are visible right here.


217
00:13:01,215 --> 00:13:04,518 line:-2
As we can see,
this model has about 1,000 classes.


218
00:13:04,585 --> 00:13:08,889 line:-2
There is also a new tab called Preview
which is really convenient.


219
00:13:08,956 --> 00:13:10,357 line:-1
And I really like it.


220
00:13:10,424 --> 00:13:14,528 line:-2
We can simply drag and drop
a few images in here,


221
00:13:14,595 --> 00:13:17,998 line:-2
and it will automatically run our model
on these images


222
00:13:18,065 --> 00:13:19,733 line:-1
and display the predictions.


223
00:13:19,800 --> 00:13:24,471 line:-2
As we can see here, our model seems
to be doing quite well on these images.


224
00:13:24,538 --> 00:13:27,841 line:-2
So that wraps up the conversion API demo.
Let's recap.


225
00:13:27,908 --> 00:13:32,212 line:-2
We invoked the conversion function
with different model types,


226
00:13:32,279 --> 00:13:33,814 line:-1
and it just worked.


227
00:13:33,881 --> 00:13:37,818 line:-2
Now let's try converting
a slightly more complex model.


228
00:13:37,885 --> 00:13:41,555 line:-2
For that, I'd like to invite
my colleague Gitesh,


229
00:13:41,622 --> 00:13:45,993 line:-2
who will convert a model
that's used in translating audio to text.


230
00:13:46,860 --> 00:13:48,662 line:-1
Thanks, Aseem.


231
00:13:48,729 --> 00:13:52,933 line:-2
Hi, I am Gitesh,
an engineer in the Core ML team.


232
00:13:53,000 --> 00:13:57,304 line:-2
In this demo,
I will illustrate the automatic handling


233
00:13:57,371 --> 00:14:00,941 line:-2
of flexible shapes
and related capabilities


234
00:14:01,008 --> 00:14:04,011 line:-1
of the new Core ML Tools conversion API.


235
00:14:04,745 --> 00:14:10,050 line:-2
And I will demonstrate these using
an Automatic Speech Recognition task.


236
00:14:10,117 --> 00:14:14,588 line:-2
In this task,
the input is a speech audio file,


237
00:14:14,655 --> 00:14:17,891 line:-2
and the output
is the text transcription of it.


238
00:14:17,958 --> 00:14:21,361 line:-2
There are many approaches
to Automatic Speech Recognition.


239
00:14:21,428 --> 00:14:26,767 line:-2
The system that I use in my example
consists of three stages.


240
00:14:27,601 --> 00:14:30,337 line:-1
There are pre- and post-processing stages


241
00:14:30,404 --> 00:14:34,875 line:-2
and a neural network model in the middle
to do the heavy lifting.


242
00:14:36,343 --> 00:14:40,714 line:-2
Pre-processing involves extracting
the mel spectrum,


243
00:14:40,781 --> 00:14:43,217 line:-1
also called MFCCs,


244
00:14:43,283 --> 00:14:45,819 line:-1
from the raw audio file.


245
00:14:45,886 --> 00:14:49,857 line:-2
These MFCCs are fed
to the neural network model,


246
00:14:50,724 --> 00:14:54,361 line:-2
which returns
a character level time series


247
00:14:54,428 --> 00:14:56,730 line:-1
of probability distributions.


248
00:14:57,798 --> 00:15:01,969 line:-2
Those are then post-processed
by a CTC decoder


249
00:15:02,035 --> 00:15:04,671 line:-1
to produce the final transcription.


250
00:15:05,672 --> 00:15:10,010 line:-2
Pre- and post-processing stages
employ standard techniques


251
00:15:10,077 --> 00:15:12,579 line:-1
which can be easily implemented.


252
00:15:12,646 --> 00:15:17,417 line:-2
Therefore, my focus will be
on converting this model in the center.


253
00:15:18,118 --> 00:15:21,688 line:-2
I use a pre-trained TensorFlow model
called DeepSpeech.


254
00:15:22,923 --> 00:15:27,394 line:-1
At a high level, this model uses an LSTM


255
00:15:27,461 --> 00:15:31,365 line:-2
and a few dense layers stacked
on top of each other.


256
00:15:31,431 --> 00:15:36,470 line:-2
And such an architecture is quite common
for seq2seq models.


257
00:15:36,537 --> 00:15:39,439 line:-2
Now, let's jump right
into the Jupyter notebook


258
00:15:39,506 --> 00:15:42,009 line:-1
to convert this model to Core ML


259
00:15:42,075 --> 00:15:44,678 line:-1
and try it out on a few audio samples.


260
00:15:44,745 --> 00:15:47,481 line:-1
We start with importing some packages.


261
00:15:47,548 --> 00:15:50,751 line:-2
I found the pre-trained weights
for DeepSpeech model


262
00:15:50,817 --> 00:15:54,121 line:-1
on this GitHub repository from Mozilla


263
00:15:54,188 --> 00:15:57,391 line:-2
and have already downloaded
those weights and a script


264
00:15:57,457 --> 00:16:00,827 line:-2
to export the TensorFlow 1 model
from that repository.


265
00:16:00,894 --> 00:16:02,563 line:-1
Let's run the script.


266
00:16:05,899 --> 00:16:10,571 line:-2
We now have a frozen TensorFlow graph
in the protobuf format.


267
00:16:10,637 --> 00:16:13,674 line:-1
Let's look into the outputs of this graph.


268
00:16:13,740 --> 00:16:18,345 line:-2
And for that, I have already written
some inspection utilities.


269
00:16:20,814 --> 00:16:23,650 line:-1
So, this model has four outputs,


270
00:16:23,717 --> 00:16:26,820 line:-1
and this first one, called "mfccs,"


271
00:16:26,887 --> 00:16:30,824 line:-2
represents the output
of the pre-processing stage.


272
00:16:30,891 --> 00:16:34,094 line:-1
Which means the exported TensorFlow graph


273
00:16:34,161 --> 00:16:37,231 line:-1
contains not just the DeepSpeech model,


274
00:16:37,297 --> 00:16:40,367 line:-1
but also the pre-processing sub-graph.


275
00:16:41,335 --> 00:16:44,471 line:-2
Let's strip off
this pre-processing component


276
00:16:44,538 --> 00:16:47,474 line:-2
by providing
the remaining three output names


277
00:16:47,541 --> 00:16:49,743 line:-1
to the unified converter function.


278
00:16:49,810 --> 00:16:54,047 line:-2
And with this information,
let's call the Core ML converter.


279
00:17:01,255 --> 00:17:03,824 line:-1
Nice! The conversion is successful.


280
00:17:03,891 --> 00:17:07,828 line:-2
Now let's run this converted model
on an audio sample.


281
00:17:07,895 --> 00:17:10,864 line:-1
First, we load and play an audio file.


282
00:17:12,699 --> 00:17:16,236 line:-2
<i>Once upon a time,</i>
<i>there was an exploding chicken.</i>


283
00:17:16,303 --> 00:17:18,338 line:-1
<i>He met up with a golden tiger,</i>


284
00:17:18,405 --> 00:17:21,474 line:-2
<i>and together, they walked</i>
<i>through the green forest.</i>


285
00:17:21,942 --> 00:17:22,943 line:-1
<i>The end.</i>


286
00:17:23,676 --> 00:17:25,913 line:-1
Next, we pre-process it.


287
00:17:26,579 --> 00:17:29,516 line:-2
For the full pipeline
to work in this notebook,


288
00:17:29,583 --> 00:17:34,454 line:-2
I have already constructed
these pre- and post-processing functions


289
00:17:34,521 --> 00:17:37,224 line:-1
using code in the DeepSpeech repository.


290
00:17:41,395 --> 00:17:46,099 line:-2
So, this pre-processing
has transformed the audio file


291
00:17:46,166 --> 00:17:49,303 line:-1
into a tensor object of this shape,


292
00:17:49,369 --> 00:17:52,606 line:-2
and the shape can be viewed
as one audio file,


293
00:17:52,673 --> 00:17:56,977 line:-1
pre-processed into 636 sequences,


294
00:17:57,044 --> 00:18:02,082 line:-2
each of width 19
and containing 26 coefficients.


295
00:18:02,149 --> 00:18:06,587 line:-2
The number of these sequences
change with the length of the audio.


296
00:18:06,653 --> 00:18:12,426 line:-2
For this 12 seconds audio file,
we have 636 sequences.


297
00:18:12,492 --> 00:18:17,397 line:-2
Let's now inspect the input shapes
that the model expects.


298
00:18:19,199 --> 00:18:22,803 line:-2
So, we see that
the first input of this model


299
00:18:22,870 --> 00:18:24,972 line:-1
has almost the correct shape.


300
00:18:25,038 --> 00:18:30,644 line:-2
The only difference is that it can process
16 sequences at a time.


301
00:18:30,711 --> 00:18:35,883 line:-2
Therefore, I will write a loop
to break the input features into chunks


302
00:18:35,949 --> 00:18:39,353 line:-2
and feed each segment to the model
one by one.


303
00:18:40,120 --> 00:18:43,757 line:-2
I've written that bit of code already.
Let me paste it here.


304
00:18:44,424 --> 00:18:46,860 line:-1
You don't need to follow all this code.


305
00:18:46,927 --> 00:18:50,364 line:-2
Basically,
we break the pre-processed feature


306
00:18:50,430 --> 00:18:53,267 line:-1
into slices of size 16


307
00:18:53,333 --> 00:18:56,103 line:-1
and run prediction on each slice,


308
00:18:56,170 --> 00:18:59,406 line:-1
with some state management, inside a loop.


309
00:18:59,473 --> 00:19:00,974 line:-1
Let's run it.


310
00:19:09,483 --> 00:19:10,484 line:0
Nice.


311
00:19:10,551 --> 00:19:13,153 line:0
The transcription looks pretty accurate.


312
00:19:13,220 --> 00:19:15,522 line:-1
Now, everything looks good,


313
00:19:15,589 --> 00:19:19,159 line:-2
but wouldn't it be great
if we could run the prediction


314
00:19:19,226 --> 00:19:21,962 line:-1
on the entire pre-processed feature


315
00:19:22,029 --> 00:19:23,730 line:-1
in just one go?


316
00:19:23,797 --> 00:19:25,766 line:-1
Well, it's possible,


317
00:19:25,832 --> 00:19:29,336 line:-2
and we would need a dynamic
TensorFlow model for that.


318
00:19:29,403 --> 00:19:33,640 line:-2
Let's rerun the same script
from the DeepSpeech repository


319
00:19:33,707 --> 00:19:36,310 line:-1
to obtain a dynamic graph.


320
00:19:36,376 --> 00:19:41,448 line:0
This time, we provide
an additional flag called "n_steps."


321
00:19:41,515 --> 00:19:44,618 line:-1
This flag corresponds to sequence length


322
00:19:44,685 --> 00:19:48,055 line:-1
and had a default value of 16.


323
00:19:48,121 --> 00:19:50,791 line:-1
But now we set it to -1,


324
00:19:50,858 --> 00:19:55,696 line:-2
which means that the sequence length
can take any positive value.


325
00:19:56,730 --> 00:19:59,166 line:-1
We have a new TensorFlow model.


326
00:19:59,233 --> 00:20:00,734 line:-1
Let's convert it.


327
00:20:12,913 --> 00:20:14,815 line:-1
Great. Conversion is done.


328
00:20:14,882 --> 00:20:19,086 line:-2
Let's inspect how this model
is different from the previous one.


329
00:20:22,055 --> 00:20:25,959 line:-2
Well, one difference I see
is that this Core ML model


330
00:20:26,026 --> 00:20:29,796 line:-2
can work on inputs
of arbitrary sequence length.


331
00:20:29,863 --> 00:20:32,733 line:-2
And the difference lies
not just in the shapes.


332
00:20:32,799 --> 00:20:35,802 line:-1
Under the hood, this dynamic Core ML model


333
00:20:35,869 --> 00:20:40,374 line:-2
is much more complicated
than the previous static one.


334
00:20:40,440 --> 00:20:44,912 line:-2
It has lots of dynamic operations
such as get shape,


335
00:20:44,978 --> 00:20:46,947 line:-1
dynamic reshape, et cetera.


336
00:20:47,681 --> 00:20:52,986 line:-2
However, our experience of converting it
was exactly the same.


337
00:20:53,053 --> 00:20:58,225 line:-2
The converter handled it with just
the same amount of ease as before.


338
00:20:58,292 --> 00:21:02,529 line:-2
Let's now validate the model
on the same audio file.


339
00:21:04,765 --> 00:21:07,134 line:-1
This time, we don't need the loop


340
00:21:07,201 --> 00:21:11,905 line:-2
and can directly feed
the entire input features to the model.


341
00:21:11,972 --> 00:21:13,240 line:-1
Let's run it.


342
00:21:19,012 --> 00:21:22,149 line:-1
Great. Transcription looks perfect again.


343
00:21:22,216 --> 00:21:24,785 line:-1
Let's recap what we saw in this demo.


344
00:21:25,252 --> 00:21:28,422 line:-2
So, we worked with two variants
of the DeepSpeech model.


345
00:21:29,323 --> 00:21:34,328 line:-2
On a static TensorFlow graph,
the converter produced a Core ML model


346
00:21:34,394 --> 00:21:36,930 line:-1
with inputs of fixed shape.


347
00:21:37,831 --> 00:21:42,002 line:-2
And with the dynamic variant,
we obtained a Core ML model


348
00:21:42,069 --> 00:21:45,839 line:-2
which could accept inputs
of any sequence length.


349
00:21:46,540 --> 00:21:50,310 line:-2
Converter handled both cases
transparently...


350
00:21:51,311 --> 00:21:55,315 line:0
and without making any change
to the conversion call.


351
00:21:55,883 --> 00:21:59,119 line:-2
One thing I did not get the chance
to show in the demo--


352
00:21:59,186 --> 00:22:01,955 line:0
we can start
with a dynamic TensorFlow graph


353
00:22:02,022 --> 00:22:04,525 line:0
and get a static Core ML model.


354
00:22:04,591 --> 00:22:06,593 line:0
Let's see how we can do it.


355
00:22:07,394 --> 00:22:10,731 line:0
First,
we define a Type description object...


356
00:22:12,366 --> 00:22:14,268 line:0
with name of the input...


357
00:22:15,435 --> 00:22:17,104 line:0
and its shape.


358
00:22:18,572 --> 00:22:22,709 line:0
Then we feed this object
to the conversion API.


359
00:22:24,111 --> 00:22:25,212 line:0
That's all.


360
00:22:25,279 --> 00:22:28,348 line:0
Under the hood,
the type and value inference


361
00:22:28,415 --> 00:22:30,517 line:0
propagates this shape information


362
00:22:30,584 --> 00:22:34,188 line:0
to remove all the unnecessary
dynamic operations.


363
00:22:34,254 --> 00:22:39,293 line:-2
Therefore, static models
are likely to be more performant


364
00:22:39,359 --> 00:22:43,030 line:-2
while the dynamic ones
are definitely more flexible.


365
00:22:44,131 --> 00:22:49,236 line:-2
Which one to use depends
on the requirements of your application.


366
00:22:49,870 --> 00:22:53,240 line:-1
At this point, we have seen few examples


367
00:22:53,307 --> 00:22:55,776 line:-1
of successful conversion to Core ML.


368
00:22:55,843 --> 00:22:58,245 line:-1
However, in some cases,


369
00:22:58,312 --> 00:23:02,082 line:-2
we might encounter
an unsupported op error.


370
00:23:02,149 --> 00:23:04,751 line:-1
In fact, I recently hit this issue.


371
00:23:04,818 --> 00:23:06,420 line:-1
Let me show that to you.


372
00:23:06,486 --> 00:23:10,691 line:-2
So, I was exploring this library
for natural language models


373
00:23:10,757 --> 00:23:12,559 line:-1
called "transformers."


374
00:23:12,626 --> 00:23:16,997 line:-2
And a recent model called T5
had caught my attention.


375
00:23:17,064 --> 00:23:18,398 line:-1
Let's convert it.


376
00:23:18,465 --> 00:23:22,503 line:-2
First, we load the pre-trained model
from the library.


377
00:23:22,569 --> 00:23:27,641 line:-2
Since the returned object
is an instance of tf.keras model...


378
00:23:29,376 --> 00:23:32,513 line:-2
we can pass it directly
into the Core ML converter.


379
00:23:32,579 --> 00:23:34,147 line:-1
Let's do that.


380
00:23:37,651 --> 00:23:41,088 line:-1
And here we see this unsupported op error


381
00:23:41,154 --> 00:23:43,290 line:-1
for the operation "Einsum."


382
00:23:43,357 --> 00:23:46,226 line:-1
I will now hand it back to Aseem,


383
00:23:46,293 --> 00:23:50,230 line:-2
who will go through a few approaches
to handle this issue,


384
00:23:50,297 --> 00:23:53,267 line:-2
and then we will come back
to convert this model.


385
00:23:53,767 --> 00:23:57,137 line:-2
We recognize that hitting this error
is a challenge


386
00:23:57,204 --> 00:24:00,107 line:-1
in an ever-evolving machine learning space


387
00:24:00,174 --> 00:24:04,878 line:-2
as new ops are regularly added
to TensorFlow or PyTorch,


388
00:24:04,945 --> 00:24:07,748 line:-2
or you may be using
a custom-built op yourself.


389
00:24:08,615 --> 00:24:10,284 line:-1
What to do in this case?


390
00:24:10,350 --> 00:24:15,289 line:-2
Well, one option
is to use a Core ML custom layer,


391
00:24:15,355 --> 00:24:18,392 line:-1
which allows you to accompany the ML model


392
00:24:18,458 --> 00:24:21,595 line:-2
with your own swift implementation
of the op.


393
00:24:21,662 --> 00:24:24,131 line:-1
This is great, but in many cases,


394
00:24:24,198 --> 00:24:27,568 line:-2
it may be possible
to take another easier approach.


395
00:24:28,135 --> 00:24:31,638 line:-2
You can use
what we call the "composite op,"


396
00:24:31,705 --> 00:24:35,275 line:-2
which does not require
writing additional Swift code


397
00:24:35,342 --> 00:24:39,246 line:-2
since it can keep everything bundled
in the ML model file.


398
00:24:40,113 --> 00:24:44,117 line:-2
A composite op is built
from existing MIL ops.


399
00:24:44,651 --> 00:24:47,421 line:-1
Let's dig a little bit into what MIL is


400
00:24:47,487 --> 00:24:50,224 line:-2
and how we can construct
a composite op using it.


401
00:24:50,958 --> 00:24:54,194 line:-2
We developed
the model intermediate language


402
00:24:54,261 --> 00:24:56,430 line:-1
to unify the converter stack.


403
00:24:56,496 --> 00:25:01,602 line:-2
If we expand to look at the internals,
this stack consists of three sections--


404
00:25:02,469 --> 00:25:06,039 line:-2
the frontends,
the intermediate MIL portion


405
00:25:06,106 --> 00:25:08,041 line:-1
and the backend.


406
00:25:08,108 --> 00:25:12,212 line:-2
For each source framework,
there is a separate frontend,


407
00:25:12,279 --> 00:25:16,650 line:-2
which captures
a framework specific representation.


408
00:25:16,717 --> 00:25:19,887 line:-1
After that, an MIL program is built,


409
00:25:19,953 --> 00:25:24,224 line:-2
at which point the representation
becomes source agnostic.


410
00:25:24,291 --> 00:25:27,194 line:-1
A lot of common optimization passes


411
00:25:27,261 --> 00:25:30,864 line:-2
such as operator fusions,
dead code elimination,


412
00:25:30,931 --> 00:25:33,100 line:-1
constant propagation, et cetera,


413
00:25:33,166 --> 00:25:34,468 line:-1
happen here,


414
00:25:34,535 --> 00:25:37,838 line:-1
after which the graph is serialized


415
00:25:37,905 --> 00:25:40,207 line:-1
to the protobuf ML Model format.


416
00:25:40,274 --> 00:25:42,743 line:-1
Another way to look at the same picture


417
00:25:42,809 --> 00:25:45,312 line:-2
is that each source framework
has its own dialect


418
00:25:45,379 --> 00:25:49,883 line:-2
which is converted to MIL,
as a consolidation point, to ML model.


419
00:25:49,950 --> 00:25:54,621 line:-2
This is one way of going to the MIL format
which is what the converter does.


420
00:25:54,688 --> 00:25:58,959 line:-2
But there is another way
to directly write out an MIL program


421
00:25:59,726 --> 00:26:02,129 line:-1
using the builder API.


422
00:26:02,196 --> 00:26:04,498 line:-1
MIL is a stand-alone language


423
00:26:04,565 --> 00:26:08,802 line:-2
that can be used to directly express
a neural network model.


424
00:26:08,869 --> 00:26:11,605 line:-1
And it is quite similar in its API


425
00:26:11,672 --> 00:26:14,608 line:-2
to the ones many of you
are already quite familiar with,


426
00:26:14,675 --> 00:26:17,544 line:-2
whether you are a TensorFlow 2
or a PyTorch user.


427
00:26:17,611 --> 00:26:19,847 line:-1
Let's check out this builder API.


428
00:26:19,913 --> 00:26:24,117 line:-2
Here is how we can start writing
an MIL program in Python.


429
00:26:24,751 --> 00:26:27,588 line:-1
We import the builder and define the input


430
00:26:27,654 --> 00:26:29,756 line:-1
by specifying its shape,


431
00:26:29,823 --> 00:26:34,828 line:-1
which is 1, 100, 100, 3 in this case.


432
00:26:34,895 --> 00:26:39,466 line:-2
We can print a description of the program
by simply calling print.


433
00:26:39,533 --> 00:26:43,237 line:-2
In the description below,
we see that the type of the input


434
00:26:43,303 --> 00:26:45,973 line:-1
was inferred to be Float32,


435
00:26:46,039 --> 00:26:47,708 line:-1
which is the default type.


436
00:26:47,774 --> 00:26:49,910 line:-1
Now let's add the first op.


437
00:26:49,977 --> 00:26:54,047 line:-2
So, we added a ReLu op
with this simple syntax.


438
00:26:54,114 --> 00:26:56,984 line:-2
Let's add another op,
a transpose op this time.


439
00:26:57,551 --> 00:26:59,920 line:-1
A great thing about the MIL builder


440
00:26:59,987 --> 00:27:03,824 line:-2
is that it instantly performs
type and shape inference.


441
00:27:03,891 --> 00:27:07,160 line:-2
We can see here that the shape
of the output of the transpose


442
00:27:07,227 --> 00:27:10,697 line:-2
has been updated correctly
in the description below.


443
00:27:10,764 --> 00:27:14,268 line:-2
Let's add a reduction operation
on the last two axes.


444
00:27:14,968 --> 00:27:17,871 line:-2
And we see, as expected,
the shape of the tensor


445
00:27:17,938 --> 00:27:19,740 line:-1
is 1, 3 now.


446
00:27:19,806 --> 00:27:21,708 line:-1
Let's add one last op.


447
00:27:21,775 --> 00:27:26,246 line:-2
Finally, the program returns
the output of the log op.


448
00:27:26,313 --> 00:27:30,250 line:-2
So, we see that the API
to define a network in MIL


449
00:27:30,317 --> 00:27:31,985 line:-1
is quite straightforward.


450
00:27:32,052 --> 00:27:36,523 line:-2
Let's now see how it can be used
to implement a composite op


451
00:27:36,590 --> 00:27:39,193 line:-1
and bypass the unsupported op error.


452
00:27:39,259 --> 00:27:42,796 line:-2
Let me hand it back over to Gitesh
to illustrate this.


453
00:27:43,330 --> 00:27:45,933 line:-1
So, we were converting the T5 model


454
00:27:45,999 --> 00:27:50,504 line:-2
and had come across
an unsupported op error for Einsum.


455
00:27:51,171 --> 00:27:54,041 line:-1
I read the TensorFlow documentation on it


456
00:27:54,107 --> 00:27:58,111 line:-2
and found that it refers
to Einstein summation notation.


457
00:27:59,012 --> 00:28:03,650 line:-2
Lots of operations like reduce_sum,
transpose, trace, et cetera,


458
00:28:03,717 --> 00:28:07,654 line:-2
can be expressed in this notation
using a string.


459
00:28:07,721 --> 00:28:09,790 line:-1
For this particular conversion,


460
00:28:09,857 --> 00:28:13,794 line:-2
let's focus on the notation
that this model uses.


461
00:28:14,761 --> 00:28:16,430 line:-1
Looking at the error trace,


462
00:28:16,496 --> 00:28:20,667 line:0
we see that this model uses Einsum
with this notation,


463
00:28:20,734 --> 00:28:24,471 line:0
which translates to
following mathematical expression.


464
00:28:24,538 --> 00:28:26,840 line:0
This might look complicated,


465
00:28:26,907 --> 00:28:31,278 line:0
but, effectively,
it is just a batched matrix multiplication


466
00:28:31,345 --> 00:28:34,114 line:0
with a transpose on the second input.


467
00:28:34,181 --> 00:28:39,253 line:0
And that's great since MIL supports
this operation directly.


468
00:28:39,319 --> 00:28:41,722 line:0
Let's now write a composite op.


469
00:28:41,788 --> 00:28:45,592 line:0
First, we import MIL Builder
and a decorator.


470
00:28:47,194 --> 00:28:49,429 line:0
Then we define a function


471
00:28:49,496 --> 00:28:52,799 line:0
with the same name
as TensorFlow operation,


472
00:28:52,866 --> 00:28:55,302 line:0
which is Einsum in this case.


473
00:28:56,003 --> 00:29:00,474 line:0
Next, we decorate this function
to register it with the converter.


474
00:29:01,441 --> 00:29:05,546 line:0
This would ensure that the right function
would be invoked


475
00:29:05,612 --> 00:29:10,050 line:0
whenever Einsum operation
is encountered during the conversion.


476
00:29:12,719 --> 00:29:15,355 line:0
And finally, we grab the inputs


477
00:29:15,422 --> 00:29:19,092 line:0
and define a MatMul operation
using MIL Builder.


478
00:29:19,693 --> 00:29:20,928 line:0
That's all.


479
00:29:20,994 --> 00:29:23,830 line:-1
Let's call the Core ML converter again.


480
00:29:35,843 --> 00:29:37,678 line:-1
Conversion is completed.


481
00:29:37,744 --> 00:29:41,748 line:-2
To verify if it is successful,
let's print the ML model.


482
00:29:46,587 --> 00:29:47,588 line:-1
Perfect!


483
00:29:48,455 --> 00:29:51,758 line:-1
To recap, while converting T5 model,


484
00:29:51,825 --> 00:29:56,063 line:-2
we had hit an unsupported op error
for Einsum.


485
00:29:56,129 --> 00:29:59,700 line:-2
In general,
Einsum is a complicated operation


486
00:29:59,766 --> 00:30:03,470 line:-2
and can represent
a variety of tensor operations,


487
00:30:03,537 --> 00:30:07,674 line:-2
but we did not have to worry
about all the possible cases.


488
00:30:07,741 --> 00:30:13,580 line:-2
We just handled the particular
parameterization needed for this model,


489
00:30:13,647 --> 00:30:18,118 line:-2
and that was easily implemented
using composite ops.


490
00:30:18,185 --> 00:30:21,255 line:-1
To summarize, we have packed Core ML Tools


491
00:30:21,321 --> 00:30:25,692 line:-2
with many new features
such as powerful type inference,


492
00:30:25,759 --> 00:30:28,629 line:-1
user-friendly APIs, et cetera,


493
00:30:28,695 --> 00:30:33,300 line:-2
which make Core ML converters
easier to use and more extensible.


494
00:30:33,367 --> 00:30:35,335 line:-1
To read more about these features,


495
00:30:35,402 --> 00:30:40,207 line:-2
visit our new documentation
that contains several examples


496
00:30:40,274 --> 00:30:43,010 line:-1
including the demos for this session.


497
00:30:43,510 --> 00:30:46,079 line:-1
To wrap up, we have announced


498
00:30:46,146 --> 00:30:51,852 line:-2
the new PyTorch converter
and enhanced support for TensorFlow 2.


499
00:30:51,919 --> 00:30:55,589 line:-2
These are available
through the new unified API


500
00:30:55,656 --> 00:30:58,225 line:-1
and made possible via MIL.


501
00:30:59,426 --> 00:31:02,796 line:-2
We would like to invite all of you
to try them out


502
00:31:02,863 --> 00:31:06,867 line:-2
and help us make Core ML Tools
even better with your feedback.


503
00:31:06,934 --> 00:31:08,368 line:-1
Thank you.


504
00:31:08,435 --> 00:31:10,537 line:-1
[chimes]

