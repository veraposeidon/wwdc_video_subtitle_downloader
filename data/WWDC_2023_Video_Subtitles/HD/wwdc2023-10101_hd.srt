2
00:00:00,334 --> 00:00:06,340 line:-1
♪ ♪


3
00:00:10,010 --> 00:00:12,379 line:-1
Ethan: Hi, I'm Ethan.


4
00:00:12,412 --> 00:00:14,648 line:-2
I'm from the Siri Understanding team
and here to talk to you


5
00:00:14,681 --> 00:00:17,618 line:-2
about some exciting developments
in speech recognition.


6
00:00:17.651 --> 00:00:20.587 line:-2 align:center
In iOS 10,
we introduced the Speech framework.


7
00:00:20.621 --> 00:00:22.890 line:-2 align:center
It allowed you to leverage the same
technology


8
00:00:22.923 --> 00:00:25.192 line:-1 align:center
that powers Siri and keyboard dictation


9
00:00:25,225 --> 00:00:29,530 line:-2
to create voice enabled apps
by using a simple, intuitive interface.


10
00:00:29.563 --> 00:00:31.598 line:-1 align:center
However, the speech recognizer class,


11
00:00:31.632 --> 00:00:34.168 line:-2 align:center
out of the box,
isn't suitable for all apps.


12
00:00:34.201 --> 00:00:37.771 line:-2 align:center
To explain why, let's talk
about how speech recognition works.


13
00:00:37.804 --> 00:00:40.641 line:-2 align:center
A speech recognition system
first feeds audio data


14
00:00:40.674 --> 00:00:44.912 line:-2 align:center
into an Acoustic Model,
which produces a phonetic representation.


15
00:00:44.945 --> 00:00:48.315 line:-2 align:center
Then the phonetic representation
is converted into a written form,


16
00:00:48,348 --> 00:00:50,017 line:-1
or transcription.


17
00:00:50,050 --> 00:00:53,453 line:-2
Sometimes, multiple phonetic
representations fit the audio data,


18
00:00:53.487 --> 00:00:57.624 line:-2 align:center
or a single phonetic representation
may correspond to multiple transcriptions.


19
00:00:57.658 --> 00:01:00.994 line:-2 align:center
In these cases, we end up
with multiple candidate transcriptions,


20
00:01:01.028 --> 00:01:03.030 line:-1 align:center
and we need a way to disambiguate.


21
00:01:03.063 --> 00:01:06.600 line:-2 align:center
To do this, we employ something
called a language model.


22
00:01:06,633 --> 00:01:09,102 line:-2
A language model predicts
the likelihood that a given word


23
00:01:09.136 --> 00:01:11.705 line:-1 align:center
will come next in a sequence of words.


24
00:01:11,738 --> 00:01:14,308 line:-2
Applied to a whole sentence,
it can give us a feel for whether


25
00:01:14.341 --> 00:01:16.977 line:-1 align:center
the sentence is probably nonsense.


26
00:01:17,010 --> 00:01:19,880 align:center
The language model helps us
reject candidates which are unlikely,


27
00:01:19,913 --> 00:01:24,251 line:0
based on patterns of usage that the model
was exposed to during training.


28
00:01:24.284 --> 00:01:26.854 line:-2 align:center
Since iOS 10,
the Speech framework has encapsulated


29
00:01:26,887 --> 00:01:30,724 line:-2
this entire process in order to present
an interface that is easy to use.


30
00:01:30,757 --> 00:01:34,828 line:-2
To understand why that might not be ideal,
let's consider an example.


31
00:01:34.862 --> 00:01:37.731 line:-2 align:center
I love to play chess,
and I've been working on a chess app


32
00:01:37.764 --> 00:01:39.967 line:-2 align:center
that will let users dictate
individual moves,


33
00:01:40,000 --> 00:01:42,903 line:-1
as well as common openings and defenses.


34
00:01:42,936 --> 00:01:47,140 line:-2
Here, my opponent has played
the classic Queen's Gambit.


35
00:01:47,174 --> 00:01:49,810 line:-2
I've been studying,
and I like the response E5,


36
00:01:49,843 --> 00:01:51,979 line:-1
the Albin counter gambit.


37
00:01:53.080 --> 00:01:55.582 line:-1 align:center
Play the Albin counter gambit.


38
00:01:56,817 --> 00:01:58,719 align:center
Uh oh, there's a problem.


39
00:01:58,752 --> 00:02:00,454 align:center
The recognizer is misrecognizing


40
00:02:00,487 --> 00:02:03,357 line:0
my chess move as a music request.


41
00:02:03,390 --> 00:02:06,260 line:-2
The language model
that the recognizer uses was exposed


42
00:02:06.293 --> 00:02:08.929 line:-2 align:center
to a lot of music requests
during its training process,


43
00:02:08.962 --> 00:02:12.099 line:-2 align:center
so it is prepared for queries like,
“Play the album,”


44
00:02:12.132 --> 00:02:13.867 line:-1 align:center
followed by an album name.


45
00:02:13.901 --> 00:02:18.639 line:-2 align:center
Conversely, it's probably never
encountered my preferred transcription.


46
00:02:18,672 --> 00:02:21,074 line:-2
By abstracting away the behavior
of the language model,


47
00:02:21,108 --> 00:02:24,344 line:-2
the Speech framework forces
all apps to use the same model,


48
00:02:24.378 --> 00:02:27.781 line:-2 align:center
even though different domains
call for different behavior.


49
00:02:27.814 --> 00:02:31.251 line:-2 align:center
Beginning in iOS 17,
you'll be able to customize the behavior


50
00:02:31,285 --> 00:02:33,854 line:-2
of the SFSpeechRecognizer's
language model,


51
00:02:33,887 --> 00:02:37,057 line:-2
tailor it to your application,
and improve its accuracy.


52
00:02:38.058 --> 00:02:40.427 line:-2 align:center
To get started
with language model customization,


53
00:02:40.460 --> 00:02:43.363 line:-2 align:center
first create a collection
of training data.


54
00:02:43,397 --> 00:02:45,999 line:-2
You can do this
during your development process.


55
00:02:46.033 --> 00:02:48.735 line:-2 align:center
Then, in your app,
you'll prepare the data,


56
00:02:48.769 --> 00:02:51.905 line:-2 align:center
configure a recognition request,
and then run it.


57
00:02:53,240 --> 00:02:57,377 line:-2
Let's talk about the process of building
your collection of training data.


58
00:02:57,411 --> 00:03:00,814 line:-2
At a high level,
training data will consist of bits


59
00:03:00,848 --> 00:03:05,285 line:-2
of text that represent phrases
your app's users are likely to speak.


60
00:03:05,319 --> 00:03:07,988 line:-2
These will teach the model
to expect those phrases


61
00:03:08,021 --> 00:03:11,792 line:-2
and boost the likelihood
that they'll be recognized correctly.


62
00:03:11.825 --> 00:03:14.328 line:-2 align:center
Experiment often,
as it's surprising to see how


63
00:03:14,361 --> 00:03:17,865 line:-2
capable the recognizer is,
and how much it improves over time.


64
00:03:18,765 --> 00:03:21,134 line:-2
The speech framework introduces
a new class that acts


65
00:03:21,168 --> 00:03:22,870 line:-1
as a container for training data.


66
00:03:22.903 --> 00:03:25.572 line:-1 align:center
It's built using the result builder DSL.


67
00:03:25,606 --> 00:03:30,744 line:-2
You can provide an exact phrase or part
of a phrase using a PhraseCount object.


68
00:03:30.777 --> 00:03:33.881 line:-2 align:center
A PhraseCount will also describe
how many times the sample should be


69
00:03:33,914 --> 00:03:36,250 line:-1
represented in the final data set.


70
00:03:36.283 --> 00:03:40.454 line:-2 align:center
This can be used to weight
certain phrases more heavily than others.


71
00:03:40.487 --> 00:03:42.856 line:-2 align:center
Only so much data can
be accepted by the system,


72
00:03:42.890 --> 00:03:47.694 line:-2 align:center
so balance your need to boost phrases
against your overall training data budget.


73
00:03:47,728 --> 00:03:50,430 line:-2
You can also leverage templates
to generate a large number


74
00:03:50.464 --> 00:03:52.866 line:-1 align:center
of samples that fit a regular pattern.


75
00:03:52.900 --> 00:03:57.604 line:-2 align:center
Here, I've defined three classes of words
that together make up a chess move.


76
00:03:57,638 --> 00:04:01,408 line:-2
The piece to move, which doubles
as the file that I'm targeting,


77
00:04:01,441 --> 00:04:04,945 line:-2
the royal piece that defines
which side of the board to play on,


78
00:04:04.978 --> 00:04:06.947 line:-1 align:center
and the rank to move to.


79
00:04:06,980 --> 00:04:08,815 line:-1
By putting them together into a pattern,


80
00:04:08.849 --> 00:04:13.253 line:-2 align:center
I can easily generate data samples
representing every possible move.


81
00:04:13,287 --> 00:04:15,656 line:-2
Here, the count applies
to the entire template,


82
00:04:15,689 --> 00:04:18,659 line:-2
so I'll get 10,000 samples
representing chess moves,


83
00:04:18.692 --> 00:04:21.895 line:-2 align:center
divided evenly among all
of the resulting data samples.


84
00:04:21,929 --> 00:04:25,098 align:center
When I'm done building up the data object,
I export it to a file


85
00:04:25,132 --> 00:04:27,601 align:center
and deploy into my app
like any other asset.


86
00:04:28,936 --> 00:04:31,772 line:-2
If your app makes use
of specialized terminology,


87
00:04:31,805 --> 00:04:35,442 line:-2
for example, a medical app that
includes the names of pharmaceuticals,


88
00:04:35.475 --> 00:04:38.812 line:-2 align:center
you can define both the spelling
and pronunciations of those terms,


89
00:04:38.846 --> 00:04:41.949 line:-2 align:center
and provide phrase counts
that demonstrates their usage.


90
00:04:41.982 --> 00:04:45.919 line:-2 align:center
Pronunciations are accepted
in the form of X-SAMPA strings.


91
00:04:45.953 --> 00:04:50.123 line:-2 align:center
Each locale supports a unique subset
of pronunciation symbols.


92
00:04:50.157 --> 00:04:52.926 line:-2 align:center
Refer to the documentation
for the full set of locales


93
00:04:52.960 --> 00:04:55.062 line:-1 align:center
and supported symbols.


94
00:04:55.095 --> 00:04:57.497 line:-2 align:center
For my app,
I want to make sure the recognizer can


95
00:04:57,531 --> 00:05:02,603 line:-2
understand the Winawer variation,
a common variant of the French defense.


96
00:05:02,636 --> 00:05:05,873 line:-2
I describe the pronunciation
using the subset of X-SAMPA symbols


97
00:05:05,906 --> 00:05:08,041 line:-1
that are supported by this locale.


98
00:05:09,009 --> 00:05:13,480 line:-2
You can use the same API to train on data
that your app can access at runtime.


99
00:05:13,514 --> 00:05:16,383 line:-2
You might do this to support
usage patterns that are specific


100
00:05:16,416 --> 00:05:19,186 line:-2
to your users,
such as focusing on the chess openings


101
00:05:19,219 --> 00:05:22,189 line:-2
and defenses
that your user is trying to learn.


102
00:05:22.222 --> 00:05:24.691 line:-2 align:center
You might also want
to train on named entities.


103
00:05:24.725 --> 00:05:28.495 line:-2 align:center
Maybe your app supports network play
against your user's contacts.


104
00:05:28.529 --> 00:05:32.332 line:-2 align:center
And as always, respecting
the user's privacy is paramount.


105
00:05:33,033 --> 00:05:37,337 line:-2
For example, a communication app may want
to boost commands to call contacts


106
00:05:37.371 --> 00:05:38.805 line:-1 align:center
based on the frequency with which


107
00:05:38,839 --> 00:05:41,208 line:-1
those contacts appear in the call history.


108
00:05:41,241 --> 00:05:42,809 line:-1
This kind of information should always


109
00:05:42,843 --> 00:05:44,778 line:-1
stay on device.


110
00:05:44.811 --> 00:05:49.183 line:-2 align:center
You simply call into the same methods from
within your app to generate a data object,


111
00:05:49,216 --> 00:05:52,619 line:-2
write it to a file,
and ingest it as shown earlier.


112
00:05:53,620 --> 00:05:57,524 line:-2
Once training data is generated,
it is bound to a single locale.


113
00:05:57,558 --> 00:06:00,527 line:-2
If you want to support multiple locales
within a single script,


114
00:06:00,561 --> 00:06:02,596 line:-2
you can use
standard localization facilities


115
00:06:02,629 --> 00:06:05,232 line:-1
like NSLocalizedString to do so.


116
00:06:05,265 --> 00:06:08,602 line:-2
Now, let's talk about deploying
your model in your app.


117
00:06:08.635 --> 00:06:12.873 line:-2 align:center
First, you need to call a new method,
prepareCustomLanguageModel,


118
00:06:12.906 --> 00:06:15.843 line:-2 align:center
that accepts the file
we generated in the previous step


119
00:06:15.876 --> 00:06:19.580 line:-2 align:center
and produces
two new files that we'll use later.


120
00:06:19,613 --> 00:06:23,016 line:-2
This method call can have
a large amount of associated latency,


121
00:06:23,050 --> 00:06:25,452 line:-2
so it's best to call it
off the main thread,


122
00:06:25.485 --> 00:06:30.257 line:-2 align:center
and hide the latency behind some UI,
such as a loading screen.


123
00:06:30.290 --> 00:06:32.526 line:-2 align:center
Sometimes,
you need to keep data on the device


124
00:06:32,559 --> 00:06:35,863 line:-2
where it's generated
in order to respect the user's privacy.


125
00:06:35,896 --> 00:06:37,831 line:-1
LM customization supports this


126
00:06:37,865 --> 00:06:40,734 line:-2
by never sending customization data
over the network.


127
00:06:40.767 --> 00:06:44.671 line:-2 align:center
All customized requests
are serviced strictly on device.


128
00:06:44.705 --> 00:06:47.174 line:-2 align:center
When your app constructs
the speech recognition request,


129
00:06:47,207 --> 00:06:50,477 line:-2
you first enforce
that the recognition is run on device.


130
00:06:50,511 --> 00:06:55,115 line:-2
Failing to do so will cause requests
to be serviced without customization.


131
00:06:55,148 --> 00:06:58,986 line:-2
Then you attach the language model
to your request object.


132
00:06:59,019 --> 00:07:03,290 line:-2
Now, with LM customization
turned on in my app...


133
00:07:03,323 --> 00:07:05,993 line:0
Play the Albin counter gambit.


134
00:07:06,026 --> 00:07:08,762 align:center
My custom terms work as well.


135
00:07:08,795 --> 00:07:11,231 line:0
Play the Winawer variation.


136
00:07:12,566 --> 00:07:14,034 align:center
By customizing the language model,


137
00:07:14,067 --> 00:07:16,803 line:0
I've tuned the recognizer
to my application's domain,


138
00:07:16,837 --> 00:07:19,406 align:center
and I've gained some control
over how it behaves.


139
00:07:19,439 --> 00:07:20,574 align:center
Most importantly,


140
00:07:20.607 --> 00:07:24.011 line:-2 align:center
I've improved
speech recognition accuracy for my app.


141
00:07:24,044 --> 00:07:27,381 line:-2
Now that the Speech framework can
be adapted to more apps and more users,


142
00:07:27.414 --> 00:07:32.186 line:-2 align:center
it's even more powerful and can be used
to create even better experiences.


143
00:07:32.219 --> 00:07:34.488 line:-2 align:center
Language model customization
provides you a way


144
00:07:34.521 --> 00:07:37.991 line:-4 align:center
to enhance the speech recognizer
and customize it for your apps.


145
00:07:38,025 --> 00:07:40,627 line:-2
I'm super excited
to see all of the amazing things


146
00:07:40,661 --> 00:07:42,029 line:-1
you'll accomplish with it.


147
00:07:42,062 --> 00:07:44,464 line:-2
Thank you,
and remember to play for the center.


148
00:07:44,498 --> 00:07:48,068 line:0
♪ ♪

