2
00:00:00.334 --> 00:00:07.341 line:-1
♪ ♪


3
00:00:10.911 --> 00:00:12.980 line:-1
David: Hi. My name is David Findlay.


4
00:00:13.013 --> 00:00:15.682 line:-2
I'm a machine learning engineer
on the Create ML team.


5
00:00:15.716 --> 00:00:17.317 line:-2
We've been working
on some great improvements


6
00:00:17.351 --> 00:00:19.419 line:-1
to the Create ML app and frameworks.


7
00:00:19.453 --> 00:00:22.422 line:-2
I'm excited to take you
on a tour of what's new.


8
00:00:22.456 --> 00:00:25.459 line:-2
Training a large-scale model from scratch
can take thousands of hours,


9
00:00:25.492 --> 00:00:28.228 line:-2
millions of annotated files,
and expert domain knowledge.


10
00:00:28.262 --> 00:00:30.931 line:-2
Our goal is to give you the tools
to build great apps


11
00:00:30.964 --> 00:00:33.800 line:-2
that use machine learning
without all of the overhead.


12
00:00:33.834 --> 00:00:36.336 line:-2
We've gone through the process
of creating state-of-the-art models


13
00:00:36.370 --> 00:00:37.771 line:-1
that power many features,


14
00:00:37.804 --> 00:00:39.773 line:-2
like the Search experience
in the Photos app


15
00:00:39.806 --> 00:00:42.609 line:-2
and Custom Sound Recognition
in Accessibility.


16
00:00:42.643 --> 00:00:45.145 line:-2
Create ML gives you access
to our latest technology


17
00:00:45.179 --> 00:00:47.848 line:-2
so you can build your own
custom machine learning experiences,


18
00:00:47.881 --> 00:00:49.383 line:-1
without the hassle.


19
00:00:49.416 --> 00:00:53.453 line:-2
I'll start by going through improvements
that we've made to Create ML.


20
00:00:53.487 --> 00:00:56.356 line:-2
Then I'll introduce a brand-new way
of building machine learning models


21
00:00:56.390 --> 00:00:59.126 line:-1
to understand scenes with multiple labels.


22
00:00:59.159 --> 00:01:01.495 line:-2
And lastly,
I'll talk about new Augmentation APIs


23
00:01:01.528 --> 00:01:04.164 line:-2
that we designed to help you
improve the quality of your model


24
00:01:04.198 --> 00:01:06.767 line:-1
when you're limited on training data.


25
00:01:06.800 --> 00:01:09.903 line:-2
Let's get started
with Text Classification improvements.


26
00:01:09.937 --> 00:01:11.939 line:-2
A text classifier is
a machine learning task


27
00:01:11.972 --> 00:01:15.576 line:-2
designed to recognize patterns
in natural language text.


28
00:01:15.609 --> 00:01:17.711 line:-1
To train such a model, all you need to do


29
00:01:17.744 --> 00:01:20.814 line:-2
is provide it with a table of text
and label pairs.


30
00:01:20.848 --> 00:01:24.985 line:-2
In this example, I have sports,
entertainment, and nature labels.


31
00:01:26.653 --> 00:01:28.522 line:-2
You can choose
the transfer-learning algorithm


32
00:01:28.555 --> 00:01:31.692 line:-2
that uses a pre-trained embedding model
as a feature extractor.


33
00:01:31.725 --> 00:01:34.394 line:-2
This year,
we designed a new embedding model


34
00:01:34.428 --> 00:01:37.464 line:-2
and trained it on billions
of labeled text examples.


35
00:01:37.497 --> 00:01:41.768 line:-2
It's a bidirectional encoder
representations from transformers model,


36
00:01:41.802 --> 00:01:43.837 line:-1
or BERT for short.


37
00:01:43.871 --> 00:01:45.939 line:-2
You can find the new option
in the Create ML app


38
00:01:45.973 --> 00:01:48.742 line:-2
in the model parameters section
of the Settings tab.


39
00:01:48.775 --> 00:01:52.779 line:-2
The BERT embedding model is multilingual,
which means your training data


40
00:01:52.813 --> 00:01:55.449 line:-2
can now contain
more than just one language.


41
00:01:55.482 --> 00:01:58.318 line:-2
On top of supporting
multilingual text classifiers,


42
00:01:58.352 --> 00:02:01.822 line:-2
BERT can also boost the accuracy
of your monolingual text classifiers.


43
00:02:02.589 --> 00:02:07.594 line:-2
You can leverage BERT on iOS 17,
iPadOS 17, and macOS Sonoma.


44
00:02:07,628 --> 00:02:10,531
We have a whole video
that covers all of the details.


45
00:02:10,564 --> 00:02:12,799
Make sure to watch
"Explore Natural Language


46
00:02:12,833 --> 00:02:16,103
multilingual models" to find out more.


47
00:02:16.136 --> 00:02:18.939 line:-2
Next, I want to talk about
how we use transfer-learning


48
00:02:18.972 --> 00:02:21.074 line:-1
in the image classification task.


49
00:02:21.108 --> 00:02:24.178 line:-2
The image classifier in Create ML
is designed to help you build models


50
00:02:24.211 --> 00:02:26.380 line:-2
to answer the question,
what is the best label


51
00:02:26.413 --> 00:02:29.183 line:-1
to describe the contents of my image?


52
00:02:29.216 --> 00:02:32.219 line:-2
Similar to the text classifier,
the image classifier leverages


53
00:02:32.252 --> 00:02:36.523 line:-2
a pre-trained model to extract
relevant information from images.


54
00:02:36.557 --> 00:02:39.793 line:-2
The latest version of the Apple Neural
Scene Analyzer is now available to you


55
00:02:39.826 --> 00:02:43.096 line:-2
for building state-of-the-art models
with very little training data.


56
00:02:43.130 --> 00:02:46.166 line:-2
Image understanding models in the OS
continue to evolve


57
00:02:46.200 --> 00:02:48.535 line:-1
to give you the best possible experience.


58
00:02:48.569 --> 00:02:50.571 line:-2
You can find out more
by checking out our article


59
00:02:50.604 --> 00:02:53.140 line:-1
on the machine learning research website.


60
00:02:53.173 --> 00:02:56.376 line:-2
In the Create ML app, you'll notice
a new feature extractor option


61
00:02:56.410 --> 00:02:58.812 line:-2
in the model parameters section
of the Settings tab.


62
00:03:00.514 --> 00:03:03.350 line:-2
The new feature extractor has
a smaller output embedding size


63
00:03:03.383 --> 00:03:05.652 line:-1
when compared to our previous version.


64
00:03:05.686 --> 00:03:08.488 line:-2
On top of general improvements,
this can boost the accuracy


65
00:03:08.522 --> 00:03:10.991 line:-2
of your classifier,
lead to faster training time,


66
00:03:11.024 --> 00:03:14.761 line:-2
and reduce the memory needed
to store the extracted features.


67
00:03:14.795 --> 00:03:17.264 line:-2
Now that I've covered
improvements in Create ML,


68
00:03:17.297 --> 00:03:19.833 line:-2
I want to talk about the new
multi-label image classifier.


69
00:03:19.867 --> 00:03:23.470 line:-2
Before I get there, recall that
single-label image classification


70
00:03:23.504 --> 00:03:27.107 line:-2
is designed to predict the best label
describing the contents of an image.


71
00:03:27.140 --> 00:03:32.379 line:-2
For example, you might describe
this image as dog or maybe outdoors.


72
00:03:32.412 --> 00:03:34.214 line:-1
But you need to pick one.


73
00:03:34.248 --> 00:03:36.617 line:-2
If you're interested in objects,
then you can use


74
00:03:36.650 --> 00:03:40.053 line:-2
the object detector
to locate objects within a scene.


75
00:03:40.087 --> 00:03:42.723 line:-2
For example, I've drawn a bounding box
around the dog


76
00:03:42.756 --> 00:03:44.925 line:-1
and another one around the ball.


77
00:03:44.958 --> 00:03:47.094 line:-2
Now, this is great,
but I'm also interested


78
00:03:47.127 --> 00:03:48.929 line:-1
in the scene that the objects are in.


79
00:03:48.962 --> 00:03:51.164 line:-2
I can't really draw a bounding box
to represent


80
00:03:51.198 --> 00:03:53.867 line:-1
that the dog is in a park or outdoors.


81
00:03:53.901 --> 00:03:56.737 line:-2
That's where the new
multi-label image classifier comes in.


82
00:03:56.770 --> 00:04:01.642 line:-2
It allows you to predict a set of objects,
attributes, or labels for your images.


83
00:04:01.675 --> 00:04:07.281 line:-2
For example this image contains
a dog, toy, grass, and park.


84
00:04:07.314 --> 00:04:09.716 line:-1
Let's go build one using Create ML.


85
00:04:09.750 --> 00:04:13.921 line:-2
As usual, the first thing I'll need to do
is collect some training data.


86
00:04:13.954 --> 00:04:17.157 line:-2
I decided to have a bit of fun
and build a classifier that detects


87
00:04:17.191 --> 00:04:20.060 line:-2
multiple succulent plants
in different scenes.


88
00:04:20.093 --> 00:04:23.363 line:-2
For example,
here I have an image of Haworthia,


89
00:04:23.397 --> 00:04:27.201 line:-1
Jade, and Aloe in pots on a window sill.


90
00:04:27.234 --> 00:04:31.572 line:-2
In the next image,
I have a person holding a cactus in a pot.


91
00:04:31.605 --> 00:04:35.442 line:-2
While collecting training images,
it's also okay to include some images


92
00:04:35.475 --> 00:04:39.780 line:-2
that only have a single label,
like a photo of just aloe.


93
00:04:39.813 --> 00:04:43.016 line:-2
You'll need to organize your annotations
in a JSON file.


94
00:04:43.050 --> 00:04:46.620 line:-2
All you need to do is annotate each file
with a set of annotations.


95
00:04:48.088 --> 00:04:53.093 line:-2
Now, let me give you a demo
of building a model in the Create ML app.


96
00:04:53.126 --> 00:04:57.464 line:-2
In the Create ML app, I'll select the new
multi-label image classifier template.


97
00:05:02.736 --> 00:05:05.572 line:-2
And create a project
named Succulent Classifier.


98
00:05:13.280 --> 00:05:16.617 line:-1
This takes me to the Settings tab.


99
00:05:16.650 --> 00:05:19.319 line:-2
To start, I'll drag in my training data,
which gives me a summary


100
00:05:19.353 --> 00:05:21.522 line:-2
of the number of classes
and training images.


101
00:05:26.560 --> 00:05:29.429 line:-2
I also have the option
to drag in validation data,


102
00:05:29.463 --> 00:05:33.367 line:-2
but I'll choose to randomly split
the training data for now.


103
00:05:33.400 --> 00:05:37.905 line:-2
I'll use the default number of iterations
and also leave out augmentations.


104
00:05:37.938 --> 00:05:40.774 line:-2
I'm done setting up my model,
so I'll go ahead and click Train.


105
00:05:45.812 --> 00:05:49.016 line:-2
This model takes only a few minutes
to train on my Mac.


106
00:05:49.049 --> 00:05:51.318 line:-2
Right away,
the model starts extracting features


107
00:05:51.351 --> 00:05:54.922 line:-2
using the new feature extractor
that I introduced earlier.


108
00:05:54.955 --> 00:05:58.725 line:-2
Once that's finished,
the app starts to train a classifier.


109
00:05:58.759 --> 00:06:01.728 line:-2
During the training process,
the app measures the quality of my model


110
00:06:01.762 --> 00:06:06.533 line:-2
by computing a mean-average
precision score, or MAP for short.


111
00:06:06.567 --> 00:06:09.036 line:-2
In general,
I want to maximize the MAP score


112
00:06:09.069 --> 00:06:11.638 line:-2
because that means my model has
both a higher precision


113
00:06:11.672 --> 00:06:15.475 line:-2
and a higher recall on average
for all of the labels in my dataset.


114
00:06:15.509 --> 00:06:19.112 line:-2
My model finished training
and converged early at 74 iterations,


115
00:06:19.146 --> 00:06:21.949 line:-2
with an MAP score of 97%
on the training set


116
00:06:21.982 --> 00:06:24.518 line:-1
and 93% on the validation set.


117
00:06:24.551 --> 00:06:27.588 line:-2
The next step is to evaluate
my model on test data.


118
00:06:30.023 --> 00:06:33.327 line:-2
I'll drag a folder in from my desktop
and click the Test button.


119
00:06:35.362 --> 00:06:37.865 line:-2
The test data should contain
the same set of class labels


120
00:06:37.898 --> 00:06:40.367 line:-1
that I used to train my model.


121
00:06:40.400 --> 00:06:42.970 line:-2
The app calculated
a few high-level statistics,


122
00:06:43.003 --> 00:06:45.472 line:-2
like the MAP Score
and which class labels have


123
00:06:45.506 --> 00:06:48.575 line:-2
the highest and lowest
precision and recall.


124
00:06:48.609 --> 00:06:51.411 line:-1
Let's focus on the Metrics tab.


125
00:06:51.445 --> 00:06:53.914 line:-2
The app calculates metrics
for each class label,


126
00:06:53.947 --> 00:06:57.284 line:-2
like False Positives,
False Negatives, Precision,


127
00:06:57.317 --> 00:07:00.120 line:-1
Recall, and Confidence Threshold.


128
00:07:00.153 --> 00:07:03.357 line:-2
When making predictions with my model,
a prediction is correct


129
00:07:03.390 --> 00:07:06.693 line:-2
if the confidence is above the threshold
for a given class label.


130
00:07:06.727 --> 00:07:09.429 line:-2
Let's explore the images
that the model predicted


131
00:07:09.463 --> 00:07:13.267 line:-1
above the confidence threshold for aloe.


132
00:07:13.300 --> 00:07:15.302 line:-1
Now I'll click on an example.


133
00:07:17.704 --> 00:07:21.942 line:-2
The model predicted that this image
contains aloe with a 90% confidence,


134
00:07:21.975 --> 00:07:25.646 line:-2
which is above the aloe
confidence threshold of 40%.


135
00:07:25.679 --> 00:07:27.981 line:-2
For other labels,
the model predicted a confidence


136
00:07:28.015 --> 00:07:29.883 line:-1
below their respective thresholds.


137
00:07:29.917 --> 00:07:32.586 line:-2
In other words,
the model did not predict them.


138
00:07:32.619 --> 00:07:36.323 line:-2
Next, I want to explore images
that my model did not predict aloe


139
00:07:36.356 --> 00:07:38.959 line:-1
but are labeled as aloe.


140
00:07:38.992 --> 00:07:41.995 line:-2
I can do that by selecting
the False Negatives result type.


141
00:07:43.530 --> 00:07:47.034 line:-2
This image is interesting.
Let's explore further.


142
00:07:47.067 --> 00:07:50.337 line:-2
Here, the aloe is behind the barrel cactus
and the moon cactus,


143
00:07:50.370 --> 00:07:53.207 line:-2
so the model has
a hard time predicting aloe.


144
00:07:53.240 --> 00:07:57.344 line:-2
But the good news is that the model
predicts two other labels correctly.


145
00:07:57.377 --> 00:07:59.713 line:-1
Next, I'll head over to the Preview tab.


146
00:08:01.348 --> 00:08:03.383 line:-2
This is where I can preview
my model predictions


147
00:08:03.417 --> 00:08:06.053 line:-1
on images that I haven't labeled yet.


148
00:08:06.086 --> 00:08:08.222 line:-2
I have a succulent arrangement
that I planted myself


149
00:08:08.255 --> 00:08:10.257 line:-1
that I think would be fun to try out.


150
00:08:12.593 --> 00:08:13.594 line:-1
Nailed it.


151
00:08:13.627 --> 00:08:15.896 line:-2
My model correctly predicted
my moon cactus,


152
00:08:15.929 --> 00:08:18.966 line:-2
bunny ears cactus,
and barrel cactus in my kitchen.


153
00:08:18.999 --> 00:08:21.301 line:-2
I'm pretty happy
with the quality of my model,


154
00:08:21.335 --> 00:08:24.938 line:-2
but I'll definitely continue experimenting
to understand my model's limitations


155
00:08:24.972 --> 00:08:28.709 line:-2
as well as adding more succulents
and scenes to my dataset.


156
00:08:28.742 --> 00:08:31.445 line:-1
For now, let's move on.


157
00:08:31.478 --> 00:08:33.881 line:-1
From the Output tab,


158
00:08:33.914 --> 00:08:36.250 line:-2
I can save
the model that I trained to disk.


159
00:08:38.519 --> 00:08:42.022 line:-2
Let's go through some code that you'll
need to write to create predictions.


160
00:08:42.055 --> 00:08:46.260 line:-2
The first step is to create a Vision model
from the compiled Core ML model.


161
00:08:46.293 --> 00:08:49.763 line:-2
Then using the vision framework,
I'll create an image request handler


162
00:08:49.796 --> 00:08:52.699 line:-2
with a source image
and then perform a request.


163
00:08:54.034 --> 00:08:57.004 line:-2
Lastly, I can retrieve
the classification observations


164
00:08:57.037 --> 00:09:01.074 line:-2
and filter them using a precision
and recall value that I'm interested in.


165
00:09:01.108 --> 00:09:04.044 line:-2
To find out more about how to choose
a precision and recall value


166
00:09:04,077 --> 00:09:06,847
that work for your use case,
make sure to watch the video


167
00:09:06,880 --> 00:09:08,682
from WWDC 2019,


168
00:09:08,715 --> 00:09:11,518
"Understanding Images
in Vision Framework."


169
00:09:11.552 --> 00:09:15.022 line:-2
Before I move on,
I want to take a moment and mention that,


170
00:09:15.055 --> 00:09:18.192 line:-2
similar to our image classifier
and multi-label image classifier,


171
00:09:18.225 --> 00:09:20.060 line:-1
we've enhanced the evaluation tab


172
00:09:20.093 --> 00:09:22.462 line:-2
with an explore option
for object detection.


173
00:09:22.496 --> 00:09:24.498 line:-1
Make sure to check it out.


174
00:09:24.531 --> 00:09:27.167 line:-2
I want to shift your focus
to my last topic,


175
00:09:27.201 --> 00:09:31.104 line:-2
training machine learning models
on limited data using augmentations.


176
00:09:31.138 --> 00:09:33.106 line:-1
To get a model that generalizes well,


177
00:09:33.140 --> 00:09:36.243 line:-2
the images in your training set
should have a variety of characteristics


178
00:09:36.276 --> 00:09:39.446 line:-2
like different lighting conditions,
orientations, and backgrounds.


179
00:09:39.479 --> 00:09:43.016 line:-2
But capturing and labeling
training images in different situations


180
00:09:43.050 --> 00:09:44.785 line:-1
can be time-consuming.


181
00:09:44.818 --> 00:09:47.855 line:-2
Data augmentation is a technique
that generates new training examples


182
00:09:47.888 --> 00:09:51.124 line:-2
from your existing ones
by applying transformations.


183
00:09:51.158 --> 00:09:53.293 line:-2
In the case of images,
the transformations can be


184
00:09:53.327 --> 00:09:58.498 line:-2
horizontal or vertical flipping,
cropping, or contrast, just to name a few.


185
00:09:58.532 --> 00:10:01.468 line:-2
In this example,
I start with an image of a succulent


186
00:10:01.502 --> 00:10:03.837 line:-1
and generate four variations.


187
00:10:03.871 --> 00:10:08.342 line:-2
And transformations can be combined,
like flipping and increasing the contrast.


188
00:10:08.375 --> 00:10:10.844 line:-2
Augmentations can boost
the quality of your model,


189
00:10:10.878 --> 00:10:13.647 line:-2
especially when you have
a small training data set.


190
00:10:13.680 --> 00:10:16.149 line:-2
You can use them to improve
your model's generalization


191
00:10:16.183 --> 00:10:18.118 line:-2
because it prevents your model
from learning attributes


192
00:10:18.151 --> 00:10:21.154 line:-2
like the exact location
of an object in a scene.


193
00:10:21.188 --> 00:10:24.525 line:-2
However, it's important to consider
that training is usually slower


194
00:10:24.558 --> 00:10:28.629 line:-2
because feature extraction happens
at each training iteration.


195
00:10:28,662 --> 00:10:30,831
If you haven't already,
make sure to watch our videos


196
00:10:30,864 --> 00:10:35,002
from WWDC 2022,
where we introduced Create ML Components.


197
00:10:35,035 --> 00:10:37,971
We designed the framework to help you
build custom machine learning models


198
00:10:38,005 --> 00:10:41,441
using components
like transformers and estimators.


199
00:10:41,475 --> 00:10:44,411
This year, we added new
Augmentation APIs that you can use


200
00:10:44,444 --> 00:10:47,314
to design your own
custom augmentation pipelines.


201
00:10:47.347 --> 00:10:51.318 line:-2
If you have experience with SwiftUI,
this may be familiar to you.


202
00:10:51.351 --> 00:10:53.954 line:-1
The first step is to create an augmenter.


203
00:10:53.987 --> 00:10:58.525 line:-2
Similar to SwiftUI,
the augmenter uses result builders.


204
00:10:58.559 --> 00:11:02.996 line:-2
In the body of the augmenter, you can add
transformations to augment your data.


205
00:11:03.030 --> 00:11:06.667 line:-2
Since the augmenter is generic,
your data can be labeled images,


206
00:11:06.700 --> 00:11:09.369 line:-1
labeled sounds, or even something else.


207
00:11:09.403 --> 00:11:12.206 line:-2
The important part is that
the input and output types


208
00:11:12.239 --> 00:11:14.675 line:-1
of each transformation need to match.


209
00:11:14.708 --> 00:11:18.212 line:-2
For example, taking an image
and producing an image.


210
00:11:18.245 --> 00:11:21.148 line:-2
I want to augment my images
by flipping them horizontally


211
00:11:21.181 --> 00:11:23.517 line:-1
with a 50% probability.


212
00:11:23.550 --> 00:11:26.186 line:-2
I'll start by adding
ApplyRandomly to the Augmenter.


213
00:11:26,220 --> 00:11:29,723
This applies a transformation
with a given probability.


214
00:11:29,756 --> 00:11:34,328
Then I'll add a horizontal
flipping transformer to its body.


215
00:11:34,361 --> 00:11:36,597
Okay, now that I have my augmenter,


216
00:11:36,630 --> 00:11:38,799
I can use it to create augmentations


217
00:11:38.832 --> 00:11:41.235 line:-1
by calling the applied method.


218
00:11:41.268 --> 00:11:44.338 line:-2
When you design augmentations,
it's important to carefully consider


219
00:11:44.371 --> 00:11:45.672 line:-1
the nature of your data.


220
00:11:46,540 --> 00:11:49,576
It's unlikely that you'll encounter
an upside down succulent.


221
00:11:49,610 --> 00:11:54,081
So it doesn't make sense to apply
a vertical flip augmentation in this case.


222
00:11:54,114 --> 00:11:58,118
Or imagine for a moment that you wanted
to classify traffic signs instead.


223
00:11:58,151 --> 00:11:59,753
When you apply a flip augmentation,


224
00:11:59,786 --> 00:12:02,956
the label may no longer
correctly describe your image.


225
00:12:02,990 --> 00:12:05,492
So remember to consider
the nature of your data


226
00:12:05,526 --> 00:12:07,828
before designing a custom augmentation.


227
00:12:07.861 --> 00:12:12.432 line:-2
All right, the next step is to add
more transformations to the augmenter.


228
00:12:13.433 --> 00:12:15.669 line:-1
This time, I'll randomly rotate my images


229
00:12:15.702 --> 00:12:18.739 line:-2
using the
UniformRandomFloatingPointParameter.


230
00:12:18.772 --> 00:12:22.676 line:-2
This generates a random angle
each time that I apply the augmentation.


231
00:12:22.709 --> 00:12:26.246 line:-1
And lastly, I'll randomly crop the images.


232
00:12:26.280 --> 00:12:30.450 line:-2
Note that each transformation
in the augmenter is applied in sequence.


233
00:12:30.484 --> 00:12:33.187 line:-1
So first, my image is randomly flipped.


234
00:12:33.220 --> 00:12:37.691 line:-2
The result is randomly rotated
and then randomly cropped.


235
00:12:37.724 --> 00:12:41.628 line:-2
I've only scratched the surface
of what you can do with the Augmenter.


236
00:12:41.662 --> 00:12:45.632 line:-2
Here are some of the components
that we provide to get you started.


237
00:12:45.666 --> 00:12:47.935 line:-1
But what if you want to go even further?


238
00:12:47.968 --> 00:12:51.638 line:-2
Let's go through an example of how
you can build custom transformations


239
00:12:51.672 --> 00:12:53.774 line:-1
and use them to augment your images.


240
00:12:54.808 --> 00:12:57.744 line:-2
In order to build a robust classifier,
it's important to capture


241
00:12:57.778 --> 00:13:00.814 line:-2
your training images
in different scenes and environments.


242
00:13:00.848 --> 00:13:03.317 line:-2
In this example,
I'll create a custom augmentation


243
00:13:03.350 --> 00:13:06.587 line:-2
that places a succulent
in a random location in a random scene.


244
00:13:07.688 --> 00:13:10.858 line:-2
I'll start by defining
a RandomImageBackground.


245
00:13:10.891 --> 00:13:14.094 line:-2
It conforms to a new protocol,
RandomTransformer,


246
00:13:14.127 --> 00:13:18.832 line:-2
which is similar to a transformer
but takes a random number generator.


247
00:13:18.866 --> 00:13:21.802 line:-2
Since I want my augmentations
to place succulents randomly


248
00:13:21.835 --> 00:13:24.538 line:-2
with different background scenes,
I'll create an initializer


249
00:13:24.571 --> 00:13:27.040 line:-1
that takes background images.


250
00:13:27.074 --> 00:13:29.176 line:-2
To conform
to the RandomTransformer protocol,


251
00:13:29.209 --> 00:13:31.845 line:-1
I'll need to implement the applied method.


252
00:13:31.879 --> 00:13:33.480 line:-1
When applying my augmentation,


253
00:13:33.514 --> 00:13:37.084 line:-2
the first step is
to randomly select a background.


254
00:13:37.117 --> 00:13:40.053 line:-2
Then, I'll choose a random location
in the background image


255
00:13:40.087 --> 00:13:44.391 line:-2
where I'd like to place my input image,
taking care not to crop it.


256
00:13:44.424 --> 00:13:48.395 line:-2
Next, I'll translate my input image
to the random location.


257
00:13:48.428 --> 00:13:53.567 line:-2
And finally, I'll place the input image
over the randomly selected background.


258
00:13:53.600 --> 00:13:58.205 line:-2
I'll add my new custom augmentation
to the end so that the flipping, rotating,


259
00:13:58.238 --> 00:14:02.476 line:-2
and cropping happen before I place
the succulent in different backgrounds.


260
00:14:02.509 --> 00:14:04.778 line:-1
And this is my final augmenter.


261
00:14:04.811 --> 00:14:06.847 line:-1
Using my augmenter, I can generate


262
00:14:06.880 --> 00:14:10.317 line:-2
some really interesting images
for training a classifier.


263
00:14:10.350 --> 00:14:11.952 line:-1
That's the next step.


264
00:14:11,985 --> 00:14:14,188
When using augmentations
it makes more sense


265
00:14:14,221 --> 00:14:17,357
to train progressively
using the update method.


266
00:14:17,391 --> 00:14:19,459
With that in mind,
let's go through an example


267
00:14:19,493 --> 00:14:21,461
of training using augmentations.


268
00:14:21.495 --> 00:14:24.598 line:-2
And I won't hold back:
I'll also incorporate batching,


269
00:14:24.631 --> 00:14:26.867 line:-1
randomization, and early stopping.


270
00:14:26.900 --> 00:14:31.004 line:-2
I'll start by creating
an empty image classifier model.


271
00:14:31.038 --> 00:14:33.507 line:-1
Then I'll create a training loop.


272
00:14:33.540 --> 00:14:37.778 line:-2
In the training loop, the first step is
to shuffle and augment my training images.


273
00:14:37.811 --> 00:14:39.913 line:-1
You'll want to shuffle before augmenting


274
00:14:39.947 --> 00:14:44.318 line:-2
so that on each iteration,
the batches contain different images.


275
00:14:44.351 --> 00:14:46.954 line:-2
The result of the augmenter
is an async sequence,


276
00:14:46.987 --> 00:14:50.657 line:-2
which means
that transformations happen lazily.


277
00:14:50.691 --> 00:14:53.560 line:-2
Using the batches method,
I can group the async sequence


278
00:14:53.594 --> 00:14:56.196 line:-1
of augmentations into groups.


279
00:14:56.230 --> 00:15:00.167 line:-2
In this case,
I've used a batch size of 16.


280
00:15:00.200 --> 00:15:04.771 line:-2
Finally, I'll update my model
with each batch of augmented images.


281
00:15:04.805 --> 00:15:07.007 line:-2
This is why using the update method
is a good choice


282
00:15:07.040 --> 00:15:08.709 line:-1
when doing data augmentation.


283
00:15:08.742 --> 00:15:11.979 line:-2
On each iteration,
you get a new set of images.


284
00:15:12.012 --> 00:15:14.982 line:-2
Depending on your augmenter,
the update method is unlikely


285
00:15:15.015 --> 00:15:18.051 line:-2
to encounter the exact same image
more than once.


286
00:15:18.085 --> 00:15:21.321 line:-2
This encourages the model
to generalize instead of memorize.


287
00:15:22.189 --> 00:15:25.859 line:-2
I selected 100 iterations,
but ideally you should stop training


288
00:15:25.893 --> 00:15:28.795 line:-2
when the validation accuracy
stops improving.


289
00:15:28.829 --> 00:15:32.132 line:-2
In this example, the training accuracy
continues to improve,


290
00:15:32.165 --> 00:15:36.370 line:-2
but notice that the validation accuracy
starts to decrease after a few iterations.


291
00:15:37.738 --> 00:15:40.073 line:-2
This means that the model is
memorizing the training data,


292
00:15:40.107 --> 00:15:42.342 line:-1
making it less generalizable to new data


293
00:15:42.376 --> 00:15:45.279 line:-2
and performing worse
on validation and test data.


294
00:15:45.312 --> 00:15:48.282 line:-2
Continuing training
after this is detrimental.


295
00:15:48.315 --> 00:15:52.519 line:-2
As a final example, I'll add
early stopping to the training loop.


296
00:15:52.553 --> 00:15:56.123 line:-2
After the update step,
you can compute validation metrics.


297
00:15:56.156 --> 00:15:58.792 line:-2
I'm using validation accuracy
to stop training early,


298
00:15:58.825 --> 00:16:01.161 line:-1
which works for classifier models.


299
00:16:01.195 --> 00:16:03.030 line:-1
But that is really up to you.


300
00:16:03.063 --> 00:16:04.398 line:-1
You can use validation loss


301
00:16:04.431 --> 00:16:08.168 line:-2
or even design your own metric
to assess the quality of your model.


302
00:16:08.202 --> 00:16:10.404 line:-1
Then, I'll break out of the training loop


303
00:16:10.437 --> 00:16:14.274 line:-2
when the accuracy hasn't improved
for the last five iterations.


304
00:16:14.308 --> 00:16:17.811 line:-2
And that concludes
my augmented image classifier.


305
00:16:17.845 --> 00:16:20.681 line:-2
In this video,
I introduced new state-of-the-art models


306
00:16:20.714 --> 00:16:22.916 line:-2
shipped in the OS
that you can use to build


307
00:16:22.950 --> 00:16:26.553 line:-2
great machine learning experiences
in your apps.


308
00:16:26.587 --> 00:16:28.789 line:-2
Then I gave an example
of how you can build models


309
00:16:28.822 --> 00:16:32.860 line:-2
for understanding scenes using
our new multi-label image classifier.


310
00:16:32.893 --> 00:16:35.495 line:-2
Finally, I went into detail
about how you can build


311
00:16:35.529 --> 00:16:39.032 line:-2
your own custom augmentations
to improve your model quality.


312
00:16:39.066 --> 00:16:40.400 line:-1
And that's a wrap.


313
00:16:40.434 --> 00:16:43.437 line:-2
It's time for you to enhance your apps
with Create ML.

