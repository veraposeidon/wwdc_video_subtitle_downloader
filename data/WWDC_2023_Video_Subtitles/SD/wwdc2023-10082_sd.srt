2
00:00:00.133 --> 00:00:02.903 line:-1 position:50%
♪ Mellow instrumental hip-hop ♪


3
00:00:02,903 --> 00:00:10,510 position:90% size:2% line:0
♪


4
00:00:10.510 --> 00:00:12.412 line:-1 position:50%
Ryan Taylor: Hello!
My name is Ryan.


5
00:00:12.412 --> 00:00:13.647 line:-1 position:50%
Conner Brooks: And I'm Conner.


6
00:00:13,647 --> 00:00:15,649 line:-1
Ryan: In this session, we are
going to introduce you


7
00:00:15,649 --> 00:00:18,285 line:-1
to ARKit for spatial computing.


8
00:00:18,285 --> 00:00:20,153 line:-1
We will discuss
the critical role that it plays


9
00:00:20.153 --> 00:00:22.322 line:-1 position:50%
on this new platform
and how you can leverage it


10
00:00:22,322 --> 00:00:25,259 line:-1
to build the next
generation of apps.


11
00:00:25,259 --> 00:00:28,228 line:-1
ARKit uses sophisticated
computer vision algorithms


12
00:00:28,228 --> 00:00:30,530 line:-1
to construct an understanding
of the world around you,


13
00:00:30,530 --> 00:00:32,532 line:-1
as well as your movements.


14
00:00:32.532 --> 00:00:35.535 line:-1 position:50%
We first introduced
this technology in iOS 11


15
00:00:35,535 --> 00:00:36,803 line:-1
as a way for developers
to create


16
00:00:36.803 --> 00:00:38.939 line:-1 position:50%
amazing augmented reality
experiences


17
00:00:38.939 --> 00:00:42.342 line:-1 position:50%
that you can use
in the palm of your hand.


18
00:00:42.342 --> 00:00:44.511 line:-1 position:50%
On this platform,
ARKit has matured


19
00:00:44.511 --> 00:00:46.613 line:-1 position:50%
into a full-blown
system service,


20
00:00:46,613 --> 00:00:50,651 line:-1
rebuilt from the ground up with
a new real-time foundation.


21
00:00:50.651 --> 00:00:52.786 line:-1 position:50%
ARKit is deeply woven
into the fabric


22
00:00:52.786 --> 00:00:56.156 line:-1 position:50%
of the entire operating system,
powering everything


23
00:00:56,156 --> 00:01:00,327 line:-1
from interacting with a window,
to playing an immersive game.


24
00:01:00.327 --> 00:01:02.829 line:-1 position:50%
As part of this journey,
we have also given our API


25
00:01:02,829 --> 00:01:04,631 line:-1
a complete overhaul.


26
00:01:04.631 --> 00:01:06.400 line:-1 position:50%
The new design is
a result of everything


27
00:01:06.400 --> 00:01:08.769 line:-1 position:50%
that we learned on iOS,
plus the unique needs


28
00:01:08,769 --> 00:01:10,070 line:-1
of spatial computing,


29
00:01:10,070 --> 00:01:12,673 line:-1
and we think
you are going to love it.


30
00:01:12,673 --> 00:01:15,275 line:-1
ARKit provides a variety
of powerful features


31
00:01:15.275 --> 00:01:18.312 line:-1 position:50%
that you can combine
to do incredible things,


32
00:01:18,312 --> 00:01:21,248 line:-1
such as place virtual content
on a table.


33
00:01:21,248 --> 00:01:22,983 line:-1
You can reach out
and touch the content,


34
00:01:22,983 --> 00:01:25,285 line:-1
as if it is really there,
and then watch


35
00:01:25.285 --> 00:01:27.721 line:-1 position:50%
as the content interacts
with the real world.


36
00:01:27,721 --> 00:01:30,691 line:-1
It is truly
a magical experience.


37
00:01:30,691 --> 00:01:33,026 line:-1
Now that you have seen a glimpse
of what can be accomplished


38
00:01:33.026 --> 00:01:35.395 line:-1 position:50%
using ARKit
on this new platform,


39
00:01:35,395 --> 00:01:38,065 line:-1
let me walk you
through our agenda.


40
00:01:38,065 --> 00:01:40,434 line:-1
We will begin with an overview
of the fundamental concepts


41
00:01:40.434 --> 00:01:43.637 line:-1 position:50%
and building blocks
that make up our API.


42
00:01:43.637 --> 00:01:46.006 line:-1 position:50%
Next, we will dive
into world tracking,


43
00:01:46.006 --> 00:01:48.475 line:-1 position:50%
which is essential
for placing virtual content


44
00:01:48,475 --> 00:01:50,677 line:-1
relative to the real world.


45
00:01:50.677 --> 00:01:53.580 line:-1 position:50%
Then, we will explore our
scene understanding features,


46
00:01:53,580 --> 00:01:57,351 line:-1
which provide useful information
about your surroundings.


47
00:01:57,351 --> 00:02:00,253 line:-1
After that, we will introduce
you to our newest feature,


48
00:02:00,253 --> 00:02:03,623 line:-1
hand tracking, an exciting new
addition that you can leverage


49
00:02:03,623 --> 00:02:06,560 line:-1
for placing virtual content
relative to your hands


50
00:02:06,560 --> 00:02:10,063 line:-1
or building other types
of bespoke interactions.


51
00:02:10,063 --> 00:02:13,100 line:-1
And lastly, we will come
full circle and take a look


52
00:02:13,100 --> 00:02:15,669 line:-1
at a practical application
of some of these features


53
00:02:15,669 --> 00:02:20,007 line:-1
by examining code from the video
that we showed you a moment ago.


54
00:02:20,007 --> 00:02:22,909 line:-1
All right, let's get started!


55
00:02:22.909 --> 00:02:25.479 line:-1 position:50%
Our new API has been
meticulously crafted


56
00:02:25.479 --> 00:02:31.718 line:-1 position:50%
in two invigorating flavors,
modern Swift and classic C.


57
00:02:31.718 --> 00:02:35.155 line:-1 position:50%
All ARKit features are
now provided à la carte.


58
00:02:35,155 --> 00:02:38,158 line:-1
We wanted developers to have
as much flexibility as possible,


59
00:02:38,158 --> 00:02:40,260 line:-1
so that you can simply pick
and choose what you need


60
00:02:40,260 --> 00:02:43,096 line:-1
to build your experience.


61
00:02:43,096 --> 00:02:44,965 line:-1
Access to ARKit data
has been designed


62
00:02:44,965 --> 00:02:47,401 line:-1
with a privacy-first approach.


63
00:02:47.401 --> 00:02:48.869 line:-1 position:50%
We have put safeguards
into place


64
00:02:48.869 --> 00:02:50.570 line:-1 position:50%
to protect people's privacy,


65
00:02:50.570 --> 00:02:54.508 line:-1 position:50%
while also maintaining
simplicity for developers.


66
00:02:54.508 --> 00:02:58.311 line:-1 position:50%
The API consists of three
fundamental building blocks:


67
00:02:58,311 --> 00:02:59,813 line:-1
sessions,


68
00:02:59,813 --> 00:03:01,214 line:-1
data providers,


69
00:03:01,214 --> 00:03:03,517 line:-1
and anchors.


70
00:03:03,517 --> 00:03:05,452 line:-1
Let's begin with anchors
and then work our way


71
00:03:05.452 --> 00:03:08.188 line:-1 position:50%
back up to sessions.


72
00:03:08,188 --> 00:03:10,624 line:-1
An anchor represents
a position and orientation


73
00:03:10,624 --> 00:03:12,759 line:-1
in the real world.


74
00:03:12.759 --> 00:03:14.928 line:-1 position:50%
All anchors include
a unique identifier,


75
00:03:14.928 --> 00:03:17.664 line:-1 position:50%
as well as a transform.


76
00:03:17,664 --> 00:03:20,701 line:-1
Some types of anchors
are also trackable.


77
00:03:20,701 --> 00:03:23,136 line:-1
When a trackable anchor
is not being tracked,


78
00:03:23,136 --> 00:03:24,738 line:-1
you should hide
any virtual content


79
00:03:24.738 --> 00:03:28.075 line:-1 position:50%
that you have anchored with it.


80
00:03:28.075 --> 00:03:32.179 line:-1 position:50%
A data provider represents
an individual ARKit feature.


81
00:03:32,179 --> 00:03:33,914 line:-1
Data providers
allow you to poll for


82
00:03:33.914 --> 00:03:37.617 line:-1 position:50%
or observe data updates,
such as anchor changes.


83
00:03:37,617 --> 00:03:39,152 line:-1
Different types
of data providers


84
00:03:39.152 --> 00:03:42.389 line:-1 position:50%
provide different types of data.


85
00:03:42.389 --> 00:03:45.492 line:-1 position:50%
A session represents
a combined set of ARKit features


86
00:03:45,492 --> 00:03:46,660 line:-1
that you would like
to use together


87
00:03:46,660 --> 00:03:48,995 line:-1
for a particular experience.


88
00:03:48,995 --> 00:03:53,266 line:-1
You run a session by providing
it with a set of data providers.


89
00:03:53.266 --> 00:03:54.835 line:-1 position:50%
Once the session is running,


90
00:03:54.835 --> 00:03:58.205 line:-1 position:50%
the data providers
will begin receiving data.


91
00:03:58,205 --> 00:04:01,241 line:-1
Updates arrive asynchronously
and at different frequencies,


92
00:04:01.241 --> 00:04:03.577 line:-1 position:50%
depending on the type of data.


93
00:04:03.577 --> 00:04:05.479 line:-1 position:50%
Let's move on now
and talk about privacy


94
00:04:05,479 --> 00:04:08,748 line:-1
and how your app
gets access to ARKit data.


95
00:04:08,748 --> 00:04:11,284 line:-1
Privacy is a fundamental
human right.


96
00:04:11,284 --> 00:04:14,087 line:-1
It is also one
of our core values.


97
00:04:14,087 --> 00:04:16,857 line:-1
ARKit's architecture and API
have been thoughtfully designed


98
00:04:16,857 --> 00:04:18,558 line:-1
to protect people's privacy.


99
00:04:18,558 --> 00:04:20,594 line:-1
In order for ARKit to construct
an understanding


100
00:04:20,594 --> 00:04:23,196 line:-1
of the world around you,
the device has many cameras


101
00:04:23,196 --> 00:04:25,198 line:-1
and other types of sensors.


102
00:04:25,198 --> 00:04:27,968 line:-1
Data from these sensors,
such as camera frames,


103
00:04:27.968 --> 00:04:30.170 line:-1 position:50%
is never sent to client space.


104
00:04:30,170 --> 00:04:32,873 line:-1
Instead, sensor data is sent
to ARKit's daemon


105
00:04:32.873 --> 00:04:36.076 line:-1 position:50%
for secure processing
by our algorithms.


106
00:04:36,076 --> 00:04:38,545 line:-1
The resulting data that is
produced by these algorithms


107
00:04:38,545 --> 00:04:41,114 line:-1
is then carefully curated
before being forwarded


108
00:04:41,114 --> 00:04:43,183 line:-1
to any clients that are
requesting the data,


109
00:04:43,183 --> 00:04:45,652 line:-1
such as your app.


110
00:04:45,652 --> 00:04:49,322 line:-1
There are a few prerequisites
to accessing ARKit data.


111
00:04:49,322 --> 00:04:52,926 line:-1
First, your app must enter
a Full Space.


112
00:04:52,926 --> 00:04:57,130 line:-1
ARKit does not send data to apps
that are in the Shared Space.


113
00:04:57,130 --> 00:05:01,768 line:0
Second, some types of ARKit data
require permission to access.


114
00:05:01,768 --> 00:05:03,737 line:0
If the person does
not grant permission,


115
00:05:03,737 --> 00:05:07,774 line:0
then we will not send
that type of data to your app.


116
00:05:07,774 --> 00:05:10,177 line:-1
To facilitate this,
ARKit provides


117
00:05:10,177 --> 00:05:15,115 line:-1
a convenient authorization API
for handling permission.


118
00:05:15,115 --> 00:05:18,018 line:-1
Using your session,
you can request authorization


119
00:05:18,018 --> 00:05:21,388 line:-1
for the types of data
that you would like to access.


120
00:05:21,388 --> 00:05:24,324 line:-1
If you do not do this,
ARKit will automatically prompt


121
00:05:24,324 --> 00:05:26,793 line:-1
the person for permission
when you run the session,


122
00:05:26,793 --> 00:05:29,029 line:-1
if necessary.


123
00:05:29,029 --> 00:05:32,933 line:-1
Here, we are requesting access
to hand tracking data.


124
00:05:32.933 --> 00:05:34.835 line:-1 position:50%
You can batch all of
the authorization types


125
00:05:34,835 --> 00:05:39,039 line:-1
that you need together
in a single request.


126
00:05:39,039 --> 00:05:41,107 line:-1
Once we have
the authorization results,


127
00:05:41.107 --> 00:05:43.210 line:-1 position:50%
we iterate over them
and check the status


128
00:05:43,210 --> 00:05:46,213 line:-1
for each authorization type.


129
00:05:46,213 --> 00:05:48,515 line:-1
If the person has
granted permission,


130
00:05:48,515 --> 00:05:51,818 line:-1
the status will be allowed.


131
00:05:51.818 --> 00:05:53.687 line:-1 position:50%
Attempting to run a session
with a data provider


132
00:05:53,687 --> 00:05:56,690 line:-1
that provides data that
the person has denied access to


133
00:05:56.690 --> 00:05:59.192 line:-1 position:50%
will result
in the session failing.


134
00:05:59,192 --> 00:06:01,628 line:-1
Now, let's take a closer look
at each of the features


135
00:06:01.628 --> 00:06:03.864 line:-1 position:50%
that ARKit supports
on this platform,


136
00:06:03.864 --> 00:06:06.499 line:-1 position:50%
starting with world tracking.


137
00:06:06.499 --> 00:06:08.768 line:-1 position:50%
World tracking allows you
to anchor virtual content


138
00:06:08,768 --> 00:06:10,604 line:-1
in the real world.


139
00:06:10.604 --> 00:06:12.439 line:-1 position:50%
ARKit tracks
the device's movement


140
00:06:12.439 --> 00:06:15.442 line:-1 position:50%
in six degrees of freedom
and updates each anchor,


141
00:06:15,442 --> 00:06:16,843 line:-1
so that they stay
in the same place


142
00:06:16.843 --> 00:06:19.212 line:-1 position:50%
relative to your surroundings.


143
00:06:19.212 --> 00:06:21.448 line:-1 position:50%
The type of DataProvider
that world tracking uses


144
00:06:21,448 --> 00:06:23,283 line:-1
is called WorldTrackingProvider,


145
00:06:23,283 --> 00:06:26,553 line:-1
and it gives you
several important capabilities.


146
00:06:26.553 --> 00:06:28.488 line:-1 position:50%
It allows you to add
WorldAnchors,


147
00:06:28.488 --> 00:06:30.590 line:-1 position:50%
which ARKit will then update
to remain fixed


148
00:06:30.590 --> 00:06:32.192 line:-1 position:50%
relative to people's
surroundings


149
00:06:32.192 --> 00:06:34.527 line:-1 position:50%
as the device moves around.


150
00:06:34,527 --> 00:06:36,329 line:-1
WorldAnchors are
an essential tool


151
00:06:36.329 --> 00:06:38.798 line:-1 position:50%
for virtual content placement.


152
00:06:38,798 --> 00:06:41,368 line:-1
Any WorldAnchors that you add
are automatically persisted


153
00:06:41,368 --> 00:06:43,770 line:-1
across app launches and reboots.


154
00:06:43,770 --> 00:06:45,872 line:-1
If this behavior is undesirable
for the experience


155
00:06:45,872 --> 00:06:47,941 line:-1
that you are building,
you can simply remove


156
00:06:47,941 --> 00:06:49,809 line:-1
the anchors when
you are done with them,


157
00:06:49,809 --> 00:06:52,412 line:-1
and they will no longer
be persisted.


158
00:06:52.412 --> 00:06:54.381 line:-1 position:50%
It is important to note
that there are some cases


159
00:06:54.381 --> 00:06:57.484 line:-1 position:50%
where persistence
will not be available.


160
00:06:57.484 --> 00:06:59.052 line:-1 position:50%
You can also use
a WorldTrackingProvider


161
00:06:59,052 --> 00:07:01,922 line:-1
to get the device's pose
relative to the app origin,


162
00:07:01,922 --> 00:07:04,157 line:-1
which is necessary if you are
doing your own rendering


163
00:07:04,157 --> 00:07:06,126 line:-1
using Metal.


164
00:07:06,126 --> 00:07:09,162 line:-1
Let's begin by taking a closer
look at what a WorldAnchor is


165
00:07:09.162 --> 00:07:11.665 line:-1 position:50%
and why you would
want to use one.


166
00:07:11.665 --> 00:07:13.833 line:-1 position:50%
A WorldAnchor is
a TrackableAnchor


167
00:07:13.833 --> 00:07:16.436 line:-1 position:50%
with an initializer
that takes a transform,


168
00:07:16,436 --> 00:07:17,904 line:-1
which is the position
and orientation


169
00:07:17.904 --> 00:07:19.706 line:-1 position:50%
that you would like
to place the anchor at,


170
00:07:19,706 --> 00:07:22,142 line:-1
relative to the app's origin.


171
00:07:22.142 --> 00:07:24.411 line:-1 position:50%
We have prepared an example
to help visualize the difference


172
00:07:24,411 --> 00:07:26,746 line:-1
between virtual content
that is not anchored


173
00:07:26.746 --> 00:07:29.249 line:-1 position:50%
versus content that is anchored.


174
00:07:29,249 --> 00:07:32,152 line:-1
Here we have two cubes.


175
00:07:32,152 --> 00:07:35,822 line:-1
The blue cube on the left is not
being updated by a WorldAnchor,


176
00:07:35,822 --> 00:07:38,124 line:-1
while the red cube
on the right is being updated


177
00:07:38,124 --> 00:07:40,226 line:-1
by a WorldAnchor.


178
00:07:40,226 --> 00:07:42,729 line:-1
Both cubes were placed relative
to the app's origin


179
00:07:42.729 --> 00:07:45.131 line:-1 position:50%
when the app was launched.


180
00:07:45.131 --> 00:07:46.866 line:-1 position:50%
As the device moves around,


181
00:07:46,866 --> 00:07:50,003 line:-1
both cubes remain
where they were placed.


182
00:07:50.003 --> 00:07:54.040 line:-1 position:50%
You can press and hold the crown
to recenter the app.


183
00:07:54,040 --> 00:07:55,608 line:-1
When recentering occurs,


184
00:07:55.608 --> 00:07:59.212 line:-1 position:50%
the app's origin will be moved
to your current location.


185
00:07:59,212 --> 00:08:01,915 line:-1
Notice that the blue cube,
which is not anchored,


186
00:08:01.915 --> 00:08:04.050 line:-1 position:50%
relocates to maintain
its relative placement


187
00:08:04.050 --> 00:08:05.618 line:-1 position:50%
to the app's origin;


188
00:08:05.618 --> 00:08:07.854 line:-1 position:50%
while the red cube,
which is anchored,


189
00:08:07,854 --> 00:08:11,224 line:-1
remains fixed
relative to the real world.


190
00:08:11,224 --> 00:08:14,594 line:-1
Let's take a look at how
WorldAnchor persistence works.


191
00:08:14,594 --> 00:08:16,496 line:-1
As the device moves around,


192
00:08:16.496 --> 00:08:19.532 line:-1 position:50%
ARKit builds a map
of your surroundings.


193
00:08:19,532 --> 00:08:22,402 line:-1
When you add WorldAnchors,
we insert them into the map


194
00:08:22.402 --> 00:08:25.171 line:-1 position:50%
and automatically
persist them for you.


195
00:08:25,171 --> 00:08:29,676 line:-1
Only WorldAnchor identifiers
and transforms are persisted.


196
00:08:29,676 --> 00:08:32,112 line:-1
No other data,
such as your virtual content,


197
00:08:32,112 --> 00:08:33,747 line:-1
is included.


198
00:08:33,747 --> 00:08:35,315 line:-1
It is up to you
to maintain a mapping


199
00:08:35.315 --> 00:08:37.751 line:-1 position:50%
of WorldAnchor identifiers
to any virtual content


200
00:08:37,751 --> 00:08:41,421 line:-1
that you associate with them.


201
00:08:41.421 --> 00:08:43.957 line:-1 position:50%
Maps are location based,
so when you take your device


202
00:08:43,957 --> 00:08:45,325 line:-1
to a new location --


203
00:08:45.325 --> 00:08:47.560 line:-1 position:50%
for instance,
from home to the office --


204
00:08:47.560 --> 00:08:49.863 line:-1 position:50%
the map of your home
will be unloaded,


205
00:08:49.863 --> 00:08:53.633 line:-1 position:50%
and then a different map will be
localized for the office.


206
00:08:53.633 --> 00:08:55.969 line:-1 position:50%
Any anchors that you add
at this new location


207
00:08:55,969 --> 00:08:59,939 line:-1
will go into that map.


208
00:08:59.939 --> 00:09:01.775 line:-1 position:50%
When you leave the office
at the end of the day


209
00:09:01,775 --> 00:09:03,877 line:-1
and head home,
the map that ARKit


210
00:09:03.877 --> 00:09:06.146 line:-1 position:50%
has been building at the office,
along with any anchors


211
00:09:06,146 --> 00:09:09,616 line:-1
that you placed there,
will be unloaded.


212
00:09:09,616 --> 00:09:11,051 line:-1
Once again, though,
we have been


213
00:09:11,051 --> 00:09:14,921 line:-1
automatically persisting the map
along with your anchors.


214
00:09:14.921 --> 00:09:16.556 line:-1 position:50%
Upon returning home,


215
00:09:16,556 --> 00:09:19,459 line:-1
ARKit will recognize that
the location has changed,


216
00:09:19,459 --> 00:09:21,461 line:-1
and we will begin
the process of relocalizing


217
00:09:21.461 --> 00:09:26.066 line:-1 position:50%
by checking for an existing map
for this location.


218
00:09:26,066 --> 00:09:28,768 line:-1
If we find one,
we will localize with it,


219
00:09:28.768 --> 00:09:31.438 line:-1 position:50%
and all of the anchors that
you previously added at home


220
00:09:31.438 --> 00:09:35.742 line:-1 position:50%
will become tracked once again.


221
00:09:35.742 --> 00:09:38.845 line:-1 position:50%
Let's move on
to the device pose.


222
00:09:38.845 --> 00:09:41.147 line:-1 position:50%
Along with adding
and removing WorldAnchors,


223
00:09:41,147 --> 00:09:42,816 line:-1
you can also use
a WorldTrackingProvider


224
00:09:42.816 --> 00:09:45.218 line:-1 position:50%
to get the pose of the device.


225
00:09:45.218 --> 00:09:47.954 line:-1 position:50%
The pose is the position
and orientation of the device


226
00:09:47.954 --> 00:09:50.723 line:-1 position:50%
relative to the app's origin.


227
00:09:50,723 --> 00:09:52,659 line:-1
Querying the pose is required
if you are doing


228
00:09:52,659 --> 00:09:55,261 line:-1
your own rendering with Metal
and CompositorServices


229
00:09:55.261 --> 00:09:58.231 line:-1 position:50%
in a fully immersive experience.


230
00:09:58.231 --> 00:10:00.867 line:-1 position:50%
This query
is relatively expensive.


231
00:10:00,867 --> 00:10:03,703 line:-1
Exercise caution when
querying for the device pose


232
00:10:03,703 --> 00:10:07,474 line:-1
for other types of app logic,
such as content placement.


233
00:10:07,474 --> 00:10:09,943 line:-1
Let's quickly walk through
a simplified rendering example


234
00:10:09,943 --> 00:10:12,378 line:-1
to demonstrate how you can
provide device poses


235
00:10:12.378 --> 00:10:16.082 line:-1 position:50%
from ARKit
to CompositorServices.


236
00:10:16,082 --> 00:10:19,219 line:-1
We have a Renderer struct
that will hold our session,


237
00:10:19.219 --> 00:10:23.556 line:-1 position:50%
world tracking provider,
and latest pose.


238
00:10:23.556 --> 00:10:28.962 line:-1 position:50%
When initializing the Renderer,
we start by creating a session.


239
00:10:28,962 --> 00:10:31,297 line:-1
Next, we create
a world tracking provider,


240
00:10:31,297 --> 00:10:33,600 line:-1
which we will use to query
for the device pose


241
00:10:33.600 --> 00:10:36.369 line:-1 position:50%
when we render each frame.


242
00:10:36.369 --> 00:10:38.638 line:-1 position:50%
Now, we can go ahead
and run our session


243
00:10:38.638 --> 00:10:41.474 line:-1 position:50%
with any data providers
that we need.


244
00:10:41,474 --> 00:10:45,345 line:-1
In this case, we are only using
a world tracking provider.


245
00:10:45,345 --> 00:10:47,280 line:-1
We also create a pose
to avoid allocating


246
00:10:47,280 --> 00:10:49,916 line:-1
in the render function.


247
00:10:49.916 --> 00:10:51.985 line:-1 position:50%
Let's jump over
to our render function now,


248
00:10:51,985 --> 00:10:55,088 line:-1
which we will be calling
at frame rate.


249
00:10:55,088 --> 00:10:57,524 line:-1
Using the drawable
from CompositorServices,


250
00:10:57.524 --> 00:11:00.593 line:-1 position:50%
we fetch the target render time.


251
00:11:00,593 --> 00:11:02,529 line:-1
Next, we use
the target render time


252
00:11:02.529 --> 00:11:06.266 line:-1 position:50%
to query for the pose
of the device.


253
00:11:06,266 --> 00:11:09,469 line:-1
If successful, we can extract
a transform of the pose


254
00:11:09.469 --> 00:11:11.638 line:-1 position:50%
relative to the app's origin.


255
00:11:11,638 --> 00:11:15,842 line:-1
This is the transform to use
for rendering your content.


256
00:11:15,842 --> 00:11:18,611 line:-1
Lastly, before we submit
the frame for compositing,


257
00:11:18.611 --> 00:11:20.346 line:-1 position:50%
we set the pose on the drawable,


258
00:11:20,346 --> 00:11:22,649 line:-1
so that the compositor knows
which pose we used


259
00:11:22.649 --> 00:11:25.752 line:-1 position:50%
to render content for the frame.


260
00:11:25,752 --> 00:11:28,254 position:50%
For more information
on doing your own rendering,


261
00:11:28,254 --> 00:11:30,356 position:50%
see the dedicated session
for using Metal


262
00:11:30,356 --> 00:11:32,559 line:0
to create immersive apps.


263
00:11:32,559 --> 00:11:34,561 position:50%
Additionally,
there is a great session


264
00:11:34,561 --> 00:11:37,063 line:0
on spatial computing
performance considerations


265
00:11:37,063 --> 00:11:39,599 line:0
that we encourage you
to check out as well.


266
00:11:39,599 --> 00:11:43,136 line:-1
Next, let's take a look
at scene understanding.


267
00:11:43,136 --> 00:11:45,371 line:-1
Scene understanding
is a category of features


268
00:11:45,371 --> 00:11:48,808 line:-1
that inform you about your
surroundings in different ways.


269
00:11:48,808 --> 00:11:51,344 line:-1
Let's begin
with plane detection.


270
00:11:51.344 --> 00:11:53.546 line:-1 position:50%
Plane detection provides
anchors for horizontal


271
00:11:53.546 --> 00:11:58.117 line:-1 position:50%
and vertical surfaces that
ARKit detects in the real world.


272
00:11:58,117 --> 00:12:00,420 line:-1
The type of DataProvider
that plane detection uses


273
00:12:00,420 --> 00:12:03,356 line:-1
is called
PlaneDetectionProvider.


274
00:12:03.356 --> 00:12:05.491 line:-1 position:50%
As planes are detected
in your surroundings,


275
00:12:05.491 --> 00:12:09.529 line:-1 position:50%
they are provided to you
in the form of PlaneAnchors.


276
00:12:09,529 --> 00:12:12,432 line:-1
PlaneAnchors can be used
to facilitate content placement,


277
00:12:12.432 --> 00:12:16.002 line:-1 position:50%
such as placing
a virtual object on a table.


278
00:12:16,002 --> 00:12:18,905 line:-1
Additionally, you can use planes
for physics simulations


279
00:12:18,905 --> 00:12:22,208 line:-1
where basic, flat geometry,
such as a floor or wall,


280
00:12:22.208 --> 00:12:24.944 line:-1 position:50%
is sufficient.


281
00:12:24,944 --> 00:12:27,213 line:-1
Each PlaneAnchor
includes an alignment,


282
00:12:27,213 --> 00:12:31,851 line:-1
which is horizontal or vertical;
the geometry of the plane;


283
00:12:31.851 --> 00:12:35.355 line:-1 position:50%
and a semantic classification.


284
00:12:35.355 --> 00:12:36.990 line:-1 position:50%
Planes can be classified
as a variety


285
00:12:36.990 --> 00:12:40.627 line:-1 position:50%
of different types of surfaces,
such as floor or table.


286
00:12:40.627 --> 00:12:43.763 line:-1 position:50%
If we are unable to identify
a particular surface,


287
00:12:43.763 --> 00:12:46.599 line:-1 position:50%
the provided classification will
be marked as either unknown,


288
00:12:46,599 --> 00:12:51,738 line:-1
undetermined, or not available,
depending on the circumstances.


289
00:12:51.738 --> 00:12:55.975 line:-1 position:50%
Now, let's move on
to scene geometry.


290
00:12:55.975 --> 00:12:59.279 line:-1 position:50%
Scene geometry provides anchors
containing a polygonal mesh


291
00:12:59,279 --> 00:13:02,015 line:-1
that estimates the shape
of the real world.


292
00:13:02.015 --> 00:13:04.417 line:-1 position:50%
The type of DataProvider
that scene geometry uses


293
00:13:04.417 --> 00:13:08.087 line:-1 position:50%
is called
SceneReconstructionProvider.


294
00:13:08,087 --> 00:13:10,490 line:-1
As ARKit scans
the world around you,


295
00:13:10.490 --> 00:13:13.559 line:-1 position:50%
we reconstruct your surroundings
as a subdivided mesh,


296
00:13:13,559 --> 00:13:18,131 line:-1
which is then provided to you
in the form of MeshAnchors.


297
00:13:18.131 --> 00:13:20.600 line:-1 position:50%
Like PlaneAnchors,
MeshAnchors can be used


298
00:13:20,600 --> 00:13:23,436 line:-1
to facilitate content placement.


299
00:13:23.436 --> 00:13:24.270 line:-1 position:50%
You can also achieve


300
00:13:24,270 --> 00:13:26,406 line:-1
higher-fidelity
physics simulations


301
00:13:26,406 --> 00:13:28,775 line:-1
in cases where you need
virtual content to interact


302
00:13:28,775 --> 00:13:33,112 line:-1
with objects that are not
just simple, flat surfaces.


303
00:13:33.112 --> 00:13:37.884 line:-1 position:50%
Each MeshAnchor includes
geometry of the mesh.


304
00:13:37.884 --> 00:13:42.822 line:-1 position:50%
This geometry contains
vertices, normals, faces,


305
00:13:42.822 --> 00:13:47.427 line:-1 position:50%
and semantic classifications,
which are per face.


306
00:13:47,427 --> 00:13:49,462 line:-1
Mesh faces can be
classified as a variety


307
00:13:49,462 --> 00:13:51,597 line:-1
of different types of objects.


308
00:13:51,597 --> 00:13:54,500 line:-1
If we are unable to identify
a particular object,


309
00:13:54,500 --> 00:13:58,204 line:-1
the provided classification
will be none.


310
00:13:58.204 --> 00:14:02.141 line:-1 position:50%
Lastly, let's take a look
at image tracking.


311
00:14:02.141 --> 00:14:05.511 line:-1 position:50%
Image tracking enables you
to detect 2D images


312
00:14:05,511 --> 00:14:07,947 line:-1
in the real world.


313
00:14:07.947 --> 00:14:10.249 line:-1 position:50%
The type of DataProvider
that image tracking uses


314
00:14:10,249 --> 00:14:13,152 line:-1
is called ImageTrackingProvider.


315
00:14:13,152 --> 00:14:14,787 line:-1
You configure
ImageTrackingProvider


316
00:14:14,787 --> 00:14:18,524 line:-1
with a set of ReferenceImages
that you want to detect.


317
00:14:18.524 --> 00:14:22.562 line:-1 position:50%
These ReferenceImages can be
created in a few different ways.


318
00:14:22,562 --> 00:14:24,997 line:-1
One option is to load them
from an AR resource group


319
00:14:24.997 --> 00:14:28.034 line:-1 position:50%
in your project's asset catalog.


320
00:14:28.034 --> 00:14:30.136 line:-1 position:50%
Alternatively,
you can also initialize


321
00:14:30,136 --> 00:14:33,606 line:-1
a ReferenceImage yourself
by providing a CVPixelBuffer


322
00:14:33,606 --> 00:14:36,976 position:50%
or CGImage.


323
00:14:36,976 --> 00:14:38,678 line:-1
When an image is detected,


324
00:14:38.678 --> 00:14:41.881 line:-1 position:50%
ARKit provides you
with an ImageAnchor.


325
00:14:41,881 --> 00:14:43,683 line:-1
ImageAnchors can be used
to place content


326
00:14:43.683 --> 00:14:46.686 line:-1 position:50%
at known,
statically placed images.


327
00:14:46,686 --> 00:14:48,755 line:-1
For instance, you can display
some information


328
00:14:48,755 --> 00:14:52,358 line:-1
about a movie
next to a movie poster.


329
00:14:52,358 --> 00:14:54,394 line:-1
ImageAnchors are
TrackableAnchors


330
00:14:54.394 --> 00:14:56.529 line:-1 position:50%
that include an estimated
scale factor,


331
00:14:56,529 --> 00:14:59,098 line:-1
which indicates how the size
of the detected image


332
00:14:59,098 --> 00:15:01,801 line:-1
compares to the physical size
that you specified


333
00:15:01,801 --> 00:15:05,538 line:-1
and the ReferenceImage
that the anchor corresponds to.


334
00:15:05,538 --> 00:15:08,741 line:-1
Now, to tell you about
our new feature, hand tracking,


335
00:15:08.741 --> 00:15:10.643 line:-1 position:50%
and then walk you
through the example,


336
00:15:10.643 --> 00:15:12.044 line:-1 position:50%
here is Conner.


337
00:15:12.044 --> 00:15:12.945 line:-1 position:50%
Conner: Howdy.


338
00:15:12,945 --> 00:15:14,380 line:-1
Let's take a look
at hand tracking,


339
00:15:14,380 --> 00:15:16,816 line:-1
a brand-new addition to ARKit.


340
00:15:16,816 --> 00:15:18,451 line:-1
Hand tracking provides you
with anchors


341
00:15:18,451 --> 00:15:21,454 line:-1
containing skeletal data
for each of your hands.


342
00:15:21.454 --> 00:15:23.856 line:-1 position:50%
The type of DataProvider
that hand tracking uses


343
00:15:23,856 --> 00:15:26,292 line:-1
is called HandTrackingProvider.


344
00:15:26.292 --> 00:15:28.694 line:-1 position:50%
When your hands are detected,
they are provided to you


345
00:15:28,694 --> 00:15:30,730 line:-1
in the form of HandAnchors.


346
00:15:30.730 --> 00:15:33.199 line:-1 position:50%
A HandAnchor is
a TrackableAnchor.


347
00:15:33,199 --> 00:15:37,170 line:-1
HandAnchors include a skeleton
and a chirality.


348
00:15:37.170 --> 00:15:41.874 line:-1 position:50%
The chirality tells us whether
this is a left or a right hand.


349
00:15:41,874 --> 00:15:44,944 line:-1
A HandAnchor's transform
is the wrist's transform


350
00:15:44.944 --> 00:15:47.380 line:-1 position:50%
relative to the app origin.


351
00:15:47.380 --> 00:15:51.350 line:-1 position:50%
The skeleton consists of joints,
which can be queried by name.


352
00:15:51,350 --> 00:15:55,721 line:-1
A joint contains
its parent joint; its name;


353
00:15:55,721 --> 00:15:59,091 line:-1
a localTransform, which is
relative to its parent joint;


354
00:15:59,091 --> 00:16:02,295 position:50%
a rootTransform, which is
relative to the root joint;


355
00:16:02,295 --> 00:16:04,363 line:0
and finally, each joint
contains a bool,


356
00:16:04,363 --> 00:16:07,400 line:0
which indicates whether or not
this joint is tracked.


357
00:16:07,400 --> 00:16:09,769 line:-1
Here we enumerate all
of the available joints


358
00:16:09.769 --> 00:16:11.437 line:-1 position:50%
in the hand skeleton.


359
00:16:11.437 --> 00:16:14.473 line:-1 position:50%
Let's walk through a subset
of the joint's hierarchy.


360
00:16:14.473 --> 00:16:17.643 line:-1 position:50%
The wrist is the root joint
for the hand.


361
00:16:17.643 --> 00:16:21.113 line:-1 position:50%
For each finger, the first joint
is parented to the wrist;


362
00:16:21.113 --> 00:16:25.351 line:-1 position:50%
for example, 1 is parented to 0.


363
00:16:25.351 --> 00:16:28.254 line:-1 position:50%
Subsequent finger joints are
parented to the previous joint;


364
00:16:28,254 --> 00:16:33,159 line:-1
for example, 2 is parented to 1,
and so on.


365
00:16:33.159 --> 00:16:35.061 line:-1 position:50%
HandAnchors can be
used to place content


366
00:16:35,061 --> 00:16:38,464 line:-1
relative to your hands
or detect custom gestures.


367
00:16:38,464 --> 00:16:40,967 line:-1
There are two options
for receiving HandAnchors --


368
00:16:40,967 --> 00:16:43,736 line:-1
you can either poll for updates
or receive anchors


369
00:16:43,736 --> 00:16:46,305 line:-1
asynchronously
when they are available.


370
00:16:46.305 --> 00:16:47.807 line:-1 position:50%
We'll take a look
at asynchronous updates


371
00:16:47,807 --> 00:16:51,177 line:-1
in our Swift example later on,
so let's add hand anchor polling


372
00:16:51.177 --> 00:16:54.080 line:-1 position:50%
to our renderer from earlier.


373
00:16:54.080 --> 00:16:56.616 line:-1 position:50%
Here's our updated
struct definition.


374
00:16:56.616 --> 00:16:58.651 line:-1 position:50%
We've added
a hand tracking provider,


375
00:16:58.651 --> 00:17:01.954 line:-1 position:50%
along with a left and right
hand anchor.


376
00:17:01.954 --> 00:17:03.923 line:-1 position:50%
In our updated init function,
we create


377
00:17:03.923 --> 00:17:06.292 line:-1 position:50%
our new hand tracking provider
and add it to the list


378
00:17:06.292 --> 00:17:09.028 line:-1 position:50%
of providers that we run;
we then create


379
00:17:09.028 --> 00:17:12.431 line:-1 position:50%
the left and right hand anchors
that we'll need when we poll.


380
00:17:12.431 --> 00:17:14.400 line:-1 position:50%
Note, we create these
ahead of time


381
00:17:14,400 --> 00:17:17,870 line:-1
in order to avoid allocating
in the render loop.


382
00:17:17,870 --> 00:17:20,172 line:-1
With our struct updated
and initialized,


383
00:17:20.172 --> 00:17:23.976 line:-1 position:50%
we can call get_latest_anchors
in our render function.


384
00:17:23.976 --> 00:17:27.880 line:-1 position:50%
We pass the provider and
our preallocated hand anchors.


385
00:17:27.880 --> 00:17:32.285 line:-1 position:50%
Our anchors will be populated
with the latest available data.


386
00:17:32,285 --> 00:17:33,986 line:-1
With our latest
anchors populated,


387
00:17:33.986 --> 00:17:36.989 line:-1 position:50%
we can now use their data
in our experience.


388
00:17:36.989 --> 00:17:38.090 line:-1 position:50%
Very cool.


389
00:17:38,090 --> 00:17:41,193 line:-1
Now it's time to revisit the
example we showed you earlier.


390
00:17:41,193 --> 00:17:44,096 line:-1
We used a combination of ARKit
and RealityKit features


391
00:17:44,096 --> 00:17:45,898 line:-1
to build this experience.


392
00:17:45.898 --> 00:17:48.134 line:-1 position:50%
Scene geometry was used
as colliders for physics


393
00:17:48.134 --> 00:17:50.236 line:-1 position:50%
and gestures,
while hand tracking was used


394
00:17:50,236 --> 00:17:52,939 line:-1
to directly interact
with the cube entities.


395
00:17:52.939 --> 00:17:55.741 line:-1 position:50%
Let's take a look
at how we built this example.


396
00:17:55.741 --> 00:17:59.145 line:-1 position:50%
First, we'll check out
the app struct and view model.


397
00:17:59.145 --> 00:18:02.048 line:-1 position:50%
Next, we'll initialize
the ARKit session.


398
00:18:02,048 --> 00:18:05,251 line:-1
Then we'll add colliders
for our fingertips


399
00:18:05,251 --> 00:18:07,820 line:-1
and colliders
from scene reconstruction.


400
00:18:07.820 --> 00:18:10.957 line:-1 position:50%
Finally, we'll look at how
to add cubes with gestures.


401
00:18:10,957 --> 00:18:13,793 line:-1
Let's just jump right into it.


402
00:18:13,793 --> 00:18:17,096 line:-1
Here is our app, TimeForCube.


403
00:18:17.096 --> 00:18:21.367 line:-1 position:50%
We have a relatively standard
SwiftUI app and scene setup.


404
00:18:21.367 --> 00:18:24.370 line:-1 position:50%
Within our scene,
we declare an ImmersiveSpace.


405
00:18:24,370 --> 00:18:26,405 line:-1
The IimmersiveSpace is required
as we'll need to move


406
00:18:26.405 --> 00:18:29.976 line:-1 position:50%
to a Full Space in order
to get access to ARKit data.


407
00:18:29,976 --> 00:18:32,712 line:-1
Within the ImmersiveSpace,
we define a RealityView


408
00:18:32,712 --> 00:18:35,815 line:-1
which will present the content
from our view model.


409
00:18:35.815 --> 00:18:38.918 line:-1 position:50%
The view model is where most
of our app's logic will live.


410
00:18:38.918 --> 00:18:41.487 line:-1 position:50%
Let's take a quick look.


411
00:18:41,487 --> 00:18:44,523 line:-1
The view model holds
onto the ARKit session;


412
00:18:44.523 --> 00:18:46.659 line:-1 position:50%
the data providers
we'll be using;


413
00:18:46,659 --> 00:18:48,561 line:-1
our content entity,
which will contain


414
00:18:48.561 --> 00:18:50.663 line:-1 position:50%
all other entities
that we create;


415
00:18:50,663 --> 00:18:55,634 line:-1
and both our scene
and hand collider maps.


416
00:18:55.634 --> 00:18:57.937 line:-1 position:50%
Our view model also
provides various functions


417
00:18:57,937 --> 00:18:59,372 line:-1
that we'll call from the app.


418
00:18:59,372 --> 00:19:02,675 line:-1
We'll go through each of these
in context from the app.


419
00:19:02,675 --> 00:19:05,778 line:-1
The first function we'll call
is within our RealityView's


420
00:19:05.778 --> 00:19:09.081 line:-1 position:50%
make closure to set up
the contentEntity.


421
00:19:09,081 --> 00:19:12,151 line:-1
We'll add this entity to
the content of our RealityView,


422
00:19:12.151 --> 00:19:17.390 line:-1 position:50%
so that the view model can add
entities to the view's content.


423
00:19:17,390 --> 00:19:20,526 line:-1
setupContentEntity simply adds
all the finger entities


424
00:19:20,526 --> 00:19:23,029 line:-1
in our map as children
of the contentEntity


425
00:19:23.029 --> 00:19:24.864 line:-1 position:50%
and then returns it.


426
00:19:24.864 --> 00:19:25.798 line:-1 position:50%
Nice!


427
00:19:25.798 --> 00:19:28.534 line:-1 position:50%
Let's move on
to session initialization.


428
00:19:28.534 --> 00:19:31.604 line:-1 position:50%
Our session initialization
runs in one of three tasks.


429
00:19:31.604 --> 00:19:34.907 line:-1 position:50%
Our first task calls
the runSession function.


430
00:19:34,907 --> 00:19:39,211 line:-1
This function simply runs the
session with our two providers.


431
00:19:39.211 --> 00:19:40.413 line:-1 position:50%
With the session running,


432
00:19:40.413 --> 00:19:42.348 line:-1 position:50%
we can start receiving
anchor updates.


433
00:19:42.348 --> 00:19:44.250 line:-1 position:50%
Let's create and update
our fingertip colliders


434
00:19:44.250 --> 00:19:47.586 line:-1 position:50%
we'll use to interact
with cubes.


435
00:19:47.586 --> 00:19:51.590 line:-1 position:50%
Here is our task
for processing hand updates.


436
00:19:51,590 --> 00:19:54,126 line:-1
Its function iterates over
the async sequence


437
00:19:54,126 --> 00:19:56,696 line:-1
of anchor updates
on the provider.


438
00:19:56,696 --> 00:19:59,065 line:0
We ensure that the hand anchor
is tracked,


439
00:19:59,065 --> 00:20:00,833 position:50%
get the index fingertip joint,


440
00:20:00,833 --> 00:20:06,439 position:50%
and check that the joint itself
is also tracked.


441
00:20:06,439 --> 00:20:09,408 position:50%
We then compute the transform
of the tip of the index finger


442
00:20:09,408 --> 00:20:12,111 line:0
relative to the app origin.


443
00:20:12,111 --> 00:20:14,747 position:50%
Finally, we look up which
finger entity we should update


444
00:20:14,747 --> 00:20:16,749 line:0
and set its transform.


445
00:20:19,819 --> 00:20:21,854 line:-1
Let's revisit
our finger entity map.


446
00:20:21.854 --> 00:20:27.193 line:-1 position:50%
We create an entity per hand
via an extension to ModelEntity,


447
00:20:27.193 --> 00:20:31.597 line:-1 position:50%
This extension creates a 5mm
sphere with a collision shape.


448
00:20:31,597 --> 00:20:33,699 line:-1
We add a kinematic
physics body component


449
00:20:33,699 --> 00:20:37,002 line:-1
and hide this entity by adding
an opacity component.


450
00:20:37.002 --> 00:20:38.637 line:-1 position:50%
Though we'll hide these
for our use case,


451
00:20:38,637 --> 00:20:40,806 line:-1
it'd be nice to visualize
our fingertip entities


452
00:20:40,806 --> 00:20:43,409 line:-1
to verify that everything
is working as expected.


453
00:20:43.409 --> 00:20:45.644 line:-1 position:50%
Let's temporarily set
our opacity to one


454
00:20:45,644 --> 00:20:48,180 line:-1
and make sure our entities
are in the right place.


455
00:20:48,180 --> 00:20:49,281 line:-1
Great!


456
00:20:49.281 --> 00:20:51.851 line:-1 position:50%
We can see spheres
right where our fingertips are!


457
00:20:51,851 --> 00:20:54,920 line:-1
Notice, our hands are partially
covering up the spheres.


458
00:20:54,920 --> 00:20:58,023 line:-1
This is called hand occlusion,
a system feature that allows


459
00:20:58,023 --> 00:21:01,427 line:-1
a person to see their hands
on top of virtual content.


460
00:21:01,427 --> 00:21:03,095 line:-1
This is enabled by default,


461
00:21:03.095 --> 00:21:05.831 line:-1 position:50%
but if we'd like to see our
sphere a little more clearly,


462
00:21:05.831 --> 00:21:08.067 line:-1 position:50%
we can configure
hand occlusion visibility


463
00:21:08.067 --> 00:21:11.770 line:-1 position:50%
by using the upperLimbVisibility
setter on our scene.


464
00:21:11.770 --> 00:21:14.406 line:-1 position:50%
If we set limb visibility
to hidden,


465
00:21:14.406 --> 00:21:15.708 line:-1 position:50%
we'll see the entire sphere


466
00:21:15.708 --> 00:21:18.110 line:-1 position:50%
regardless of where
our hands are.


467
00:21:18,110 --> 00:21:20,546 line:-1
For our example, we'll leave
the upper limb visibility


468
00:21:20.546 --> 00:21:23.849 line:-1 position:50%
as the default value and set
the opacity back to zero.


469
00:21:23,849 --> 00:21:26,051 line:-1
Neat! Now let's add
scene colliders --


470
00:21:26,051 --> 00:21:29,155 line:-1
we'll use these for physics
and as gesture targets.


471
00:21:29.155 --> 00:21:32.458 line:-1 position:50%
Here's the task that calls
the function on our model.


472
00:21:32,458 --> 00:21:35,928 line:-1
We iterate over the async
sequence of anchor updates


473
00:21:35.928 --> 00:21:38.430 line:-1 position:50%
on the provider,
attempt to generate


474
00:21:38,430 --> 00:21:40,966 line:-1
a ShapeResource
from the MeshAnchor,


475
00:21:40.966 --> 00:21:44.303 line:-1 position:50%
then switch on the anchor
update's event.


476
00:21:44.303 --> 00:21:47.206 line:-1 position:50%
If we're adding an anchor,
we create a new entity,


477
00:21:47,206 --> 00:21:48,841 line:-1
set its transform,


478
00:21:48.841 --> 00:21:51.310 line:-1 position:50%
add a collision and
physics body component,


479
00:21:51.310 --> 00:21:53.012 line:-1 position:50%
then add an input target
component


480
00:21:53,012 --> 00:21:56,148 line:-1
so that this collider
can be a target for gestures.


481
00:21:56.148 --> 00:21:58.517 line:-1 position:50%
Finally, we add
a new entity to our map


482
00:21:58.517 --> 00:22:02.588 line:-1 position:50%
and as a child
of our content entity.


483
00:22:02.588 --> 00:22:05.658 line:-1 position:50%
To update an entity,
we retrieve it from the map,


484
00:22:05,658 --> 00:22:09,395 line:-1
then update its transform
and collision component shape.


485
00:22:09,395 --> 00:22:12,765 line:-1
For removal, we remove
the corresponding entity


486
00:22:12,765 --> 00:22:15,334 line:-1
from its parent and the map.


487
00:22:15.334 --> 00:22:17.102 line:-1 position:50%
Now that we have
hand and scene colliders,


488
00:22:17.102 --> 00:22:19.605 line:-1 position:50%
we can use gestures
to add cubes.


489
00:22:19,605 --> 00:22:22,474 line:-1
We add a SpatialTapGesture
targeted to any entity,


490
00:22:22,474 --> 00:22:24,376 line:-1
which will let us know
if someone has tapped


491
00:22:24,376 --> 00:22:27,546 line:-1
on any entity in
our RealityView's content.


492
00:22:27.546 --> 00:22:30.749 line:-1 position:50%
When that tap has ended,
we receive a 3D location


493
00:22:30.749 --> 00:22:33.552 line:-1 position:50%
we convert from global
to scene coordinates.


494
00:22:33,552 --> 00:22:35,588 line:-1
Let's visualize this location.


495
00:22:35,588 --> 00:22:37,556 line:-1
Here's what we'd see
if we added a sphere


496
00:22:37.556 --> 00:22:40.159 line:-1 position:50%
at the location of the tap.


497
00:22:40.159 --> 00:22:42.494 line:-1 position:50%
Now, we tell our view model
to add a cube


498
00:22:42.494 --> 00:22:44.597 line:-1 position:50%
relative to this location.


499
00:22:44.597 --> 00:22:47.800 line:-1 position:50%
To add a cube, we first
calculate a placement location


500
00:22:47.800 --> 00:22:51.003 line:-1 position:50%
that's 20 centimeters
above the tap location.


501
00:22:51,003 --> 00:22:53,372 line:-1
We then create the cube
and set its position


502
00:22:53.372 --> 00:22:55.841 line:-1 position:50%
to our calculated
placement location.


503
00:22:55.841 --> 00:22:58.510 line:-1 position:50%
We add an InputTargetComponent,
which allows us set


504
00:22:58.510 --> 00:23:01.614 line:-1 position:50%
which types of gestures
our entity will respond to.


505
00:23:01.614 --> 00:23:04.750 line:-1 position:50%
For our use case, we'll allow
only indirect input types


506
00:23:04,750 --> 00:23:06,552 line:-1
for these cubes,
as our fingertip colliders


507
00:23:06.552 --> 00:23:09.321 line:-1 position:50%
will provide direct interaction.


508
00:23:09.321 --> 00:23:11.924 line:-1 position:50%
We add a PhysicsBodyComponent
with custom parameters


509
00:23:11,924 --> 00:23:14,727 line:-1
to make the physics interactions
a bit nicer.


510
00:23:14,727 --> 00:23:16,962 line:-1
Last, we add our cube
to the content entity,


511
00:23:16,962 --> 00:23:20,499 line:-1
which means it is finally
time for cube.


512
00:23:20,499 --> 00:23:24,036 line:-1
Let's take one last look
at our example, end to end.


513
00:23:24.036 --> 00:23:26.705 line:-1 position:50%
Every time we tap on
the scene colliders or a cube,


514
00:23:26,705 --> 00:23:29,675 line:-1
a new cube is added
above the tap location.


515
00:23:29,675 --> 00:23:31,143 line:-1
The physics system
causes the cube


516
00:23:31.143 --> 00:23:32.911 line:-1 position:50%
to drop onto
the scene colliders,


517
00:23:32.911 --> 00:23:36.348 line:-1 position:50%
and our hand colliders allow us
to interact with the cubes.


518
00:23:36,348 --> 00:23:38,651 position:50%
For more information
about RealityKit,


519
00:23:38,651 --> 00:23:41,153 line:0
check out the introductory
session on using RealityKit


520
00:23:41,153 --> 00:23:42,521 position:50%
for spatial computing.


521
00:23:42,521 --> 00:23:44,790 position:50%
And, if you already have
an existing ARKit experience


522
00:23:44,790 --> 00:23:46,525 line:0
on iOS that you're interested
in bringing over


523
00:23:46,525 --> 00:23:48,460 line:0
to this platform,
be sure to watch


524
00:23:48,460 --> 00:23:51,997 line:0
the dedicated session on
this topic for further guidance.


525
00:23:51.997 --> 00:23:54.199 line:-1 position:50%
Our entire team is incredibly
thrilled for you


526
00:23:54.199 --> 00:23:56.602 line:-1 position:50%
to get your hands on
the new version of ARKit.


527
00:23:56.602 --> 00:23:58.938 line:-1 position:50%
We cannot wait to see all
of the groundbreaking apps


528
00:23:58.938 --> 00:24:01.940 line:-1 position:50%
that you will create
for this exciting new platform.


529
00:24:01,940 --> 00:24:03,075 line:-1
Ryan: Thanks for watching!


530
00:24:03,075 --> 00:24:07,646 size:2% align:right line:0
♪

