2
00:00:00.334 --> 00:00:05.439
[upbeat music]


3
00:00:10.010 --> 00:00:10.978
Rob: Hi.


4
00:00:11.044 --> 00:00:14.448
I'm Rob Simutis from the Camera
software team, with Sebastian Medina


5
00:00:14.515 --> 00:00:17.150
from the Photos team,
and welcome to our session,


6
00:00:17.217 --> 00:00:20.687
"Create a more
responsive camera experience."


7
00:00:20.754 --> 00:00:24.725
We'll be presenting a slew of
new powerful APIs in the AVFoundation


8
00:00:24.791 --> 00:00:28.161
capture classes,
and in the PhotoKit framework.


9
00:00:28.228 --> 00:00:31.899
First, we'll talk about
deferred photo processing.


10
00:00:31.965 --> 00:00:34.868
Then, I'll show how
you can really "capture the moment"


11
00:00:34.935 --> 00:00:37.371
with zero shutter lag.


12
00:00:37.437 --> 00:00:41.642
Third, I'll cover our new
Responsive Capture APIs.


13
00:00:41.708 --> 00:00:44.811
And finally, I'll go over a different way
to be responsive


14
00:00:44.878 --> 00:00:47.614
with updated video effects.


15
00:00:47.681 --> 00:00:51.885
Starting with iOS 13,
you could use AVCapturePhotoSetting's


16
00:00:51.952 --> 00:00:55.756
photo Quality Prioritization enum
value to change the responsiveness,


17
00:00:55.822 --> 00:00:59.326
or how quickly you could capture
and get a processed photo back,


18
00:00:59.393 --> 00:01:01.962
and then be able take the next photo.


19
00:01:02.029 --> 00:01:04.498
Your app could choose
among different enum values


20
00:01:04.565 --> 00:01:09.069
to achieve the right responsiveness,
but it impacted image quality.


21
00:01:09.136 --> 00:01:11.505
Or if you always wanted
to use the quality value,


22
00:01:11.572 --> 00:01:13.440
you could impact shot-to-shot time.


23
00:01:13.507 --> 00:01:16.510
In iOS 17,
you can still use this API,


24
00:01:16.577 --> 00:01:19.246
but we're bringing you new,
complementary APIs


25
00:01:19.313 --> 00:01:22.216
so you can improve the chances
of getting the shot you wanted


26
00:01:22.282 --> 00:01:26.453
while also getting
higher-quality photos.


27
00:01:26.520 --> 00:01:29.923
I'm going to walk you through this app
my team built for our session.


28
00:01:29.990 --> 00:01:32.826
By flipping each toggle switch to "on,"
we can enable


29
00:01:32.893 --> 00:01:35.229
the new features this year,
and we'll build on the concepts


30
00:01:35.295 --> 00:01:39.433
one by one to make a more
responsive photography experience.


31
00:01:39.499 --> 00:01:43.704
So let's get moving,
starting with deferred photo processing.


32
00:01:43.770 --> 00:01:47.774
Today, to get the highest quality
photos out of the AVCapturePhotoOutput,


33
00:01:47.841 --> 00:01:51.345
you use a photo quality
prioritization enum value of "quality"


34
00:01:51.411 --> 00:01:53.981
on the settings
when you capture a photo.


35
00:01:54.047 --> 00:01:55.849
For our "balanced"
and "quality" values,


36
00:01:55.916 --> 00:02:00.087
this typically involves some type of
multiple-frame fusion and noise reduction.


37
00:02:00.153 --> 00:02:02.122
On iPhone 11 Pro and newer models,


38
00:02:02.189 --> 00:02:05.926
one of our most advanced
techniques is called "Deep Fusion."


39
00:02:05.993 --> 00:02:09.429
This gives amazing, crisp detail
in high resolution photos.


40
00:02:09.496 --> 00:02:10.764
In this Deep Fusion shot,


41
00:02:10.831 --> 00:02:14.601
the parrot's feathers are super-sharp,
and really stand out.


42
00:02:14.668 --> 00:02:16.103
But it comes at a cost.


43
00:02:16.170 --> 00:02:19.640
This processing must complete before
the next capture request will start,


44
00:02:19.706 --> 00:02:21.942
and it may take some time to finish.


45
00:02:22.009 --> 00:02:23.744
Let's look at a real-life example.


46
00:02:23.810 --> 00:02:25.512
I'm putting together
a presentation about how


47
00:02:25.579 --> 00:02:29.216
the camera software team gets
to the office to get its work done.


48
00:02:29.283 --> 00:02:34.354
Here's my colleague Devin using the latest
bike technology to get around Apple Park.


49
00:02:34.421 --> 00:02:36.356
As I tap,
I'm waiting for the shutter button


50
00:02:36.423 --> 00:02:41.261
to finish spinning from
one shot before I can take the next one.


51
00:02:41.328 --> 00:02:43.964
The end results are
great Deep Fusion photos.


52
00:02:44.031 --> 00:02:46.233
Just check out that beard detail!


53
00:02:46.300 --> 00:02:50.170
But while I was tapping,
the processing runs synchronously,


54
00:02:50.237 --> 00:02:52.773
and the shot to shot
time feels a bit sluggish.


55
00:02:52.840 --> 00:02:55.876
So I might have gotten a good
photo with crisp detail,


56
00:02:55.943 --> 00:02:58.946
but maybe not exactly
the shot I was looking for.


57
00:02:59.012 --> 00:03:01.949
Let's check out a diagram of events.


58
00:03:02.015 --> 00:03:06.320
you call AVCapturePhotoOutput's
capturePhoto method with your settings


59
00:03:06.386 --> 00:03:10.090
and your delegate receives callbacks
at various points in the process...


60
00:03:10.157 --> 00:03:13.660
such as willBeginCapture
for resolvedSettings.


61
00:03:13.727 --> 00:03:16.396
The camera software stack grabs
the frames from the sensor,


62
00:03:16.463 --> 00:03:21.568
and uses our processing techniques
to fuse them into a Deep Fusion image...


63
00:03:21.635 --> 00:03:23.837
Then it sends the photo back to you via


64
00:03:23.904 --> 00:03:26.874
the didFinishProcessingPhoto
delegate callback.


65
00:03:26.940 --> 00:03:30.143
This processing must complete
before the next capture happens,


66
00:03:30.210 --> 00:03:32.179
and that might take some time.


67
00:03:32.246 --> 00:03:33.847
You can even call capturePhoto before


68
00:03:33.914 --> 00:03:36.316
the didFinishProcessingPhoto
callback fires,


69
00:03:36.383 --> 00:03:41.188
but it won't start until the previous
photo's processing is complete.


70
00:03:41.255 --> 00:03:44.758
With deferred photo processing,
this timeline shrinks.


71
00:03:44.825 --> 00:03:47.394
You request a photo, and,
when suitable,


72
00:03:47.461 --> 00:03:51.131
the camera pipeline will deliver
a lightly-processed "proxy" photo via


73
00:03:51.198 --> 00:03:55.636
a new didFinishCapturingDeferredPhotoProxy
delegate callback.


74
00:03:55.702 --> 00:03:59.439
You store this proxy photo
into the library as a placeholder.


75
00:03:59.506 --> 00:04:02.009
And the next photo
can be immediately captured.


76
00:04:02.075 --> 00:04:05.512
The system will run the processing
later to get the final photo


77
00:04:05.579 --> 00:04:08.982
once the camera session
has been dismissed.


78
00:04:09.049 --> 00:04:12.686
So now if I turn on deferred
photo processing in my app's settings,


79
00:04:12.753 --> 00:04:15.189
the capture session will
reconfigure itself to deliver


80
00:04:15.255 --> 00:04:20.127
me proxy photos at the time
of capture when appropriate.


81
00:04:20.194 --> 00:04:22.262
And I can get the same sharp,


82
00:04:22.329 --> 00:04:25.465
highly-detailed photos as before,
but I can take more of them while


83
00:04:25.532 --> 00:04:29.903
I'm in the moment by deferring
the final processing to a later time.


84
00:04:29.970 --> 00:04:33.941
Nice wheelie.
Those bikes are solid.


85
00:04:34.007 --> 00:04:39.413
There we go. That's a great photo
for my presentation, beard and all.


86
00:04:39.479 --> 00:04:41.515
So let's look at all
the parts that interact


87
00:04:41.582 --> 00:04:44.184
to give you a deferred-processed photo.


88
00:04:44.251 --> 00:04:47.521
As a brief refresher course
from earlier WW presentations,


89
00:04:47.588 --> 00:04:50.691
when configuring an AVCaptureSession
to capture photos,


90
00:04:50.757 --> 00:04:54.528
you add an AVCaptureDeviceInput
with an AVCaptureDevice


91
00:04:54.595 --> 00:04:58.031
that is, the camera, to the session.


92
00:04:58.098 --> 00:05:00.000
Then, you add
an AVCapturePhotoOutput


93
00:05:00.067 --> 00:05:02.503
to your session,
and you select a particular


94
00:05:02.569 --> 00:05:07.341
format or session preset,
typically the "Photo" session preset


95
00:05:07.407 --> 00:05:10.811
when your app calls capturePhoto
on the photoOutput.


96
00:05:10.878 --> 00:05:14.214
If it's a type of photo that is better
suited to deferred photo processing,


97
00:05:14.281 --> 00:05:18.452
we'll call you back with
didFinishCapturing Deferred Photo Proxy.


98
00:05:18.519 --> 00:05:22.656
From there, you send
the proxy data to the photo library.


99
00:05:22.723 --> 00:05:25.826
So what you've got now
in your library is a proxy photo,


100
00:05:25.893 --> 00:05:30.764
but you'll eventually want
to use or share the final image.


101
00:05:30.831 --> 00:05:34.368
Final photo processing happens
either on-demand when you request


102
00:05:34.434 --> 00:05:38.639
the image data back from the library,
or in the background when the system


103
00:05:38.705 --> 00:05:42.676
determines that conditions are good
to do so, such as the device being idle.


104
00:05:42.743 --> 00:05:46.713
And now I'll let my colleague Sebastian
show you how to code this up.


105
00:05:46.780 --> 00:05:48.115
Over to you, Sebastian.


106
00:05:48.182 --> 00:05:49.349
Sebastian: Thanks, Rob.


107
00:05:49.416 --> 00:05:52.853
Hi, my name's Sebastian Medina
and I'm an engineer on the Photos team.


108
00:05:52.920 --> 00:05:55.856
Today I'll go over
taking a recently captured image


109
00:05:55.923 --> 00:05:58.692
through PhotoKit
to trigger deferred processing.


110
00:05:58.759 --> 00:06:02.262
Then I’ll be requesting that same
image to show what’s new in receiving


111
00:06:02.329 --> 00:06:05.499
images from a PHImageManager request.


112
00:06:05.566 --> 00:06:09.102
Although, before I get into
processing the asset through PhotoKit,


113
00:06:09.169 --> 00:06:13.507
I need to make sure the new Camera API
for deferred processing is set up.


114
00:06:13.574 --> 00:06:16.777
This will allow my app
to accept deferred photo proxy images,


115
00:06:16.844 --> 00:06:18.745
which we can send through PhotoKit.


116
00:06:18.812 --> 00:06:22.149
Now, I’ll go ahead and write
the code to take advantage of this.


117
00:06:22.216 --> 00:06:27.254
Here, I've set up the AVCapturePhotoOutput
and AVCaptureSession objects.


118
00:06:27.321 --> 00:06:31.391
Now, I can begin configuring our session.


119
00:06:31.458 --> 00:06:34.695
In this case, I want the session to have
a preset of type photo


120
00:06:34.761 --> 00:06:38.599
so we can take advantage
of deferred processing.


121
00:06:38.665 --> 00:06:44.338
Now I’ll grab the capture device
to then setup the device input.


122
00:06:44.404 --> 00:06:50.244
Then, if possible,
I'll add the device input.


123
00:06:50.310 --> 00:06:55.682
Next, I’ll want to check
if photoOutput can be added.


124
00:06:55.749 --> 00:06:58.986
And if so, add it.


125
00:06:59.052 --> 00:07:00.687
Now for the new stuff.


126
00:07:00.754 --> 00:07:03.991
I’ll check if the new
autoDeferredPhotoDeliverySupported value


127
00:07:04.057 --> 00:07:10.697
is true to make sure I can send captured
photos through deferred processing.


128
00:07:10.764 --> 00:07:12.399
If this passes,


129
00:07:12.466 --> 00:07:15.002
then I can go ahead and opt into
the new deferred photo delivery


130
00:07:15.068 --> 00:07:16.270
with the property


131
00:07:16.336 --> 00:07:22.242
autoDeferredPhotoDeliveryEnabled.


132
00:07:22.309 --> 00:07:25.712
This deferred photo delivery check
and enablement are all you would need


133
00:07:25.779 --> 00:07:29.716
to add to your Camera code
to enable deferred photos.


134
00:07:29.783 --> 00:07:34.354
Lastly, I'll commit
our session configuration.


135
00:07:34.421 --> 00:07:37.424
So now when a call is made
to the capturePhoto method,


136
00:07:37.491 --> 00:07:42.095
the delegate callback we receive will hold
a deferred proxy object.


137
00:07:42.162 --> 00:07:50.804
Let's check out an example
of one of these callbacks.


138
00:07:50.871 --> 00:07:55.142
In this photo capture callback
I'm receiving the AVCapturePhotoOutput


139
00:07:55.209 --> 00:07:58.145
and AVCaptureDeferredPhotoProxy
objects from Camera,


140
00:07:58.212 --> 00:08:01.682
which are related
to an image I recently captured.


141
00:08:01.748 --> 00:08:03.750
First, it's good practice to make sure


142
00:08:03.817 --> 00:08:06.186
we're receiving
suitable photo output values,


143
00:08:06.253 --> 00:08:11.158
so I'll check the value
of error parameter.


144
00:08:11.225 --> 00:08:13.327
Now, we'll start getting into
saving our image


145
00:08:13.393 --> 00:08:15.963
to the photolibrary using PhotoKit.


146
00:08:16.029 --> 00:08:20.267
I'll be performing changes
on the shared PHPhotoLibrary.


147
00:08:20.334 --> 00:08:26.306
Although, just note, it's only required
to have write-access to a photo library.


148
00:08:26.373 --> 00:08:32.145
Then I'll capture the photo data from the
AVCaptureDeferredPhotoProxy object.


149
00:08:36.250 --> 00:08:38.785
Since I'll be performing changes
to the photo library,


150
00:08:38.852 --> 00:08:44.691
I'll need to set up the related
performChanges instance method.


151
00:08:44.758 --> 00:08:50.464
Like with saving any asset,
I'll use a PHAssetCreationRequest.


152
00:08:50.531 --> 00:08:56.737
Then I'll call the 'addResource' method
on a request.


153
00:08:56.803 --> 00:09:01.341
For the parameters, I'll to use the new
PHAssetResourceType '.photoProxy'.


154
00:09:01.408 --> 00:09:05.445
This is what tells PhotoKit to trigger
deferred processing on the image.


155
00:09:05.512 --> 00:09:10.417
Then I can add the previously captured
proxy image data.


156
00:09:10.484 --> 00:09:15.022
And in this case I won't be using
any options.


157
00:09:15.088 --> 00:09:18.659
Here, it's important to know that using
this new resource type on image data


158
00:09:18.725 --> 00:09:22.563
that doesn't require deferred processing
will result in an error.


159
00:09:22.629 --> 00:09:23.897
And speaking of errors,


160
00:09:23.964 --> 00:09:28.202
I'll go ahead and check for them
within the completion handler.


161
00:09:28.268 --> 00:09:29.837
And it's as easy as that.


162
00:09:29.903 --> 00:09:33.106
Go ahead and handle the success
and error within the completion handler


163
00:09:33.173 --> 00:09:35.342
as your application sees fit.


164
00:09:35.409 --> 00:09:38.312
Now, say I want to retrieve our Asset.


165
00:09:38.378 --> 00:09:41.381
I can achieve that through
a PHImageManager request,


166
00:09:41.448 --> 00:09:43.350
so I'll go through the code
to do just that.


167
00:09:43.417 --> 00:09:46.353
For parameters I have
a PHAsset object for the image


168
00:09:46.420 --> 00:09:48.322
I just sent through PhotoKit,


169
00:09:48.388 --> 00:09:52.826
the target size of the image
to be returned, and the content mode.


170
00:09:52.893 --> 00:09:59.633
I'll grab a default PHImageManager object.
then, I can call the request image asset


171
00:09:59.700 --> 00:10:05.205
for the requestImageForAsset method
using our imageManager object.


172
00:10:05.272 --> 00:10:09.810
For the parameters,
I'll use the asset I previously fetched,


173
00:10:09.877 --> 00:10:13.380
the target size, content mode,


174
00:10:13.447 --> 00:10:16.683
and in this case,
I will not be using any options.


175
00:10:16.750 --> 00:10:19.520
Now I can handle the callbacks
through the resultHandler


176
00:10:19.586 --> 00:10:25.859
where the resultImage is a UIImage and
info is a dictionary related to the image.


177
00:10:25.926 --> 00:10:29.696
Today, the first callback will hold
a lower resolution image


178
00:10:29.763 --> 00:10:33.901
with the info dicitionary key
PHImageResultIsDegradedKey


179
00:10:33.967 --> 00:10:36.570
while the final image callback will not.


180
00:10:36.637 --> 00:10:41.775
So, I can make a check for those here.


181
00:10:41.842 --> 00:10:44.611
The addition of creating processed images
through PhotoKit


182
00:10:44.678 --> 00:10:47.948
gives a good opportunity
to bring up our new API which will allow


183
00:10:48.015 --> 00:10:53.153
developers to receive a secondary image
from the requestImageForAsset method.


184
00:10:53.220 --> 00:10:55.289
Since it may take longer
for an image going through


185
00:10:55.355 --> 00:10:58.592
deferred processing to finalize,
this new secondary image


186
00:10:58.659 --> 00:11:01.395
can be shown in the meantime.


187
00:11:01.461 --> 00:11:03.630
To receive this new image,
you would use the new


188
00:11:03.697 --> 00:11:08.402
allowSecondaryDegradedImage
in PHImageRequestOptions.


189
00:11:08.468 --> 00:11:11.672
This new image will be nestled
between the current two callbacks


190
00:11:11.738 --> 00:11:14.408
from the requestImageForAsset method.


191
00:11:14.474 --> 00:11:16.743
And the information dictionary
related to the image


192
00:11:16.810 --> 00:11:20.447
will have an entry for
PHImageResultIsDegradedKey,


193
00:11:20.514 --> 00:11:23.383
which is used today
in the first image callback.


194
00:11:23.450 --> 00:11:25.219
To better illustrate what’s going on,


195
00:11:25.285 --> 00:11:28.956
today, the requestImageForAsset method
provides two images.


196
00:11:29.022 --> 00:11:32.693
The first is a low-quality image
suitable for displaying temporarily


197
00:11:32.759 --> 00:11:36.263
while it prepares
the final high-quality image.


198
00:11:36.330 --> 00:11:38.532
With this new option,
between the current two,


199
00:11:38.599 --> 00:11:41.068
you will be provided a new,
higher resolution image


200
00:11:41.134 --> 00:11:44.404
to display while
the final is being processed.


201
00:11:44.471 --> 00:11:48.509
Displaying this new image will give your
user a more pleasant visual experience


202
00:11:48.575 --> 00:11:51.845
while waiting for the final image
to finish processing.


203
00:11:51.912 --> 00:11:55.883
Now, let's write the code
to take advantage of this.


204
00:11:55.949 --> 00:12:02.089
Although, this time, I'll be creating
a PHImageRequestOptions object.


205
00:12:02.155 --> 00:12:03.524
I will then set the new


206
00:12:03.590 --> 00:12:06.927
allowSecondaryDegradedImage option
to be true.


207
00:12:06.994 --> 00:12:11.765
This way the request knows to send back
the new secondary image callback.


208
00:12:11.832 --> 00:12:14.902
Here, I can go ahead and reuse


209
00:12:14.968 --> 00:12:17.137
the requestImageForAsset method
I wrote before,


210
00:12:17.204 --> 00:12:22.276
although now I'll be adding the image
request options object I just created.


211
00:12:22.342 --> 00:12:25.245
Since the new secondary
image info dictionary will hold


212
00:12:25.312 --> 00:12:28.615
a true value for
PHImageResultIsDegradedKey,


213
00:12:28.682 --> 00:12:33.687
just like the first callback,
I'll check for that here.


214
00:12:33.754 --> 00:12:37.124
And that's it for receiving
the new secondary image representation.


215
00:12:37.191 --> 00:12:39.826
Remember to handle the images
within the result handler


216
00:12:39.893 --> 00:12:41.895
to best support your app.


217
00:12:41.962 --> 00:12:44.998
Now you know how to add images
to your photo library


218
00:12:45.065 --> 00:12:48.669
with deferred processing and how to
receive a secondary higher quality image


219
00:12:48.735 --> 00:12:51.471
from an image request
to display in your app


220
00:12:51.538 --> 00:12:54.608
while waiting for the final image
to finish processing.


221
00:12:54.675 --> 00:12:59.179
These changes will be available
starting with iOS 17, tvOS 17,


222
00:12:59.246 --> 00:13:04.551
and macOS Sonoma alongside the new
deferred processing PhotoKit changes.


223
00:13:04.618 --> 00:13:06.253
Now, I’ll hand it back to Rob


224
00:13:06.320 --> 00:13:10.057
for more on the new tools
to create a more responsive Camera.


225
00:13:10.123 --> 00:13:12.326
Rob: Awesome. Thanks, Sebastian!


226
00:13:12.392 --> 00:13:15.596
Let's get into the fine details
to ensure you have a great experience


227
00:13:15.662 --> 00:13:17.631
with deferred photo processing.


228
00:13:17.698 --> 00:13:19.399
We'll start with the photo library.


229
00:13:19.466 --> 00:13:22.369
To use deferred photo processing,
you'll need to have write permission


230
00:13:22.436 --> 00:13:24.872
to the photo library
to store the proxy photo,


231
00:13:24.938 --> 00:13:28.342
and read permission if your app needs
to show the final photo


232
00:13:28.408 --> 00:13:30.978
or wants to modify it in any way.


233
00:13:31.044 --> 00:13:33.013
But remember,
you should only request the smallest


234
00:13:33.080 --> 00:13:36.049
amount of access to the library
as needed from your customers


235
00:13:36.116 --> 00:13:38.919
to maintain the most privacy
and trust on their behalf.


236
00:13:38.986 --> 00:13:42.055
And, we strongly recommend
that once you receive the proxy,


237
00:13:42.122 --> 00:13:46.927
you get its fileDataRepresentation
into the library as quickly as possible.


238
00:13:46.994 --> 00:13:49.429
When your app is backgrounded,
you have a limited amount


239
00:13:49.496 --> 00:13:51.899
of time to run
before the system suspends it.


240
00:13:51.965 --> 00:13:55.602
If memory pressure becomes too great,
your app may be automatically force-quit


241
00:13:55.669 --> 00:13:58.338
by the system during
that window of backgrounding.


242
00:13:58.405 --> 00:14:01.041
Getting the proxy into the library
as quickly as possible


243
00:14:01.108 --> 00:14:05.045
ensures minimal chance
of data loss for your customers.


244
00:14:05.112 --> 00:14:06.413
Next, if you normally make changes to


245
00:14:06.480 --> 00:14:09.816
a photo's pixel buffer
like applying a filter,


246
00:14:09.883 --> 00:14:13.687
or if you make changes to metadata
or other properties of AVCapturePhoto


247
00:14:13.754 --> 00:14:18.025
using AVCapturePhoto File Data
Representation Customizer,


248
00:14:18.091 --> 00:14:20.394
these won't take effect for
the finalized photo in the library


249
00:14:20.460 --> 00:14:22.095
once processing is complete.


250
00:14:22.162 --> 00:14:24.731
You'll need to do this later
as adjustments to the photo


251
00:14:24.798 --> 00:14:28.502
using the PhotoKit APIs.


252
00:14:28.569 --> 00:14:30.504
Also, your code needs
to be able to handle


253
00:14:30.571 --> 00:14:34.808
both deferred proxies and non-deferred
photos in the same session.


254
00:14:34.875 --> 00:14:37.311
This is because not all
photos make sense to handle


255
00:14:37.377 --> 00:14:39.246
with the extra steps necessary.


256
00:14:39.313 --> 00:14:42.449
For example, flash captures
taken under the "quality"


257
00:14:42.516 --> 00:14:46.553
photo quality prioritization enum value
aren't processed in a way that benefits


258
00:14:46.620 --> 00:14:50.424
from the shot-to-shot savings
like a Deep Fusion photo does.


259
00:14:50.490 --> 00:14:53.427
You also might notice
that there's no opt-in or opt-out property


260
00:14:53.493 --> 00:14:55.596
on the AVCapturePhotoSettings.


261
00:14:55.662 --> 00:14:58.832
That's because deferred photo
processing is automatic.


262
00:14:58.899 --> 00:15:01.235
If you opt in
and the camera pipeline will take a photo


263
00:15:01.301 --> 00:15:05.339
that requires longer processing time,
it will send you a proxy back.


264
00:15:05.405 --> 00:15:08.108
If it's not suitable,
it will send you the final photo,


265
00:15:08.175 --> 00:15:11.478
so there's no need to
opt-in or opt-out on a per shot basis.


266
00:15:11.545 --> 00:15:14.281
You just need to tell
the AVCapturePhotoOutput that you want


267
00:15:14.348 --> 00:15:19.486
isAutoDeferredPhotoProcessingEnabled as
true before you start the capture session.


268
00:15:19.553 --> 00:15:22.222
Finally, let's talk about user experience.


269
00:15:22.289 --> 00:15:24.992
Deferred photo processing provides
our best image quality


270
00:15:25.058 --> 00:15:27.094
with rapid shot-to-shot times,


271
00:15:27.160 --> 00:15:30.964
but that just delays the final processing
until a later point.


272
00:15:31.031 --> 00:15:33.834
If your app is one where the user


273
00:15:33.901 --> 00:15:35.169
may want the image right away
for sharing or editing


274
00:15:35.235 --> 00:15:38.038
and they're not as interested in the
highest quality photos we provide,


275
00:15:38.105 --> 00:15:42.009
then it may make sense to avoid using
deferred photo processing.


276
00:15:42.075 --> 00:15:46.680
This feature is available
starting with iPhone 11 Pro and 11 Pro Max


277
00:15:46.747 --> 00:15:49.917
and newer iPhones.


278
00:15:49.983 --> 00:15:52.286
And here are some great related videos


279
00:15:52.352 --> 00:15:57.591
on working with AVCapturePhotoOutput
and handling library permissions.


280
00:15:57.658 --> 00:16:03.330
And now, let's turn to Zero Shutter Lag
and talk about skateboarding.


281
00:16:03.397 --> 00:16:07.935
For my upcoming presentation about the
camera software team's modes of transport,


282
00:16:08.001 --> 00:16:10.671
we went to a skate park
to grab some footage.


283
00:16:10.737 --> 00:16:15.142
I'm filming my coworker with
Action Mode on my iPhone 14 Pro


284
00:16:15.209 --> 00:16:20.514
and I also want to get some high-quality
hero action shots for my slides.


285
00:16:20.581 --> 00:16:25.118
But, spoiler alert,
I will not be skateboarding.


286
00:16:25.185 --> 00:16:29.890
I tap the shutter button to grab a photo
of my colleague, Tomo, catching air.


287
00:16:29.957 --> 00:16:33.560
When I go to
the camera roll to inspect the photo ,


288
00:16:33.627 --> 00:16:35.162
this was what I got.


289
00:16:35.229 --> 00:16:38.165
I tapped the shutter button
when he was at the height of his jump,


290
00:16:38.232 --> 00:16:40.133
but the photo is of his landing.


291
00:16:40.200 --> 00:16:42.469
It's not exactly what I wanted.


292
00:16:42.536 --> 00:16:46.373
So what happened?
Shutter lag.


293
00:16:46.440 --> 00:16:49.476
Shutter lag happened.


294
00:16:49.543 --> 00:16:51.879
You can think of "shutter lag"
as the delay


295
00:16:51.945 --> 00:16:54.681
from when you request a capture
to reading out one or more frames


296
00:16:54.748 --> 00:16:58.785
from the sensor to fuse into a photo
and deliver it to you.


297
00:16:58.852 --> 00:17:00.821
Here, time is going from left to right,


298
00:17:00.888 --> 00:17:04.191
left being older frames,
and the right being newer frames.


299
00:17:04.258 --> 00:17:07.194
Say frame five is
what's in your camera viewfinder.


300
00:17:07.261 --> 00:17:09.630
Today, when
you call capturePhoto:with settings


301
00:17:09.696 --> 00:17:13.066
on an AVCapturePhotoOutput,
the camera pipeline starts grabbing


302
00:17:13.133 --> 00:17:17.104
frames from the sensor and
applies our processing techniques.


303
00:17:17.171 --> 00:17:22.042
But the bracket of frames captured
starts after touch-down, after frame five.


304
00:17:22.109 --> 00:17:26.780
What you get is a photo based
on frames six through nine, or even later.


305
00:17:26.847 --> 00:17:31.518
At 30 frames per second, each frame
is in the viewfinder for 33 milliseconds.


306
00:17:31.585 --> 00:17:33.654
It doesn't sound like much,
but it really doesn't take long


307
00:17:33.720 --> 00:17:35.455
for the action to be over with.


308
00:17:35.522 --> 00:17:37.391
That's long enough for Tomo
to have landed,


309
00:17:37.457 --> 00:17:40.227
and I've missed getting that hero shot.


310
00:17:40.294 --> 00:17:43.297
With Zero Shutter Lag enabled,
the camera pipeline


311
00:17:43.363 --> 00:17:47.167
keeps a rolling ring buffer
of frames from the past.


312
00:17:47.234 --> 00:17:50.170
Now, frame five is what you see
in the viewfinder,


313
00:17:50.237 --> 00:17:53.273
you tap to capture,
and the camera pipeline does


314
00:17:53.340 --> 00:17:56.844
a little bit of time travel,
grabs frames from the ring buffer,


315
00:17:56.910 --> 00:18:00.047
and fuses them together


316
00:18:00.113 --> 00:18:03.317
and you get the photo you wanted.


317
00:18:03.383 --> 00:18:06.720
So now if I use the second toggle
in my app's settings pane


318
00:18:06.787 --> 00:18:09.857
to enable Zero Shutter Lag,


319
00:18:09.923 --> 00:18:14.194
as Tomo catches air,
when I tap the shutter button


320
00:18:14.261 --> 00:18:18.599
I got one of the "hero" shots
I wanted for my presentation.


321
00:18:18.665 --> 00:18:24.104
Let's talk about what you need
to do to get Zero Shutter Lag in your app.


322
00:18:24.171 --> 00:18:27.541
Absolutely nothing!


323
00:18:27.608 --> 00:18:31.378
We've enabled Zero Shutter Lag
on apps that link on or after iOS 17


324
00:18:31.445 --> 00:18:34.648
for AVCaptureSessionPresets
and AVCaptureDeviceFormats


325
00:18:34.715 --> 00:18:39.753
where isHighest
Photo Quality Supported is true.


326
00:18:39.820 --> 00:18:41.822
But, should you find during
testing that you're


327
00:18:41.889 --> 00:18:44.391
not getting the results you want,
you can set


328
00:18:44.458 --> 00:18:49.463
AVCapturePhotoOutput.isZeroShutter
LagEnabled to false to opt out.


329
00:18:49.530 --> 00:18:52.366
And you can verify if
the photoOutput supports zero shutter lag


330
00:18:52.432 --> 00:18:56.270
for the configured preset or format
by checking if isZeroShutterLagSupported


331
00:18:56.336 --> 00:19:00.607
is true once the output is connected
to your session.


332
00:19:00.674 --> 00:19:04.378
Certain types of still image captures
such as flash captures,


333
00:19:04.444 --> 00:19:09.383
configuring the AVCaptureDevice
for a manual exposure, bracketed captures,


334
00:19:09.449 --> 00:19:14.621
and constituent photo delivery, which is
synchronized frames from multiple cameras,


335
00:19:14.688 --> 00:19:17.124
don't get Zero Shutter Lag.


336
00:19:17.191 --> 00:19:19.092
Because the camera pipeline
is traveling back in time


337
00:19:19.159 --> 00:19:22.796
to grab frames from the ring buffer,
users could induce camera shake


338
00:19:22.863 --> 00:19:25.632
into the photo if there's
a long delay between the gesture


339
00:19:25.699 --> 00:19:29.970
to start the capture and when
you send photo output the photoSettings.


340
00:19:30.037 --> 00:19:33.440
So you'll want to minimize any
work you do between the tap event


341
00:19:33.507 --> 00:19:36.677
and the capturePhoto API call
on the photo output.


342
00:19:36.743 --> 00:19:40.480
Rounding out our features for making a
more responsive photography experience,


343
00:19:40.547 --> 00:19:44.051
I'll now cover
the Responsive Capture APIs.


344
00:19:44.117 --> 00:19:46.787
This is a group of APIs
to allow your customers


345
00:19:46.854 --> 00:19:48.522
to take overlapping captures,


346
00:19:48.589 --> 00:19:52.025
prioritize shot-to-shot time
by adapting photo quality,


347
00:19:52.092 --> 00:19:56.296
and also give great UI feedback
for when they can take their next photo.


348
00:19:56.363 --> 00:19:59.433
First, the main API, responsive capture.


349
00:19:59.499 --> 00:20:02.769
Back at the skate park,
with the two features enabled earlier,


350
00:20:02.836 --> 00:20:05.639
I can capture about two photos per second.


351
00:20:05.706 --> 00:20:09.877
We've slowed down
the footage to help make it clear.


352
00:20:09.943 --> 00:20:12.613
At two frames per second,
you can't see as much of the action


353
00:20:12.679 --> 00:20:15.816
of Tomo in the air


354
00:20:15.883 --> 00:20:18.352
and this was the best photo
I ended up with.


355
00:20:18.418 --> 00:20:21.822
Pretty good,
but let's see if we can do better.


356
00:20:21.889 --> 00:20:26.493
I'll now turn on the 3rd and 4th switches
to enable Responsive Capture features.


357
00:20:26.560 --> 00:20:29.963
I'll go over Fast Capture Prioritization
in a little bit.


358
00:20:30.030 --> 00:20:32.699
But first, back to the park!


359
00:20:32.766 --> 00:20:36.403
And let's try that again.


360
00:20:36.470 --> 00:20:38.839
With responsive capture,
I can take more photos


361
00:20:38.906 --> 00:20:44.311
in the same amount of time, increasing
the chance of catching just the right one.


362
00:20:44.378 --> 00:20:47.681
And there's the "hero" shot
for the start of my presentation.


363
00:20:47.748 --> 00:20:50.817
The team's really gonna love it!


364
00:20:50.884 --> 00:20:54.922
You can think of a call to
the AVCapturePhotoOutput.capturePhoto


365
00:20:54.988 --> 00:20:57.724
with settings method as going
through three distinct phases:


366
00:20:57.791 --> 00:20:59.626
capturing frames from the sensor,


367
00:20:59.693 --> 00:21:02.529
processing those frames
to the final uncompressed image,


368
00:21:02.596 --> 00:21:05.866
and then encoding the photo
into a HEIC or JPEG.


369
00:21:05.933 --> 00:21:09.069
After the encoding is finished,
the photo output will call your delegate's


370
00:21:09.136 --> 00:21:12.639
didFinishProcessingPhoto callback,
or if you've opted in


371
00:21:12.706 --> 00:21:16.310
to the deferred photo processing API,
perhaps didFinishCapturing


372
00:21:16.376 --> 00:21:19.980
Deferred Photo Proxy,
if it's a suitable shot.


373
00:21:20.047 --> 00:21:22.983
But once the "capture"
phase is done and the "processing" starts,


374
00:21:23.050 --> 00:21:26.887
the photo output could,
in theory, start another capture.


375
00:21:26.954 --> 00:21:31.725
And now, that theory is reality,
and is available to your app.


376
00:21:31.792 --> 00:21:34.228
By opting in to
the main Responsive Capture API,


377
00:21:34.294 --> 00:21:37.497
the photo output will overlap
these phases so that a new photo capture


378
00:21:37.564 --> 00:21:40.901
request can start while another
request is in the processing phase,


379
00:21:40.968 --> 00:21:45.439
giving your customers faster
and more consistent back-to-back shots.


380
00:21:45.506 --> 00:21:48.675
Note that this will increase peak memory
used by the photo output,


381
00:21:48.742 --> 00:21:52.679
so if your app is also using a lot of
memory it will put pressure on the system,


382
00:21:52.746 --> 00:21:57.150
in which case you may
prefer or need to opt out.


383
00:21:57.217 --> 00:22:02.523
Back to our timeline diagram, here,
you take two photos in rapid succession.


384
00:22:02.589 --> 00:22:04.157
Your delegate will be called back for


385
00:22:04.224 --> 00:22:08.862
the willBeginCaptureFor resolvedSettings,
and didFinishCaptureFor resolvedSettings


386
00:22:08.929 --> 00:22:11.031
for photo A.


387
00:22:11.098 --> 00:22:15.402
But then instead of getting a didFinish
Processing Photo callback for Photo A,


388
00:22:15.469 --> 00:22:19.740
which is the photo is encoded
and delivered to you,


389
00:22:19.806 --> 00:22:24.578
you could get the first willBeginCapture
For resolvedSettings for photo B.


390
00:22:24.645 --> 00:22:27.114
There are now two in-flight
photo requests,


391
00:22:27.181 --> 00:22:29.216
so you'll have to make sure
your code properly handles


392
00:22:29.283 --> 00:22:32.486
callbacks for interleaved photos.


393
00:22:32.553 --> 00:22:35.122
To get those overlapping,
responsive captures,


394
00:22:35.189 --> 00:22:38.859
first enable Zero Shutter Lag
when it is supported.


395
00:22:38.926 --> 00:22:42.362
it must be on in order
to get responsive capture support.


396
00:22:42.429 --> 00:22:46.667
Then use the AVCapturePhotoOutput
isResponsiveCaptureSupported API


397
00:22:46.733 --> 00:22:50.671
to ensure the photo output supports it
for the preset or format,


398
00:22:50.737 --> 00:22:53.807
and then turn it on
by setting AVCapturePhotoOutput.


399
00:22:53.874 --> 00:22:56.343
.isResponsiveCaptureEnabled to true.


400
00:22:56.410 --> 00:22:59.413
Earlier,
we enabled "fast capture prioritization,"


401
00:22:59.479 --> 00:23:02.649
so I'll go over that, briefly, now.


402
00:23:02.716 --> 00:23:04.618
When it's turned on for the photo output,


403
00:23:04.685 --> 00:23:08.655
it will detect when multiple captures are
being taken over a short period of time,


404
00:23:08.722 --> 00:23:11.325
and in response,
will adapt the photo quality


405
00:23:11.391 --> 00:23:15.028
from the highest quality setting
to more of a "balanced" quality setting


406
00:23:15.095 --> 00:23:17.297
to maintain shot-to-shot time.


407
00:23:17.364 --> 00:23:22.336
But, since this can impact photo quality,
it's off by default.


408
00:23:22.402 --> 00:23:23.971
In Camera.app's Settings pane,


409
00:23:24.037 --> 00:23:26.874
this is called
"Prioritize Faster Shooting."


410
00:23:26.940 --> 00:23:30.577
We've chosen to have it on by default
for Camera.app because we think that


411
00:23:30.644 --> 00:23:34.147
consistent shot-to-shot times
are more important by default,


412
00:23:34.214 --> 00:23:38.752
but you might choose differently
for your app and your customers.


413
00:23:38.819 --> 00:23:40.988
As you might expect by now,
you can check


414
00:23:41.054 --> 00:23:44.658
the "is fast capture prioritization
supported" property on the photo output


415
00:23:44.725 --> 00:23:47.127
for when it's supported,
and when it is,


416
00:23:47.194 --> 00:23:50.631
you can set "is fast capture
prioritization enabled" to true


417
00:23:50.697 --> 00:23:53.734
if you or your customers
want to use the feature.


418
00:23:53.800 --> 00:23:57.704
Now, let's chat about
managing button states and appearance.


419
00:23:57.771 --> 00:23:59.940
The photo output can give
indicators of when it's ready


420
00:24:00.007 --> 00:24:02.176
to start the next capture,
or when it's processing,


421
00:24:02.242 --> 00:24:06.046
and you can update your photo
capture button appropriately.


422
00:24:06.113 --> 00:24:11.318
This is done via an enum of values called
AVCapturePhotoOutput CaptureReadiness.


423
00:24:11.385 --> 00:24:14.454
The photo output can be
in a "notRunning," "ready,"


424
00:24:14.521 --> 00:24:16.723
and three "not ready" states:


425
00:24:16.790 --> 00:24:21.261
"Momentarily," "waiting for capture,"
or "waiting for processing."


426
00:24:21.328 --> 00:24:24.932
The "not ready" enums indicate that
if you call capturePhoto with settings,


427
00:24:24.998 --> 00:24:28.902
you'll incur a longer wait time between
the capture and the photo delivery,


428
00:24:28.969 --> 00:24:33.440
increasing that shutter lag
I talked about previously.


429
00:24:33.507 --> 00:24:36.710
Your app can listen for this state change
by using a new class,


430
00:24:36.777 --> 00:24:39.980
AVCapturePhotoOutputReadinessCoordinator.


431
00:24:40.047 --> 00:24:42.816
This makes callbacks
to a delegate object you provide


432
00:24:42.883 --> 00:24:45.319
when the Photo Output's readiness changes.


433
00:24:45.385 --> 00:24:48.956
You can use this class even
if you don't use the Responsive Capture


434
00:24:49.022 --> 00:24:52.926
or Fast Capture Prioritization APIs.


435
00:24:52.993 --> 00:24:55.696
Here's how you can convey shutter
availability


436
00:24:55.762 --> 00:24:59.066
and modify button appearance
using the Readiness Coordinator


437
00:24:59.132 --> 00:25:01.201
and the Readiness enum.


438
00:25:01.268 --> 00:25:05.305
The app for our session turns off user
interaction events on the capture button


439
00:25:05.372 --> 00:25:09.142
when handling a "not ready" enum value
to prevent additional requests


440
00:25:09.209 --> 00:25:11.745
from getting enqueued inadvertently
with multiple taps,


441
00:25:11.812 --> 00:25:14.381
resulting in long shutter lag.


442
00:25:14.448 --> 00:25:16.984
After a tap,
and a capturePhoto with settings request


443
00:25:17.050 --> 00:25:20.120
has been enqueued,
the captureReadiness state goes


444
00:25:20.187 --> 00:25:24.725
between .ready and the
.notReadyMomentarily enum value.


445
00:25:24.791 --> 00:25:28.395
Flash captures hit the
.notReadyWaitingForCapture state.


446
00:25:28.462 --> 00:25:30.664
Until the flash fires,
the photo output hasn't even


447
00:25:30.731 --> 00:25:34.201
gotten frames from the sensor,
so the button is dimmed.


448
00:25:34.268 --> 00:25:39.473
Finally, if you only use zero shutter lag
and none of the other features this year,


449
00:25:39.540 --> 00:25:42.843
You might show a spinner while the
.notReadyWaitingForProcessing


450
00:25:42.910 --> 00:25:44.545
enum value is the current readiness,


451
00:25:44.611 --> 00:25:49.082
as each photo's capture
and processing is completing.


452
00:25:49.149 --> 00:25:53.120
So here's how you make use
of the readiness coordinator in code.


453
00:25:53.187 --> 00:25:55.355
First, create a readiness coordinator


454
00:25:55.422 --> 00:25:57.658
for the photo output,
and set an appropriate


455
00:25:57.724 --> 00:26:02.029
delegate object to receive
callbacks about the readiness state.


456
00:26:02.095 --> 00:26:03.764
Then, at the time of each capture,


457
00:26:03.830 --> 00:26:07.601
set up your photo settings
as you normally would.


458
00:26:07.668 --> 00:26:09.403
Then, tell the readiness coordinator


459
00:26:09.469 --> 00:26:14.441
to start tracking the capture request's
readiness state for those settings.


460
00:26:14.508 --> 00:26:18.712
And then call capturePhoto
on the photo output.


461
00:26:18.779 --> 00:26:20.380
The readiness coordinator will then call


462
00:26:20.447 --> 00:26:23.884
the captureReadinessDidChange
delegate callback.


463
00:26:23.951 --> 00:26:26.486
You update your capture button's state
and appearance


464
00:26:26.553 --> 00:26:29.756
based on the readiness enum value received
to give your customers


465
00:26:29.823 --> 00:26:34.261
the best feedback
on when they can capture next.


466
00:26:34.328 --> 00:26:37.130
Responsive Capture
and Fast Capture Prioritization APIs


467
00:26:37.197 --> 00:26:41.101
are available on iPhones with
A12 Bionic chip and newer,


468
00:26:41.168 --> 00:26:42.669
and the Readiness Coordinator


469
00:26:42.736 --> 00:26:47.574
is available wherever
AVCapturePhotoOutput is supported.


470
00:26:47.641 --> 00:26:50.544
And now I've got all of
the new features enabled in our app,


471
00:26:50.611 --> 00:26:54.615
with the most responsive camera experience
possible that also gives super-sharp,


472
00:26:54.681 --> 00:26:56.517
high-quality photos.


473
00:26:56.583 --> 00:26:59.953
But, you don't have to use
them all to get an improved experience.


474
00:27:00.020 --> 00:27:04.124
You can use just the ones
that are appropriate for your app.


475
00:27:04.191 --> 00:27:07.895
We'll finish up our session today with
updated video effects.


476
00:27:07.961 --> 00:27:10.364
Previously, Control Center on macOS


477
00:27:10.430 --> 00:27:12.566
provided options for camera
streaming features


478
00:27:12.633 --> 00:27:19.473
such as Center Stage, Portrait,
and Studio Light.


479
00:27:19.540 --> 00:27:21.141
With macOS Sonoma,


480
00:27:21.208 --> 00:27:25.245
we've moved video effects out
of Control Center and into its own menu.


481
00:27:25.312 --> 00:27:27.981
You'll see a preview
of your camera or screen share,


482
00:27:28.048 --> 00:27:32.719
and can enable Video Effects
like Portrait mode and Studio Light.


483
00:27:32.786 --> 00:27:36.356
The Portrait and Studio Light effects
are now adjustable in their intensity,


484
00:27:36.423 --> 00:27:39.793
and Studio Light
is available on more devices.


485
00:27:39.860 --> 00:27:43.230
And we have a new effect
type called "Reactions."


486
00:27:43.297 --> 00:27:45.265
When you're on a video call,
you might want


487
00:27:45.332 --> 00:27:49.403
to express that you love an idea,
or give a thumbs up about good news,


488
00:27:49.469 --> 00:27:52.706
all while letting the speaker
continue without interruption.


489
00:27:52.773 --> 00:27:56.043
Reactions seamlessly blends
your video with balloons,


490
00:27:56.109 --> 00:27:58.011
confetti, and more.


491
00:27:58.078 --> 00:28:00.881
Reactions follow the template
of portrait and studio light effects,


492
00:28:00.948 --> 00:28:03.717
where they're a system-level
camera feature,


493
00:28:03.784 --> 00:28:05.385
available out of the box,


494
00:28:05.452 --> 00:28:09.857
without any code changes needed
in your app.


495
00:28:09.923 --> 00:28:12.092
For more detail
on Portrait and Studio Light effects,


496
00:28:12.159 --> 00:28:17.097
check out the 2021 session,
"What's new in camera capture."


497
00:28:17.164 --> 00:28:20.334
We have three ways to
show reactions in the video stream


498
00:28:20.400 --> 00:28:22.769
First, you can click
on a reaction effect


499
00:28:22.836 --> 00:28:27.174
in the bottom pane in
the new Video Effects menu on macOS .


500
00:28:27.241 --> 00:28:30.210
Second, your app can call
AVCaptureDevice.performEffect


501
00:28:30.277 --> 00:28:32.980
for: a reaction type.
For example, maybe you have


502
00:28:33.046 --> 00:28:35.549
a set of reaction buttons
in one of your app's views


503
00:28:35.616 --> 00:28:39.453
that participants can click on
to perform a reaction.


504
00:28:39.520 --> 00:28:44.491
And third, when reactions are enabled,
they can be sent by making a gesture.


505
00:28:44.558 --> 00:28:47.561
Let's check this out.


506
00:28:47.628 --> 00:28:51.999
You can do thumbs up,


507
00:28:52.065 --> 00:28:54.001
thumbs down,


508
00:28:54.067 --> 00:28:57.971
fireworks with two thumbs up,


509
00:28:58.038 --> 00:29:00.541
heart,


510
00:29:00.607 --> 00:29:04.478
balloons with one victory sign,


511
00:29:04.545 --> 00:29:07.915
rain with two thumbs down,


512
00:29:07.981 --> 00:29:10.184
confetti with two victory signs,


513
00:29:10.250 --> 00:29:11.618
and my personal favorite,


514
00:29:11.685 --> 00:29:14.588
lasers, using two signs of the horns.


515
00:29:14.655 --> 00:29:18.559
Hey, that's a nice bunch of effects.


516
00:29:18.625 --> 00:29:20.894
You can check
for reaction effects support by looking


517
00:29:20.961 --> 00:29:23.297
at the reactionEffectsSupported
property on


518
00:29:23.363 --> 00:29:27.434
the AVCaptureDeviceFormat you want
to use in your capture session.


519
00:29:27.501 --> 00:29:29.703
There are properties
on AVCaptureDevice that you can


520
00:29:29.770 --> 00:29:33.473
read or key-value observe for
knowing when gesture recognition is on


521
00:29:33.540 --> 00:29:36.210
and when reaction
effects have been enabled.


522
00:29:36.276 --> 00:29:38.912
Remember, because these are under
the control of the user,


523
00:29:38.979 --> 00:29:42.282
your app can't turn them on or off.


524
00:29:42.349 --> 00:29:44.451
On iOS, it's the same idea.


525
00:29:44.518 --> 00:29:48.222
The participant goes to Control Center
to turn gesture recognition on or off,


526
00:29:48.288 --> 00:29:51.124
and you can key value observe
when this happens.


527
00:29:51.191 --> 00:29:54.061
However, to trigger effects
in your app on iOS,


528
00:29:54.127 --> 00:29:55.829
you’ll need to do it programmatically.


529
00:29:55.896 --> 00:29:59.199
So let's go
through how you can do that, now.


530
00:29:59.266 --> 00:30:01.201
When the "canPerformReactionEffects"


531
00:30:01.268 --> 00:30:03.604
property is true,
calling the performEffect


532
00:30:03.670 --> 00:30:07.341
for reactionType method will render
the reactions into the video feed.


533
00:30:07.407 --> 00:30:10.644
Your app should provide buttons
to trigger the effects.


534
00:30:10.711 --> 00:30:13.981
Reactions coming in via gestures
may be rendered in a different location


535
00:30:14.047 --> 00:30:17.050
in the video than they would
be when you call performEffect\


536
00:30:17.117 --> 00:30:19.887
depending on what cues
are used for the detection.


537
00:30:19.953 --> 00:30:22.956
We have a new enum
called AVCaptureReactionType


538
00:30:23.023 --> 00:30:24.791
for all of the different reaction effects,


539
00:30:24.858 --> 00:30:28.996
such as thumbs-up or balloons
that AVCaptureDevice will recognize


540
00:30:29.062 --> 00:30:32.900
in a capture session
and can render into the video content.


541
00:30:32.966 --> 00:30:35.469
And the
"AVCaptureDevice.availableReactionTypes"


542
00:30:35.536 --> 00:30:38.338
property returns a set
of AVCaptureReactionTypes


543
00:30:38.405 --> 00:30:40.941
based on the configured format
or session preset.


544
00:30:41.008 --> 00:30:44.211
These effects also
have built-in system UIImages


545
00:30:44.278 --> 00:30:47.514
available that you can place
in your own views.


546
00:30:47.581 --> 00:30:51.185
You can get the systemName
for a reaction from a new function


547
00:30:51.251 --> 00:30:56.657
AVCaptureReactionType.systemImageName
that takes in an AVCaptureReactionType


548
00:30:56.723 --> 00:31:02.496
and returns the appropriate string to use
with the UIImage systemName constructor.


549
00:31:02.563 --> 00:31:06.133
And we have API to tell
you when reaction effects are in progress,


550
00:31:06.200 --> 00:31:10.437
the aptly-named
AVCaptureDevice.reactionEffectsInProgress.


551
00:31:10.504 --> 00:31:13.240
When the user performs
multiple reaction effects in sequence,


552
00:31:13.307 --> 00:31:18.712
they may overlap briefly, so
this returns an array of status objects.


553
00:31:18.779 --> 00:31:22.549
You can use Key-value observing
to know when these begin and end.


554
00:31:22.616 --> 00:31:25.552
If you're a voice-over-IP conferencing
app, you can also use this


555
00:31:25.619 --> 00:31:28.922
information to send the metadata
about the effects to remote views,


556
00:31:28.989 --> 00:31:33.060
particularly when those callers have
video turned off for bandwidth reasons.


557
00:31:33.126 --> 00:31:35.462
For example,
you might to show an effect icon


558
00:31:35.529 --> 00:31:38.665
in their UI on behalf of another caller.


559
00:31:38.732 --> 00:31:41.602
Rendering effect animations
to the video stream may be


560
00:31:41.668 --> 00:31:43.704
challenging to a video encoder.


561
00:31:43.770 --> 00:31:46.073
They increase the complexity
of the content,


562
00:31:46.139 --> 00:31:49.843
and it may require a larger
bitrate budget to encode it.


563
00:31:49.910 --> 00:31:53.013
By Key-Value observing
reactionEffectsInProgress,


564
00:31:53.080 --> 00:31:57.351
you can make encoder adjustments
while the rendering is happening.


565
00:31:57.417 --> 00:32:00.053
If it's feasible for your app,
you can increase the bitrate


566
00:32:00.120 --> 00:32:02.356
of the encoder
while effects are rendering.


567
00:32:02.422 --> 00:32:07.661
Or if you're using the low-latency video
encoder through VideoToolbox


568
00:32:07.728 --> 00:32:12.966
and setting the MaxAllowedFrameQP
VTCompressionPropertyKey,


569
00:32:13.033 --> 00:32:18.105
then we encourage you to run tests in your
app using the various video configurations


570
00:32:18.172 --> 00:32:20.674
including resolutions, framerates,


571
00:32:20.741 --> 00:32:25.045
and bitrate tier supported and adjust the
max allowed FrameQP accordingly


572
00:32:25.112 --> 00:32:27.347
while the effects are in progress.


573
00:32:27.414 --> 00:32:31.451
Note that with a low MaxAllowedFrameQP
value, the framerate of effects


574
00:32:31.518 --> 00:32:35.889
can be compromised and you'll end up
with a low video framerate.


575
00:32:35.956 --> 00:32:38.892
The 2021 session
"Explore low-latency video encoding


576
00:32:38.959 --> 00:32:44.798
with VideoToolbox" has more great
information on working with this feature.


577
00:32:44.865 --> 00:32:46.834
You should also know
that the video frame rate


578
00:32:46.900 --> 00:32:49.736
may change when effects are in progress.


579
00:32:49.803 --> 00:32:52.673
For example,
if you've configured your AVCaptureSession


580
00:32:52.739 --> 00:32:54.641
to run at 60 frames per second,


581
00:32:54.708 --> 00:32:58.779
you'll get 60 frames per second
while effects aren't running.


582
00:32:58.846 --> 00:33:00.547
But while effects are in progress,


583
00:33:00.614 --> 00:33:04.551
you may get a different frame rate,
such as 30 frames per second.


584
00:33:04.618 --> 00:33:07.554
This follows the model of
Portrait and Studio Light effects


585
00:33:07.621 --> 00:33:11.525
where the end frame rate
may be lower than you specified.


586
00:33:11.592 --> 00:33:14.027
To see what that frame rate will be,
check out


587
00:33:14.094 --> 00:33:19.166
AVCaptureDeviceFormat.videoFrameRateRange
ForReactionEffectsInProgress


588
00:33:19.233 --> 00:33:22.369
for the format you're configuring
on the device.


589
00:33:22.436 --> 00:33:24.571
As with other
AVCaptureDeviceFormat properties,


590
00:33:24.638 --> 00:33:29.776
this is informational to your app,
rather than something you can control.


591
00:33:29.843 --> 00:33:33.247
On macOS and with tvOS apps using
Continuity Camera,


592
00:33:33.313 --> 00:33:35.649
reaction effects are always enabled.


593
00:33:35.716 --> 00:33:41.555
On iOS and iPad OS, applications can
opt-in via changes to their Info.plist.


594
00:33:41.622 --> 00:33:45.459
You opt in either by advertising
that you're a VoIP application category


595
00:33:45.526 --> 00:33:48.896
in your UIBackgroundModes array,
or by adding


596
00:33:48.962 --> 00:33:54.001
NSCameraReactionEffectsEnabled
with a value of YES.


597
00:33:54.067 --> 00:33:57.471
Reaction effects and gesture recognition
are available on iPhones


598
00:33:57.538 --> 00:34:02.276
and iPads with the A14 chip or newer,
such as iPhone 12,


599
00:34:02.342 --> 00:34:06.280
Apple Silicon Macs
and Intel Macs and Apple TVs


600
00:34:06.346 --> 00:34:08.615
using Continuity Camera devices,


601
00:34:08.682 --> 00:34:13.253
Apple Studio display attached
to a USB-C iPad or Apple Silicon Mac,


602
00:34:13.320 --> 00:34:18.492
and third party cameras attached
to a USB-C iPad or Apple Silicon Mac.


603
00:34:18.559 --> 00:34:20.494
And that wraps up
our session about responsive


604
00:34:20.561 --> 00:34:23.564
camera experiences
with new APIs this year.


605
00:34:23.630 --> 00:34:26.767
We talked about deferred photo processing,


606
00:34:26.834 --> 00:34:29.970
zero shutter lag,
and responsive capture APIs


607
00:34:30.037 --> 00:34:32.306
to give you new possibilities for making


608
00:34:32.372 --> 00:34:36.743
the most responsive photography
app with improved image quality


609
00:34:36.810 --> 00:34:40.247
and we also covered how
your users can really express themselves


610
00:34:40.314 --> 00:34:44.251
with updated video effects,
including new "reactions."


611
00:34:44.318 --> 00:34:48.522
I can't wait to see how you respond
to all the great new features.


612
00:34:48.589 --> 00:34:49.857
Thanks for watching.


613
00:34:49.923 --> 00:34:51.992
[upbeat music]

