2
00:00:00.334 --> 00:00:07.341 line:-1
♪ ♪


3
00:00:10.711 --> 00:00:14.414 line:-2
Pulkit: Hi, I am Pulkit,
and I am an engineer on the Core ML team.


4
00:00:14.448 --> 00:00:18.585 line:-2
I am excited to share several updates
that have been made to Core ML Tools.


5
00:00:18.619 --> 00:00:21.455 line:-2
These updates help you optimize
the size and performance


6
00:00:21.488 --> 00:00:23.757 line:-1
of your machine learning models.


7
00:00:23.790 --> 00:00:26.827 line:-2
With the capabilities of models
improving significantly,


8
00:00:26.860 --> 00:00:30.197 line:-2
more and more features are
being driven by machine learning.


9
00:00:30.230 --> 00:00:34.401 line:-2
As a result, the number of models
deployed in a single app is increasing.


10
00:00:34.434 --> 00:00:38.372 line:-2
Along with that, each model
in the app is also getting bigger,


11
00:00:38.405 --> 00:00:41.542 line:-2
putting an upward pressure
on the size of an app.


12
00:00:41.575 --> 00:00:45.445 line:-2
So, it's critical
to keep model size in check.


13
00:00:45.479 --> 00:00:48.749 line:-2
There are several benefits
of reducing model size.


14
00:00:48.782 --> 00:00:53.520 line:-2
You can ship more models in the same
memory budget if each model is smaller.


15
00:00:53.554 --> 00:00:57.925 line:-2
It can also enable you to ship bigger,
more capable models.


16
00:00:57.958 --> 00:01:00.694 line:-2
It can also help
make the model run faster.


17
00:01:00.727 --> 00:01:03.730 line:-2
This is because a smaller model
means less data


18
00:01:03.764 --> 00:01:06.466 line:-2
to move between the memory
and the processor.


19
00:01:06.500 --> 00:01:10.504 line:-2
So, it seems like reducing a model's size
is a great idea.


20
00:01:10.537 --> 00:01:12.206 line:-1
What makes a model large?


21
00:01:12.239 --> 00:01:15.409 line:-2
Let me walk through an example
to help you understand.


22
00:01:15.442 --> 00:01:19.179 line:-2
ResNet50 is a popular
image classification model.


23
00:01:19.213 --> 00:01:23.951 line:-2
Its first layer is a convolution layer
with about 9,000 parameters.


24
00:01:23.984 --> 00:01:28.789 line:-2
And it has 53 convolution layers
in total with varying sizes.


25
00:01:28,822 --> 00:01:33,727
At the end, it has a linear layer
with about 2.1 million parameters.


26
00:01:33,760 --> 00:01:37,231
This all adds up to 25 million parameters


27
00:01:37,264 --> 00:01:40,000
If I save the model
using Float16 precision,


28
00:01:40,033 --> 00:01:45,539
it uses 2 bytes per weight,
and I get a model of size 50 megabytes.


29
00:01:45,572 --> 00:01:49,176
A 50-megabyte model is large,
but when you get to some newer models


30
00:01:49,209 --> 00:01:53,280
like Stable Diffusion,
you end up with even larger models.


31
00:01:53.313 --> 00:01:57.284 line:-2
Now, let's talk about some paths
to getting a smaller model.


32
00:01:57.317 --> 00:02:00.587 line:-2
One way is to design
a more efficient model architecture


33
00:02:00.621 --> 00:02:05.025 line:-2
that can achieve good performance
with fewer and smaller weights.


34
00:02:05.058 --> 00:02:08.629 line:-2
Another way is to compress
the weights of your existing model.


35
00:02:08.662 --> 00:02:11.698 line:-2
This path of model compression
is what I will focus on.


36
00:02:11.732 --> 00:02:15.302 line:-2
I'll start by describing three
useful techniques for model compression.


37
00:02:15.335 --> 00:02:17.571 line:-1
Next, I'll demonstrate two workflows


38
00:02:17.604 --> 00:02:20.340 line:-2
that integrate
these model compression techniques.


39
00:02:20.374 --> 00:02:23.877 line:-2
I'll then illustrate
how the latest Core ML Tools helps you


40
00:02:23.911 --> 00:02:27.181 line:-2
in applying these techniques
and workflows to your models.


41
00:02:27.214 --> 00:02:30.417 line:-2
And lastly, Srijan will discuss
the impact of model compression


42
00:02:30.450 --> 00:02:32.719 line:-1
on runtime performance.


43
00:02:32.753 --> 00:02:34.988 line:-2
Let's start
with the compression techniques.


44
00:02:35.022 --> 00:02:38.158 line:-2
There are a couple of ways
to compress model weights.


45
00:02:38.192 --> 00:02:40.961 line:-2
The first way is to pack them
more efficiently


46
00:02:40.994 --> 00:02:44.331 line:-1
using a sparse matrix representation.


47
00:02:44.364 --> 00:02:48.202 line:-2
This can be achieved
by using a technique called pruning.


48
00:02:48.235 --> 00:02:52.005 line:-2
Another way is to reduce
the precision used to store the weights.


49
00:02:52,039 --> 00:02:57,144
This can be achieved either
by quantization or by palettization.


50
00:02:57,177 --> 00:02:59,346
Both of these strategies are lossy,


51
00:02:59,379 --> 00:03:02,683
and the compressed models are
typically slightly less accurate


52
00:03:02,716 --> 00:03:05,085
compared to their
uncompressed counterparts.


53
00:03:06.186 --> 00:03:08.889 line:-2
Let's now take a deeper look
at each of these techniques.


54
00:03:10.324 --> 00:03:13.126 line:-2
Weight pruning helps you
efficiently pack your model weights


55
00:03:13.160 --> 00:03:15.562 line:-1
with a sparse representation.


56
00:03:15.596 --> 00:03:17.931 line:-1
Sparsifying or pruning a weight matrix


57
00:03:17.965 --> 00:03:20.767 line:-2
means setting
some of the weight values to 0.


58
00:03:20.801 --> 00:03:22.903 line:-1
I start with a weight matrix.


59
00:03:22.936 --> 00:03:26.206 line:-2
To prune it, I can set
the smallest magnitude weights to 0.


60
00:03:27,975 --> 00:03:31,979
Now, I only need to store
the non-zero values.


61
00:03:32,012 --> 00:03:36,783
I end up saving about 2 bytes of storage
for every zero introduced.


62
00:03:36,817 --> 00:03:39,586
Of course, I will also need to store
the locations of zeros,


63
00:03:39,620 --> 00:03:43,190
to reconstruct the dense matrix later.


64
00:03:43,223 --> 00:03:47,961
Model size goes down linearly
with the level of sparsity introduced.


65
00:03:47,995 --> 00:03:52,933
A 50% sparse model means
50% of its weights are zero,


66
00:03:52,966 --> 00:03:56,937
and for a ResNet50 model,
it has a size of about 28 megabytes,


67
00:03:56,970 --> 00:03:59,973
which is approximately half
the Float16 size.


68
00:04:01.675 --> 00:04:04.811 line:-2
The second weight compression technique
is quantization,


69
00:04:04.845 --> 00:04:08.315 line:-2
which uses 8-bit precision
to store the weights.


70
00:04:08.348 --> 00:04:12.085 line:-2
To perform quantization,
you take the weight values


71
00:04:12,119 --> 00:04:17,691
and scale, shift, and round them
such that they lie in the INT8 range.


72
00:04:17,724 --> 00:04:20,928
In this example, the scale is 2.35,


73
00:04:20,961 --> 00:04:26,667
which maps the smallest value to -127,
and the bias is 0.


74
00:04:26,700 --> 00:04:30,003
Depending on the model,
a non-zero bias can also be used,


75
00:04:30,037 --> 00:04:33,841
which sometimes helps
in reducing quantization error.


76
00:04:33,874 --> 00:04:37,311
The scale and bias can later be used
to de-quantize the weights


77
00:04:37,344 --> 00:04:40,314
to bring them back
to their original range.


78
00:04:40.347 --> 00:04:43.217 line:-2
To reduce the weight precision
below 8 bits,


79
00:04:43.250 --> 00:04:47.221 line:-2
you can use a technique
called weight clustering or palettization.


80
00:04:47.254 --> 00:04:49.690 line:-2
In this technique,
weights with similar values


81
00:04:49.723 --> 00:04:51.725 line:-1
are grouped together and represented


82
00:04:51.758 --> 00:04:55.062 line:-2
using the value of the cluster centroid
they belong to.


83
00:04:55.095 --> 00:04:58.165 line:-2
These centroids are stored
in a look up table.


84
00:04:58.198 --> 00:05:01.568 line:-2
And the original weight matrix
is converted to an index table,


85
00:05:01.602 --> 00:05:05.439 line:-2
where each element points
to the corresponding cluster center.


86
00:05:05.472 --> 00:05:08.175 line:-2
In this example,
since I have four clusters,


87
00:05:08.208 --> 00:05:11.378 line:-2
I am able to represent each weight
using 2 bits,


88
00:05:11,411 --> 00:05:14,448
achieving 8x compression over Float16.


89
00:05:14,481 --> 00:05:17,918
The number of unique cluster centers
that can be used to represent the weights


90
00:05:17,951 --> 00:05:20,287
is equal to 2 to the power of n,


91
00:05:20,320 --> 00:05:23,891
where n is the precision
used for palettization.


92
00:05:23,924 --> 00:05:27,861
So 4-bit palettization means
you can have 16 clusters.


93
00:05:27,895 --> 00:05:30,697
While quantization reduces
your model size by half,


94
00:05:30,731 --> 00:05:34,134
palettization can help you make it
up to 8 times smaller.


95
00:05:35.435 --> 00:05:40.340 line:-2
To summarize, there are three different
techniques for weight compression.


96
00:05:40.374 --> 00:05:44.711 line:-2
Each of them uses a different way
to represent the weights.


97
00:05:44.745 --> 00:05:47.447 line:-2
They offer varying levels of compression,
which can be controlled


98
00:05:47.481 --> 00:05:49.349 line:-1
by their respective parameters,


99
00:05:49.383 --> 00:05:51.251 line:-1
like the amount of sparsity for pruning


100
00:05:51.285 --> 00:05:53.887 line:-1
and the number of bits for palettization.


101
00:05:53.921 --> 00:05:56.723 line:-2
Now, I will illustrate
how you can integrate these techniques


102
00:05:56.757 --> 00:05:58.625 line:-1
in your model development workflow.


103
00:05:58.659 --> 00:06:02.729 line:-2
Let's first start with the workflow
for Core ML model conversion.


104
00:06:02.763 --> 00:06:06.967 line:-2
You may start by training a model with
your favorite python training framework


105
00:06:07.000 --> 00:06:11.004 line:-2
and then use Core ML Tools
to convert that model to Core ML.


106
00:06:11,038 --> 00:06:14,241
This workflow can be extended
one step further


107
00:06:14,274 --> 00:06:17,311
to become a post-training
compression workflow.


108
00:06:17,344 --> 00:06:21,281
To do that, you add a compression step
which operates on the already trained


109
00:06:21,315 --> 00:06:25,419
and converted model weights
in order to reduce the overall size.


110
00:06:25,452 --> 00:06:28,155
Note that this workflow
can start at any point.


111
00:06:28,188 --> 00:06:31,158
For example,
you may start with a pre-trained model


112
00:06:31,191 --> 00:06:35,863
with no need for training data
or an already converted Core ML model.


113
00:06:36,964 --> 00:06:39,399
When applying this workflow,
you will have an option


114
00:06:39,433 --> 00:06:41,368
to choose the amount
of compression applied.


115
00:06:41.401 --> 00:06:44.972 line:-2
The more compression you apply,
the smaller your resulting model will be,


116
00:06:45.005 --> 00:06:48.475 line:-2
but as one may expect,
there are some tradeoffs.


117
00:06:48,509 --> 00:06:51,612
Specifically, you will be starting
with an uncompressed model


118
00:06:51,645 --> 00:06:54,615
that achieves a certain accuracy.


119
00:06:54,648 --> 00:06:58,018
As you apply some compression,
your model size will reduce,


120
00:06:58,051 --> 00:07:01,121
but it may also impact your accuracy.


121
00:07:01,154 --> 00:07:04,791
As you apply more compression,
this impact may become more prominent


122
00:07:04,825 --> 00:07:07,494
and the accuracy loss
may become unacceptable.


123
00:07:09,096 --> 00:07:12,933
This trend and the acceptable tradeoff
will be different for each use case


124
00:07:12,966 --> 00:07:15,469
and it is model- and dataset-dependent.


125
00:07:15.502 --> 00:07:17.237 line:-1
To see this tradeoff in practice,


126
00:07:17.271 --> 00:07:21.108 line:-2
let's look at a model
that segments objects in an image.


127
00:07:21.141 --> 00:07:23.143 line:-2
For my image,
the model returns probability


128
00:07:23.177 --> 00:07:26.046 line:-1
of each pixel belonging to the sofa.


129
00:07:26.079 --> 00:07:29.349 line:-2
The baseline Float16 model
segments the object very well.


130
00:07:29.383 --> 00:07:33.654 line:-2
For a 10% pruned model, the output is
very similar to the base model.


131
00:07:33.687 --> 00:07:37.491 line:-1
Artifacts start appearing at 30% sparsity


132
00:07:37.524 --> 00:07:40.360 line:-1
and increase with higher levels.


133
00:07:40.394 --> 00:07:43.830 line:-2
Once I get to 40% pruning,
the model completely breaks down,


134
00:07:43.864 --> 00:07:47.401 line:-2
and the probability map
becomes unrecognizable.


135
00:07:47.434 --> 00:07:51.638 line:-2
Similarly, 8-bit quantization
and 6-bit palettization


136
00:07:51.672 --> 00:07:53.941 line:-1
preserve the base model's output.


137
00:07:53.974 --> 00:07:58.078 line:-2
At 4-bit palettization,
you start seeing some artifacts,


138
00:07:58.111 --> 00:08:03.016 line:-2
and at 2-bit palettization, the model
fails to segment the object altogether.


139
00:08:03.050 --> 00:08:05.118 line:-2
To overcome this degradation
in model performance


140
00:08:05.152 --> 00:08:08.322 line:-2
at higher compression rates,
you can use a different workflow.


141
00:08:08.355 --> 00:08:11.625 line:-2
This workflow is called
training time compression.


142
00:08:11.658 --> 00:08:14.595 line:-2
Here, you fine-tune your model
on some data


143
00:08:14.628 --> 00:08:16.763 line:-1
while compressing the weights.


144
00:08:16.797 --> 00:08:20.167 line:-2
Compression is introduced gradually
and in a differentiable manner


145
00:08:20.200 --> 00:08:24.705 line:-2
to allow the weights to readjust
to the new constraints imposed on them.


146
00:08:24,738 --> 00:08:27,641
Once your model achieves
a satisfactory accuracy,


147
00:08:27,674 --> 00:08:31,645
you can convert it and get
a compressed Core ML model.


148
00:08:31,678 --> 00:08:34,414
Note that you may either incorporate
training time compression


149
00:08:34,448 --> 00:08:39,786
in your existing model training workflow
or start with a pre-trained model.


150
00:08:39,820 --> 00:08:43,991
Training time compression improves
the tradeoff between model accuracy


151
00:08:44,024 --> 00:08:46,860
and the amount of compression,
allowing you to maintain


152
00:08:46,894 --> 00:08:50,163
the same model performance
at higher compression rates.


153
00:08:50.197 --> 00:08:53.300 line:-2
Let's look at the same
image segmentation model again.


154
00:08:53.333 --> 00:08:56.570 line:-2
For training time pruning,
the model output is unaltered


155
00:08:56.603 --> 00:08:58.605 line:-1
up to 40% sparsity.


156
00:08:58.639 --> 00:09:02.142 line:-2
This is where
the post-training accuracy broke down.


157
00:09:02.176 --> 00:09:06.680 line:-1
In fact, now even at 50% and 75% sparsity,


158
00:09:06.713 --> 00:09:10.784 line:-2
the model achieves a similar
probability map as the base model.


159
00:09:10.817 --> 00:09:13.120 line:-2
It's at 90% sparsity
that you start observing


160
00:09:13.153 --> 00:09:16.223 line:-2
a significant degradation
in model accuracy.


161
00:09:16.256 --> 00:09:19.660 line:-2
Similarly, training time quantization
and palettization


162
00:09:19.693 --> 00:09:21.929 line:-1
also preserve the baseline model's output,


163
00:09:21.962 --> 00:09:24.665 line:-2
even up to 2 bits of compression
in this case.


164
00:09:24.698 --> 00:09:27.267 line:-1
To recap, you can apply weight compression


165
00:09:27.301 --> 00:09:31.772 line:-2
either during model conversion
or during model training.


166
00:09:31.805 --> 00:09:33.774 line:-2
The latter provides
better accuracy tradeoff


167
00:09:33.807 --> 00:09:36.643 line:-1
at the cost of longer training time.


168
00:09:36.677 --> 00:09:39.680 line:-2
Because the second workflow applies
compression during training,


169
00:09:39.713 --> 00:09:42.115 line:-2
we also need to use
differentiable operations


170
00:09:42.149 --> 00:09:44.418 line:-1
to implement the compression algorithms.


171
00:09:44.451 --> 00:09:48.021 line:-2
Let's now explore how these
compression workflows can be executed


172
00:09:48.055 --> 00:09:49.823 line:-1
with Core ML Tools.


173
00:09:49.857 --> 00:09:53.927 line:-2
Post-training model compression APIs
have been available in Core ML Tools 6


174
00:09:53.961 --> 00:09:57.030 line:-2
for pruning, palettization,
and quantization


175
00:09:57.064 --> 00:09:59.533 line:-1
under the compression utils submodule.


176
00:09:59.566 --> 00:10:03.737 line:-2
However, there were no APIs
for training time compression.


177
00:10:03.770 --> 00:10:06.373 line:-2
With Core ML Tools 7,
new APIs have been added


178
00:10:06.406 --> 00:10:10.410 line:-2
to provide capabilities
for training time compression as well.


179
00:10:10.444 --> 00:10:14.848 line:-2
And we have consolidated the older APIs
and the new ones under a single module


180
00:10:14.882 --> 00:10:17.284 line:-1
called coremltools.optimize.


181
00:10:17.317 --> 00:10:20.254 line:-2
The post-training compression APIs
have been migrated


182
00:10:20.287 --> 00:10:23.390 line:-1
under coremltools.optimize.coreml,


183
00:10:23.423 --> 00:10:25.726 line:-2
and the new training time APIs
are available


184
00:10:25.759 --> 00:10:28.795 line:-1
under coremltools.optimize.torch.


185
00:10:28.829 --> 00:10:31.598 line:-1
The latter work with PyTorch models.


186
00:10:31.632 --> 00:10:35.836 line:-2
Let's first take a closer look
at the post-training APIs.


187
00:10:35.869 --> 00:10:40.641 line:-2
In the post-training compression workflow,
the input is a Core ML model.


188
00:10:40.674 --> 00:10:45.345 line:-2
It can be updated by the three methods
available in the optimize.coreml module,


189
00:10:45.379 --> 00:10:49.049 line:-2
which apply each of the three compression
techniques that I have described.


190
00:10:49.082 --> 00:10:50.784 line:-1
To use these methods,


191
00:10:50.817 --> 00:10:54.421 line:-2
you start by creating
an OptimizationConfig object,


192
00:10:54.454 --> 00:10:57.191 line:-2
describing how you want
to compress the model.


193
00:10:57.224 --> 00:11:02.396 line:-2
Here, I am doing magnitude pruning
with 75% target sparsity.


194
00:11:02.429 --> 00:11:05.532 line:-2
Once the config is defined,
you can use the prune_weights method


195
00:11:05.566 --> 00:11:07.167 line:-1
to prune the model.


196
00:11:07.201 --> 00:11:10.437 line:-2
It's a simple, one-step process
to get a compressed model.


197
00:11:10.470 --> 00:11:14.007 line:-2
You can use similar APIs for palettizing
and quantizing the weights


198
00:11:14.041 --> 00:11:17.211 line:-2
using configs
specific to those techniques.


199
00:11:17.244 --> 00:11:20.380 line:-2
Let's consider the training time
compression workflow now.


200
00:11:20.414 --> 00:11:24.885 line:-2
In this case, as I described earlier,
you need a trainable model and data.


201
00:11:24.918 --> 00:11:28.555 line:-2
More specifically,
to compress the model with Core ML Tools,


202
00:11:28.589 --> 00:11:32.659 line:-2
you start with a PyTorch model,
likely with pre-trained weights.


203
00:11:32.693 --> 00:11:37.497 line:-2
Then use one of the available APIs
in the optimize.torch module to update it


204
00:11:37.531 --> 00:11:41.802 line:-2
and get a new PyTorch model
with compression layers inserted in it.


205
00:11:41.835 --> 00:11:46.807 line:-2
And then fine-tune it, using the data
and the original PyTorch training code.


206
00:11:46.840 --> 00:11:51.478 line:-2
This is the step where weights will
get adjusted to allow for compression.


207
00:11:51.512 --> 00:11:53.714 line:-2
And you can do this step
on your MacBook locally,


208
00:11:53.747 --> 00:11:56.717 line:-1
using the MPS PyTorch backend.


209
00:11:56.750 --> 00:11:59.286 line:-2
Once the model gets trained
to regain accuracy,


210
00:11:59.319 --> 00:12:01.688 line:-1
convert it to get a Core ML model.


211
00:12:01.722 --> 00:12:04.691 line:-2
Let's explore this further
through a code example.


212
00:12:04.725 --> 00:12:06.627 line:-2
I am starting
with the PyTorch code required


213
00:12:06.660 --> 00:12:09.763 line:-1
to fine-tune the model I want to compress.


214
00:12:09.796 --> 00:12:12.966 line:-2
You can easily leverage Core ML Tools
to add training time pruning


215
00:12:13.000 --> 00:12:15.235 line:-1
by adding just a few lines of code.


216
00:12:15.269 --> 00:12:18.238 line:-2
You first create
a MagnitudePrunerConfig object


217
00:12:18.272 --> 00:12:20.607 line:-2
describing how you want
to prune the model.


218
00:12:20.641 --> 00:12:24.211 line:-2
Here, I am setting
the target sparsity to 75%.


219
00:12:24.244 --> 00:12:26.680 line:-2
You can also write the config
in a yaml file


220
00:12:26.713 --> 00:12:29.616 line:-1
and load it using the from_yaml method.


221
00:12:29.650 --> 00:12:33.086 line:-2
Then, you create a pruner object
with the model you want to compress


222
00:12:33.120 --> 00:12:34.821 line:-1
and the config you just created.


223
00:12:34.855 --> 00:12:38.692 line:-2
Next, you call prepare
to insert pruning layers in the model.


224
00:12:38,725 --> 00:12:41,295
While fine-tuning the model,
you call step API


225
00:12:41,328 --> 00:12:43,664
to update the pruner's internal state.


226
00:12:43,697 --> 00:12:45,799
At the end of the training,
you call finalize


227
00:12:45,832 --> 00:12:48,435
to fold the pruning masks
into the weights.


228
00:12:48,468 --> 00:12:52,372
This model can then be converted
to Core ML using the conversion APIs.


229
00:12:52.406 --> 00:12:57.077 line:-2
The same workflow can be used for
quantization and palettization as well.


230
00:12:57.110 --> 00:13:01.348 line:-2
Now, Srijan will walk you through a demo
showing how you can use


231
00:13:01.381 --> 00:13:05.152 line:-2
Core ML Tools APIs
to palettize an object detection model.


232
00:13:05.185 --> 00:13:06.887 line:-1
Srijan: Thank you, Pulkit.


233
00:13:06.920 --> 00:13:10.591 line:-2
My name is Srijan, and I will be
walking you through a demo


234
00:13:10.624 --> 00:13:13.627 line:-1
of the Core ML Tools optimize API.


235
00:13:13.660 --> 00:13:16.230 line:-1
I will be using an SSD model


236
00:13:16.263 --> 00:13:21.034 line:-2
with a ResNet18 backbone
to detect people in images.


237
00:13:21.068 --> 00:13:25.672 line:-2
Let's first import
some basic model and training utilities.


238
00:13:25.706 --> 00:13:28.742 line:-2
I am going to start
with getting an instance


239
00:13:28.775 --> 00:13:32.946 line:-2
of the SSD ResNet18 model
I just talked about.


240
00:13:32.980 --> 00:13:36.583 line:-2
To simplify things,
I will just call the pre-written


241
00:13:36.617 --> 00:13:40.587 line:-1
get_ssd_model utility for that.


242
00:13:40.621 --> 00:13:44.691 line:-2
Now that the model is loaded,
let's train it for a few epochs.


243
00:13:44.725 --> 00:13:49.530 line:-2
Since it's an object detection model,
the goal of training would be


244
00:13:49.563 --> 00:13:53.300 line:-2
to reduce the SSD loss
of the detection task.


245
00:13:53.333 --> 00:13:58.071 line:-2
For conciseness, the train_epoch utility
encapsulates the code


246
00:13:58.105 --> 00:14:01.375 line:-1
required to train the model for an epoch,


247
00:14:01.408 --> 00:14:04.378 line:-2
like calling the forward
through different batches,


248
00:14:04.411 --> 00:14:08.182 line:-2
calculating the loss,
and performing gradient descent.


249
00:14:08.215 --> 00:14:12.386 line:-2
During training,
the SSD loss seems to be coming down.


250
00:14:12.419 --> 00:14:16.523 line:-2
I will now convert the model
into a Core ML model.


251
00:14:16.557 --> 00:14:19.259 line:-1
To do that, I will first trace the model


252
00:14:19.293 --> 00:14:23.997 line:-1
and then call the coremltools.convert API.


253
00:14:24.031 --> 00:14:28.335 line:-2
Let's call an imported utility
to check out the size of the model.


254
00:14:30.504 --> 00:14:34.441 line:-1
The size of the model is 23.6 megabytes.


255
00:14:34.474 --> 00:14:38.245 line:-2
Now, I will run predictions
on the Core ML model.


256
00:14:38.278 --> 00:14:41.882 line:-2
I have chosen an image of myself
from my London trip


257
00:14:41.915 --> 00:14:45.352 line:-2
as well as another image
to test the detections.


258
00:14:45.385 --> 00:14:51.391 line:-2
The confidence threshold for the model
to detect an object is set to 30%


259
00:14:51.425 --> 00:14:56.763 line:-2
so it would only plot boxes
for which it is at least 30% confident


260
00:14:56.797 --> 00:14:58.799 line:-1
of the object being present.


261
00:15:05.572 --> 00:15:07.875 line:-1
That detection seems spot on.


262
00:15:07.908 --> 00:15:12.312 line:-2
I am now curious to see
if I can reduce the size of this model.


263
00:15:12.346 --> 00:15:16.416 line:-2
I am going to try out
post-training palettization first.


264
00:15:16,450 --> 00:15:20,020
For that, I'll import some config classes


265
00:15:20,053 --> 00:15:24,758
and methods
from coremltools.optimize.coreml.


266
00:15:26,093 --> 00:15:30,464
I am now going to palettize
the weights of the model with 6 bits.


267
00:15:30,497 --> 00:15:34,768
For that,
I'll create an OpPalettizerConfig object,


268
00:15:34,801 --> 00:15:38,705
specifying mode as kmeans and nbits as 6.


269
00:15:38.739 --> 00:15:42.743 line:-2
This will specify parameters
at an op level,


270
00:15:42.776 --> 00:15:45.846 line:-1
and I can palettize each op differently.


271
00:15:45.879 --> 00:15:51.652 line:-2
However, right now, I'm gonna apply
the same 6-bit mode to all the ops.


272
00:15:51.685 --> 00:15:55.222 line:-2
I'll do that by defining
OptimizationConfig


273
00:15:55.255 --> 00:15:59.126 line:-2
and pass this op_config
as a global parameter to it.


274
00:16:00,327 --> 00:16:05,465
The optimization config is then passed
to the palettize_weights method


275
00:16:05,499 --> 00:16:10,137
along with the converted model
to get a palettized model.


276
00:16:10,170 --> 00:16:12,840
Let's see
what the size got reduced to now.


277
00:16:15,909 --> 00:16:20,113
The size of the model has gone down
to around 9 megabytes,


278
00:16:20,147 --> 00:16:22,816
but did it affect the performance
on test images?


279
00:16:22,850 --> 00:16:24,751
Let's find out.


280
00:16:24,785 --> 00:16:27,821
Wow, the detection still works well.


281
00:16:27,855 --> 00:16:31,725
I am really excited to push my luck
to trying out


282
00:16:31,758 --> 00:16:34,995
2-bit post-training palettization now.


283
00:16:35,028 --> 00:16:40,434
Doing that is as simple
as just changing nbits from 6 to 2


284
00:16:40,467 --> 00:16:42,536
in the OpPalettizerConfig


285
00:16:42,569 --> 00:16:45,572
and running
the palettize_weights API again.


286
00:16:48,342 --> 00:16:53,680
Let's use the utilities to see the size
and performance of this Core ML model.


287
00:16:57.618 --> 00:17:03.790 line:-2
As expected, the model's size has reduced
and has come down to around 3 megabytes.


288
00:17:03.824 --> 00:17:06.693 line:-1
The performance, however, is suboptimal


289
00:17:06.727 --> 00:17:11.565 line:-2
as the model isn't able to detect people
in both the images.


290
00:17:11.598 --> 00:17:15.869 line:-2
No boxes show up in the prediction,
as none of the boxes


291
00:17:15.903 --> 00:17:19.306 line:-2
predicted by the model
have confidence probability


292
00:17:19.339 --> 00:17:21.775 line:-1
above the 30% threshold.


293
00:17:21.808 --> 00:17:25.579 line:-2
Let's try out
2-bit training time palettization


294
00:17:25.612 --> 00:17:27.447 line:-1
to see if that performs better.


295
00:17:28,582 --> 00:17:32,152
I will start by importing
DKMPalettizerConfig


296
00:17:32,186 --> 00:17:34,087
and DKMPalettizer


297
00:17:34,121 --> 00:17:38,125
from coremltools.optimize.torch
to do that.


298
00:17:38,158 --> 00:17:42,095
DKM is an algorithm
to learn weight clusters


299
00:17:42,129 --> 00:17:48,202
by performing an attention-based
differentiable kmeans operation on them.


300
00:17:48,235 --> 00:17:52,239
Now it's time to define
the palettization config.


301
00:17:52,272 --> 00:17:57,477
Just need to simply specify n_bits as 2
in the global_config,


302
00:17:57,511 --> 00:18:01,481
and all supported modules
would be 2-bit palettized.


303
00:18:01,515 --> 00:18:04,084
And here, I'll create a palettizer object


304
00:18:04,117 --> 00:18:07,354
from the model and the config.


305
00:18:07,387 --> 00:18:09,990
Calling the prepare API now would insert


306
00:18:10,023 --> 00:18:13,760
palettization-friendly modules
into the model.


307
00:18:13,794 --> 00:18:17,898
Time to fine-tune the model
for a few epochs.


308
00:18:17,931 --> 00:18:22,536
Now that the model has been fine-tuned,
I will call the finalize API


309
00:18:22,569 --> 00:18:26,373
which would restore the palettized weights
as weights of the model,


310
00:18:26,406 --> 00:18:29,009
thus completing the process.


311
00:18:29,042 --> 00:18:32,546
The next step is to check out
the size of the model.


312
00:18:32,579 --> 00:18:38,252
For that, I will convert
the torch model into a Core ML model.


313
00:18:38,285 --> 00:18:42,890
Let's start by tracing the model
using torch.jit.trace.


314
00:18:42,923 --> 00:18:47,494
I will now call the convert API,
and this time, I will use


315
00:18:47,528 --> 00:18:50,397
an additional flag called PassPipeline


316
00:18:50,430 --> 00:18:53,800
and set its value
to DEFAULT_PALETTIZATION.


317
00:18:53,834 --> 00:18:59,072
This will indicate to the converter
to use a palettized representation


318
00:18:59,106 --> 00:19:01,108
for the converted weights.


319
00:19:03,677 --> 00:19:08,215
Let's see the size of the model
and its performance on test images.


320
00:19:08,248 --> 00:19:11,385
I can see that the model
I training-time palettized


321
00:19:11.418 --> 00:19:13.620 line:-1
is around 3 megabytes as well,


322
00:19:13.654 --> 00:19:16.323 line:-1
which comes down to 8x compression,


323
00:19:16.356 --> 00:19:18.926 line:-2
but unlike
the post-training palettized model,


324
00:19:18.959 --> 00:19:23.697 line:-2
this model is performing the detection
on test images correctly.


325
00:19:23.730 --> 00:19:29.536 line:-2
Since this was a demo, I just tested
model performance on two sample images.


326
00:19:29.570 --> 00:19:33.006 line:-2
In a real-world scenario,
I would use a metric


327
00:19:33.040 --> 00:19:37.377 line:-2
like mean average precision
and evaluate on a validation data set.


328
00:19:38.412 --> 00:19:40.047 line:-1
Let's recap.


329
00:19:40.080 --> 00:19:42.416 line:-1
I started with a trained model


330
00:19:42.449 --> 00:19:46.086 line:-2
and converted it
to get a 23.6-megabyte model


331
00:19:46.119 --> 00:19:48.021 line:-1
with Float16 weights.


332
00:19:48,055 --> 00:19:53,660
Then, I used the palettize_weights API
to quickly get a smaller model


333
00:19:53,694 --> 00:19:57,431
with 6-bit weights,
which did perform well on my data.


334
00:19:57,464 --> 00:20:01,001
However, when I pushed it further
to 2 bits,


335
00:20:01,034 --> 00:20:04,037
it showed a clear drop in performance.


336
00:20:04,071 --> 00:20:09,676
Post this, I updated the torch model
with the optimize.torch APIs


337
00:20:09,710 --> 00:20:12,813
and used
the differentiable kmeans algorithm


338
00:20:12,846 --> 00:20:15,382
to fine-tune for a few epochs.


339
00:20:15,415 --> 00:20:19,086
With that, I was able to get good accuracy


340
00:20:19,119 --> 00:20:22,189
with the 2-bit compression option.


341
00:20:22.222 --> 00:20:28.061 line:-2
While the demo employed a specific model
and optimization algorithm combination,


342
00:20:28.095 --> 00:20:31.231 line:-2
this workflow will generalize
to your use case


343
00:20:31.265 --> 00:20:33.767 line:-2
and will help you
in figuring out the tradeoff


344
00:20:33.800 --> 00:20:36.803 line:-2
between the amount of compression
you desire


345
00:20:36.837 --> 00:20:40.707 line:-2
and the time and data required
to retrain the model.


346
00:20:40.741 --> 00:20:44.344 line:-2
This brings us to our last topic,
performance.


347
00:20:44.378 --> 00:20:47.614 line:-2
I would like to briefly touch
upon the improvements


348
00:20:47.648 --> 00:20:50.184 line:-1
that have been made to the Core ML runtime


349
00:20:50.217 --> 00:20:55.556 line:-2
to execute such models more efficiently
when deployed in your app.


350
00:20:55.589 --> 00:20:58.025 line:-1
Let's look at a few key differences


351
00:20:58.058 --> 00:21:02.496 line:-1
between the runtime in iOS 16 and iOS 17.


352
00:21:02.529 --> 00:21:07.801 line:-2
While in iOS 16, there was support
for weight-only compressed models,


353
00:21:07.835 --> 00:21:12.439 line:-2
in iOS 17,
8-bit activation quantized models


354
00:21:12.472 --> 00:21:14.842 line:-1
can also be executed.


355
00:21:14,875 --> 00:21:18,779
In iOS 16, a weight compressed model runs


356
00:21:18,812 --> 00:21:22,916
at the same speed as the corresponding
model with float weights,


357
00:21:22,950 --> 00:21:27,855
while in iOS 17,
the Core ML runtime has been updated,


358
00:21:27,888 --> 00:21:31,592
and now compressed models run faster
in certain scenarios.


359
00:21:31.625 --> 00:21:36.563 line:-2
Similar runtime improvements are available
in newer versions


360
00:21:36.597 --> 00:21:40.801 line:-1
of macOS, tvOS, and watchOS as well.


361
00:21:40.834 --> 00:21:43.737 line:-1
But how are these improvements achieved?


362
00:21:43.770 --> 00:21:47.441 line:-2
In models,
where only the weights are compressed,


363
00:21:47.474 --> 00:21:51.144 line:-2
since the activations are
in floating point precision,


364
00:21:51.178 --> 00:21:54.615 line:-1
before an operation such as a convolution


365
00:21:54.648 --> 00:21:59.987 line:-2
or a matrix multiplication can happen,
the weight values need to be decompressed


366
00:22:00.020 --> 00:22:03.490 line:-1
to match the precision of the other input.


367
00:22:03.524 --> 00:22:07.861 line:-2
This step of decompression takes place
ahead of time


368
00:22:07.895 --> 00:22:10.264 line:-1
in the iOS 16 runtime.


369
00:22:10,297 --> 00:22:14,301
Hence, in this case,
the model is converted


370
00:22:14,334 --> 00:22:19,306
to a fully float precision model
in memory prior to execution.


371
00:22:19,339 --> 00:22:23,277
Hence, no change is observed
in inference latency.


372
00:22:23,310 --> 00:22:25,979
However, in iOS 17,


373
00:22:26,013 --> 00:22:29,516
in certain scenarios,
the weights are decompressed


374
00:22:29,550 --> 00:22:33,854
just in time,
just before the operation is executed.


375
00:22:33,887 --> 00:22:39,293
This has the advantage of loading
smaller bit weights from the memory


376
00:22:39,326 --> 00:22:44,131
at the cost of doing decompression
in every inference call.


377
00:22:44,164 --> 00:22:48,368
For certain compute units,
such as the Neural Engine,


378
00:22:48,402 --> 00:22:51,972
and certain types of models
that are memory bound,


379
00:22:52,005 --> 00:22:55,008
this could lead to inference gains.


380
00:22:55.042 --> 00:23:00.647 line:-2
To illustrate these runtime benefits,
I selected and profiled a few models


381
00:23:00.681 --> 00:23:05.552 line:-2
and plotted the relative amount
by which their inference is sped up


382
00:23:05.586 --> 00:23:07.955 line:-1
compared to their Float16 variant.


383
00:23:08,789 --> 00:23:14,695
As expected, the amount of speedup
is model- and hardware-dependent.


384
00:23:14,728 --> 00:23:19,233
These are the range of speedups
for 4-bit palettized models


385
00:23:19,266 --> 00:23:21,935
on iPhone 14 Pro Max.


386
00:23:21,969 --> 00:23:27,374
The improvements vary between 5% to 30%.


387
00:23:27,407 --> 00:23:33,180
For sparse models too, there are
varying improvements based on model type,


388
00:23:33,213 --> 00:23:36,950
with some models running 75% faster


389
00:23:36,984 --> 00:23:39,386
than their Float16 variants.


390
00:23:39.419 --> 00:23:43.757 line:-2
The question now arises:
what is the strategy


391
00:23:43.790 --> 00:23:46.426 line:-1
to get the best latency performance?


392
00:23:46.460 --> 00:23:50.163 line:-1
That would be to start with a float model


393
00:23:50.197 --> 00:23:53.333 line:-1
and use the optimize.coreml APIs


394
00:23:53.367 --> 00:23:56.904 line:-2
to explore various representations
of the model.


395
00:23:56.937 --> 00:23:58.438 line:-1
This would be quick,


396
00:23:58.472 --> 00:24:01.975 line:-2
as it doesn't require
retraining the model.


397
00:24:02.009 --> 00:24:05.913 line:-2
Then, profile it
on the device of your interest.


398
00:24:05.946 --> 00:24:10.717 line:-2
For this, Core ML performance reports
in Xcode will give you


399
00:24:10.751 --> 00:24:16.089 line:-2
a lot of visibility into inference,
including where operations run.


400
00:24:16.123 --> 00:24:19.493 line:-2
Then, shortlist
based on which configurations


401
00:24:19.526 --> 00:24:21.695 line:-1
give you the best gains.


402
00:24:21.728 --> 00:24:25.899 line:-2
After this,
you can focus on evaluating accuracy


403
00:24:25.933 --> 00:24:31.071 line:-2
and trying to improve, which may require
applying some training time compression


404
00:24:31,104 --> 00:24:33,173
with torch and Core ML Tools


405
00:24:33,207 --> 00:24:35,509
before finalizing your model.


406
00:24:36.643 --> 00:24:41.114 line:-2
To summarize, it is important
to reduce the size of the models,


407
00:24:41.148 --> 00:24:44.418 line:-2
and now you can do that
more easily than ever


408
00:24:44.451 --> 00:24:46.587 line:-1
with the new Core ML Tools APIs


409
00:24:46.620 --> 00:24:50.424 line:-2
and achieve lower memory footprint
and inference speedups.


410
00:24:50.457 --> 00:24:53.694 line:-2
To check out more options
and benchmarking data,


411
00:24:53,727 --> 00:24:55,996
do visit our documentation.


412
00:24:56,029 --> 00:24:58,365
I would also highly recommend


413
00:24:58,398 --> 00:25:02,970
tuning in to the "Improve Core ML
integration with async prediction" video


414
00:25:03,003 --> 00:25:06,573
which talks about improvements
made to the Core ML framework


415
00:25:06,607 --> 00:25:08,609
that I did not cover in the slides today.


416
00:25:08.642 --> 00:25:11.278 line:-1
Thank you, and happy compressing.

