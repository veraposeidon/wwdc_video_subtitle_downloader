2
00:00:00.100 --> 00:00:03.437 line:-1 position:50%
♪ Mellow instrumental hip-hop ♪


3
00:00:03,437 --> 00:00:10,177 line:0 position:90% size:2%
♪


4
00:00:10.177 --> 00:00:12.412 line:-1 position:50%
Omid Khalili: Hello!
My name is Omid.


5
00:00:12.412 --> 00:00:15.415 line:-1 position:50%
Oliver and I are engineers
on the ARKit team


6
00:00:15.415 --> 00:00:18.051 line:-1 position:50%
and we are thrilled
to review the concepts --


7
00:00:18.051 --> 00:00:19.987 line:-1 position:50%
some familiar and some new --


8
00:00:19.987 --> 00:00:23.257 line:-1 position:50%
that you'll need know about
when bringing your iOS AR app


9
00:00:23.257 --> 00:00:25.392 line:-1 position:50%
to our new platform.


10
00:00:25,392 --> 00:00:29,363 position:50%
ARKit was introduced
on iOS in 2017


11
00:00:29,363 --> 00:00:32,532 position:50%
and with it, we introduced
three key concepts


12
00:00:32,532 --> 00:00:35,636 position:50%
to building augmented reality
applications.


13
00:00:35,636 --> 00:00:36,837 position:50%
With world tracking,


14
00:00:36,837 --> 00:00:40,407 position:50%
ARKit is able to track your
device's position in the world


15
00:00:40,407 --> 00:00:42,376 position:50%
with six degrees of freedom.


16
00:00:42,376 --> 00:00:44,378 position:50%
This allows anchoring
virtual content


17
00:00:44,378 --> 00:00:48,682 position:50%
with a position and orientation
to the real world.


18
00:00:48,682 --> 00:00:50,450 position:50%
Scene understanding
provides insight


19
00:00:50,450 --> 00:00:52,920 position:50%
about the real world around you.


20
00:00:52,920 --> 00:00:55,889 position:50%
Using the provided geometry
and semantic knowledge,


21
00:00:55,889 --> 00:00:58,759 position:50%
your content can be
intelligently placed


22
00:00:58,759 --> 00:01:02,963 position:50%
and realistically interact
with surroundings.


23
00:01:02,963 --> 00:01:06,767 position:50%
Finally, rendering engines can
correctly register and composite


24
00:01:06,767 --> 00:01:09,803 position:50%
your virtual content
over captured images


25
00:01:09,803 --> 00:01:11,405 position:50%
utilizing camera transforms


26
00:01:11,405 --> 00:01:14,808 position:50%
and intrinsics
provided by ARKit.


27
00:01:14,808 --> 00:01:17,210 position:50%
Initially, we started
with a SceneKit view


28
00:01:17,210 --> 00:01:19,313 position:50%
to use ARKit's
camera transforms


29
00:01:19,313 --> 00:01:22,249 position:50%
and render 3D content on iOS.


30
00:01:22,249 --> 00:01:24,584 position:50%
We then introduced
RealityKit,


31
00:01:24,584 --> 00:01:26,687 position:50%
laying out the foundation
for an engine


32
00:01:26,687 --> 00:01:30,357 position:50%
capable of highly realistic
physically-based rendering


33
00:01:30,357 --> 00:01:34,294 position:50%
and accurate object simulation
with your surroundings.


34
00:01:34.294 --> 00:01:36.163 line:-1 position:50%
To enable spatial computing,


35
00:01:36.163 --> 00:01:38.699 line:-1 position:50%
ARKit and RealityKit
have matured


36
00:01:38.699 --> 00:01:42.135 line:-1 position:50%
and are deeply integrated
into the operating system.


37
00:01:42.135 --> 00:01:46.206 line:-1 position:50%
For example, ARKit's tracking
and Scene Understanding


38
00:01:46.206 --> 00:01:48.642 line:-1 position:50%
are now running
as system services,


39
00:01:48.642 --> 00:01:51.378 line:-1 position:50%
backing everything
from window placement


40
00:01:51.378 --> 00:01:53.647 line:-1 position:50%
to spatial audio.


41
00:01:53.647 --> 00:01:56.316 line:-1 position:50%
The system takes on
responsibilities


42
00:01:56.316 --> 00:01:59.953 line:-1 position:50%
that used to belong
to applications.


43
00:01:59.953 --> 00:02:02.689 line:-1 position:50%
Camera pass-through and matting
of the user's hands


44
00:02:02.689 --> 00:02:03.857 line:-1 position:50%
are now built-in,


45
00:02:03.857 --> 00:02:09.196 line:-1 position:50%
so your application gets these
capabilities for free.


46
00:02:09.196 --> 00:02:12.899 line:-1 position:50%
Another built-in capability
is that ARKit world maps


47
00:02:12.899 --> 00:02:16.703 line:-1 position:50%
are continuously persisted
by a system service,


48
00:02:16.703 --> 00:02:20.774 line:-1 position:50%
so your application
doesn't need to do it anymore.


49
00:02:20.774 --> 00:02:23.343 line:-1 position:50%
We believe that
this will free you up


50
00:02:23.343 --> 00:02:27.381 line:-1 position:50%
to focus on building the best
application and content possible


51
00:02:27.381 --> 00:02:29.316 line:-1 position:50%
for this platform.


52
00:02:29.316 --> 00:02:32.185 line:-1 position:50%
Here is an example demonstrating
these capabilities,


53
00:02:32.185 --> 00:02:35.522 line:-1 position:50%
along with new ones
introduced with this platform.


54
00:02:35.522 --> 00:02:39.760 line:-1 position:50%
For example, ARKit now provides
hand tracking to your app,


55
00:02:39.760 --> 00:02:41.361 line:-1 position:50%
which allows people to reach out


56
00:02:41.361 --> 00:02:44.031 line:-1 position:50%
and directly interact
with virtual content


57
00:02:44.031 --> 00:02:47.501 line:-1 position:50%
that can then interact
with their surroundings.


58
00:02:47.501 --> 00:02:50.137 line:-1 position:50%
In order to take advantage
of all the new capabilities


59
00:02:50.137 --> 00:02:53.273 line:-1 position:50%
and immersive experiences
this new platform offers,


60
00:02:53.273 --> 00:02:56.977 line:-1 position:50%
you'll need to update your iOS
ARKit-based experience.


61
00:02:56.977 --> 00:03:00.180 line:-1 position:50%
This is a great opportunity
to reimagine your app


62
00:03:00.180 --> 00:03:03.984 line:-1 position:50%
and AR experience
for spatial computing.


63
00:03:03.984 --> 00:03:05.919 line:-1 position:50%
As a part of this transition,


64
00:03:05.919 --> 00:03:07.554 line:-1 position:50%
you'll be using familiar
concepts


65
00:03:07.554 --> 00:03:11.191 line:-1 position:50%
that we've introduced
with ARKit and RealityKit.


66
00:03:11.191 --> 00:03:14.594 line:-1 position:50%
We're going to cover how these
concepts have carried over,


67
00:03:14.594 --> 00:03:15.929 line:-1 position:50%
how they've evolved,


68
00:03:15.929 --> 00:03:18.465 line:-1 position:50%
and how you can
take advantage of them.


69
00:03:18.465 --> 00:03:20.033 line:-1 position:50%
Let's get started!


70
00:03:20.033 --> 00:03:23.970 line:-1 position:50%
First, we'll explore some new
ways you can present your app


71
00:03:23.970 --> 00:03:25.505 line:-1 position:50%
for spatial computing,


72
00:03:25.505 --> 00:03:30.410 line:-1 position:50%
and introduce new content tools
available to you.


73
00:03:30.410 --> 00:03:33.246 line:-1 position:50%
Next, we will talk
about Reality Kit,


74
00:03:33.246 --> 00:03:34.948 line:-1 position:50%
which is the engine to use
to render


75
00:03:34.948 --> 00:03:38.118 line:-1 position:50%
and interact
with your content.


76
00:03:38.118 --> 00:03:40.987 line:-1 position:50%
We'll see how RealityView
lets your app


77
00:03:40.987 --> 00:03:46.460 line:-1 position:50%
leverage spatial computing
similar to ARView on iOS.


78
00:03:46.460 --> 00:03:49.096 line:-1 position:50%
Then, we'll talk about
the different ways


79
00:03:49.096 --> 00:03:53.834 line:-1 position:50%
your app can bring content
into people's surroundings.


80
00:03:53.834 --> 00:03:58.038 line:-1 position:50%
Raycasting is something
many iOS applications use


81
00:03:58.038 --> 00:04:00.040 line:-1 position:50%
to place content.


82
00:04:00.040 --> 00:04:03.210 line:-1 position:50%
We'll show an example
of how to combine ARKit data


83
00:04:03.210 --> 00:04:06.213 line:-1 position:50%
and RealityKit
to enable raycasting


84
00:04:06.213 --> 00:04:09.516 line:-1 position:50%
for spatial computing.


85
00:04:09.516 --> 00:04:13.253 line:-1 position:50%
And finally, we'll review
the updates to ARKit


86
00:04:13.253 --> 00:04:19.359 line:-1 position:50%
and see the new ways to utilize
familiar concepts from iOS.


87
00:04:19.359 --> 00:04:22.062 line:-1 position:50%
Lets get into preparing
to migrate your experience


88
00:04:22.062 --> 00:04:25.398 line:-1 position:50%
for spatial computing.


89
00:04:25.398 --> 00:04:29.669 line:-1 position:50%
Spatial computing allows you
to take your iOS AR experience


90
00:04:29.669 --> 00:04:32.506 line:-1 position:50%
and expand it
beyond the window.


91
00:04:32.506 --> 00:04:36.476 line:-1 position:50%
This platform offers new ways
to present your application


92
00:04:36.476 --> 00:04:37.911 line:-1 position:50%
that you'll want to consider


93
00:04:37.911 --> 00:04:41.181 line:-1 position:50%
as you bring your
iOS experience over.


94
00:04:41.181 --> 00:04:44.151 line:-1 position:50%
Here is an example from
our Hello World sample app.


95
00:04:44.151 --> 00:04:45.886 line:-1 position:50%
You can now display UI,


96
00:04:45.886 --> 00:04:48.889 line:-1 position:50%
including windows
and three-dimensional content,


97
00:04:48.889 --> 00:04:51.224 line:-1 position:50%
anywhere around you.


98
00:04:51.224 --> 00:04:53.627 line:-1 position:50%
By default,
applications on this platform


99
00:04:53.627 --> 00:04:56.196 line:-1 position:50%
launch into the Shared Space.


100
00:04:56.196 --> 00:04:59.733 line:-1 position:50%
The Shared Space is where
apps exist side by side,


101
00:04:59.733 --> 00:05:02.669 line:-1 position:50%
much like multiple apps
on a Mac desktop.


102
00:05:02.669 --> 00:05:07.107 line:-1 position:50%
Inside a Shared Space, your app
can open one or more windows


103
00:05:07.107 --> 00:05:08.842 line:-1 position:50%
to display content.


104
00:05:08.842 --> 00:05:10.844 line:-1 position:50%
Additionally,
your app can create


105
00:05:10.844 --> 00:05:13.246 line:-1 position:50%
a three-dimensional volume.


106
00:05:13.246 --> 00:05:17.117 line:-1 position:50%
For example, now you can show
a list of available board games


107
00:05:17.117 --> 00:05:20.287 line:-1 position:50%
in one window,
the rules in another,


108
00:05:20.287 --> 00:05:24.291 line:-1 position:50%
and open the selected game
in its own volume.


109
00:05:24.291 --> 00:05:28.094 line:-1 position:50%
The game can be played while
keeping a Safari window open


110
00:05:28.094 --> 00:05:30.997 line:-1 position:50%
to read up
on winning strategies.


111
00:05:30.997 --> 00:05:34.000 line:-1 position:50%
The content you add
to the window and volume


112
00:05:34.000 --> 00:05:36.436 line:-1 position:50%
stays contained
within its bounds


113
00:05:36.436 --> 00:05:39.873 line:-1 position:50%
to allow sharing the space
with other applications.


114
00:05:39,873 --> 00:05:43,043 position:50%
In some cases, you may want
your app to have more control


115
00:05:43,043 --> 00:05:46,012 position:50%
over the level of immersion
in your experience --


116
00:05:46,012 --> 00:05:49,316 position:50%
maybe to play a game
that interacts with your room.


117
00:05:49,316 --> 00:05:53,053 position:50%
For this, your app can open
a dedicated Full Space


118
00:05:53.053 --> 00:05:55.789 line:-1 position:50%
in which only
your app's windows,


119
00:05:55.789 --> 00:05:59.659 line:-1 position:50%
volumes,
and 3D objects appear.


120
00:05:59.659 --> 00:06:01.261 line:-1 position:50%
Once in a Full Space,


121
00:06:01.261 --> 00:06:05.532 line:-1 position:50%
your application has access
to more features.


122
00:06:05.532 --> 00:06:08.235 line:-1 position:50%
Using RealityKit's
anchor entities,


123
00:06:08.235 --> 00:06:11.338 line:-1 position:50%
you can target and attach
objects to the surroundings


124
00:06:11.338 --> 00:06:13.039 line:-1 position:50%
like tables, floors,


125
00:06:13.039 --> 00:06:17.143 line:-1 position:50%
and even parts of your hands
like the palm or wrist.


126
00:06:17.143 --> 00:06:22.215 line:-1 position:50%
Anchor entities work without
requiring user permission.


127
00:06:22.215 --> 00:06:23.550 line:-1 position:50%
ARKit data is something else


128
00:06:23.550 --> 00:06:26.620 line:-1 position:50%
your app can only access
in a Full Space.


129
00:06:26.620 --> 00:06:29.856 line:-1 position:50%
With permission,
ARKit will provide data


130
00:06:29.856 --> 00:06:33.393 line:-1 position:50%
about the real-world surfaces,
scene geometry,


131
00:06:33.393 --> 00:06:35.362 line:-1 position:50%
and skeletal hand tracking,


132
00:06:35.362 --> 00:06:38.598 line:-1 position:50%
expanding your app's
ability for realistic physics


133
00:06:38.598 --> 00:06:42.135 line:-1 position:50%
and natural interactions.


134
00:06:42,135 --> 00:06:47,007 position:50%
Windows, volumes, and spaces
are all SwiftUI scene types.


135
00:06:47,007 --> 00:06:50,210 position:50%
There's so much more
for you to learn about these.


136
00:06:50,210 --> 00:06:55,248 position:50%
For starters, you can go
to the session mentioned here.


137
00:06:55.248 --> 00:06:58.618 line:-1 position:50%
Next, let's review the main
steps needed to prepare


138
00:06:58.618 --> 00:07:03.189 line:-1 position:50%
your content for bringing it
to spatial computing.


139
00:07:03.189 --> 00:07:07.961 line:-1 position:50%
Memorable AR experiences on iOS
begin with great 3D content;


140
00:07:07.961 --> 00:07:12.432 line:-1 position:50%
the same is true for spatial
experiences on this platform.


141
00:07:12.432 --> 00:07:14.401 line:-1 position:50%
And when it comes
to 3D content,


142
00:07:14.401 --> 00:07:17.304 line:-1 position:50%
it's great to rely
on an open standard like


143
00:07:17.304 --> 00:07:21.207 line:-1 position:50%
Universal Scene Description,
or USD for short.


144
00:07:21.207 --> 00:07:23.543 line:-1 position:50%
USD is production proven,


145
00:07:23.543 --> 00:07:26.780 line:-1 position:50%
and scales from creators
making single assets


146
00:07:26.780 --> 00:07:31.551 line:-1 position:50%
to large studios working on
AAA games and films.


147
00:07:31.551 --> 00:07:34.354 line:-1 position:50%
Apple was an early
adopter of USD,


148
00:07:34.354 --> 00:07:37.123 line:-1 position:50%
adding it to our platforms
in 2017


149
00:07:37.123 --> 00:07:39.326 line:-1 position:50%
and growing support since.


150
00:07:39.326 --> 00:07:42.796 line:-1 position:50%
Today, USD is at the heart
of 3D content


151
00:07:42.796 --> 00:07:44.898 line:-1 position:50%
for spatial computing.


152
00:07:44.898 --> 00:07:47.901 line:-1 position:50%
With USD assets ready,
you can bring them into


153
00:07:47.901 --> 00:07:51.304 line:-1 position:50%
our new developer tool,
Reality Composer Pro,


154
00:07:51.304 --> 00:07:56.009 line:-1 position:50%
to compose, edit, and preview
your 3D content.


155
00:07:56.009 --> 00:07:59.979 line:-1 position:50%
If you're using CustomMaterials
for your 3D content on iOS,


156
00:07:59.979 --> 00:08:03.350 line:-1 position:50%
then you will need to rebuild
them using its shader graph.


157
00:08:03.350 --> 00:08:06.519 line:-1 position:50%
You also have the ability to
edit your RealityKit components


158
00:08:06.519 --> 00:08:08.888 line:-1 position:50%
directly through the UI.


159
00:08:08.888 --> 00:08:13.793 line:-1 position:50%
And finally, you can import your
Reality Composer Pro project


160
00:08:13.793 --> 00:08:15.428 line:-1 position:50%
directly into Xcode,


161
00:08:15.428 --> 00:08:19.065 line:-1 position:50%
allowing you to easily
bundle all of your USD assets,


162
00:08:19.065 --> 00:08:24.270 line:-1 position:50%
materials, and custom components
into your Xcode project.


163
00:08:24,270 --> 00:08:25,739 position:50%
We have some great sessions


164
00:08:25,739 --> 00:08:28,842 position:50%
to help you learn more
about Reality Composer Pro


165
00:08:28,842 --> 00:08:34,414 position:50%
and how to build your own custom
materials for spatial computing.


166
00:08:34.414 --> 00:08:35.949 line:-1 position:50%
Now that we've seen
the different ways


167
00:08:35.949 --> 00:08:38.051 line:-1 position:50%
to present your application,


168
00:08:38.051 --> 00:08:41.221 line:-1 position:50%
let's learn more about
the features RealityView offers


169
00:08:41.221 --> 00:08:44.090 line:-1 position:50%
as you bring
your experience over.


170
00:08:44.090 --> 00:08:45.959 line:-1 position:50%
We just saw
how spatial computing


171
00:08:45.959 --> 00:08:48.762 line:-1 position:50%
allows apps to display content
in your space.


172
00:08:48.762 --> 00:08:51.297 line:-1 position:50%
One of the key differences
coming from iOS


173
00:08:51.297 --> 00:08:55.635 line:-1 position:50%
is how different elements
can be presented side by side.


174
00:08:55.635 --> 00:09:00.540 line:-1 position:50%
Notice how your 3D content
and 2D elements can appear


175
00:09:00.540 --> 00:09:03.343 line:-1 position:50%
and work along side each other.


176
00:09:03.343 --> 00:09:05.412 line:-1 position:50%
Coming from iOS,
you're going to use


177
00:09:05.412 --> 00:09:08.615 line:-1 position:50%
familiar frameworks
to create each of these.


178
00:09:08.615 --> 00:09:11.684 line:-1 position:50%
You'll use SwiftUI
to build the best 2D UI


179
00:09:11.684 --> 00:09:15.889 line:-1 position:50%
and get system gestures events
like the ones on iOS.


180
00:09:15.889 --> 00:09:19.058 line:-1 position:50%
And you'll use RealityKit
to render your 3D content


181
00:09:19.058 --> 00:09:21.928 line:-1 position:50%
for spatial experiences.


182
00:09:21.928 --> 00:09:24.864 line:-1 position:50%
The way to interface with both
of these at the same time


183
00:09:24.864 --> 00:09:26.566 line:-1 position:50%
is through RealityView -


184
00:09:26.566 --> 00:09:30.036 line:-1 position:50%
a new SwiftUI view
that we're introducing


185
00:09:30.036 --> 00:09:34.207 line:-1 position:50%
to cater to the unique needs
of spatial computing.


186
00:09:34.207 --> 00:09:38.611 line:-1 position:50%
RealityView truly bridges
SwiftUI and RealityKit,


187
00:09:38.611 --> 00:09:41.781 line:-1 position:50%
allowing you to combine
2D and 3D elements


188
00:09:41.781 --> 00:09:45.485 line:-1 position:50%
and create a memorable
spatial experience.


189
00:09:45.485 --> 00:09:49.122 line:-1 position:50%
You'll be using the RealityView
to hold all the entities


190
00:09:49.122 --> 00:09:52.258 line:-1 position:50%
you wish to display
and interact with.


191
00:09:52.258 --> 00:09:54.661 line:-1 position:50%
You can get gesture events
and connect them


192
00:09:54.661 --> 00:09:57.297 line:-1 position:50%
to the entities in your view
to control them.


193
00:09:57.297 --> 00:09:59.899 line:-1 position:50%
And with access to ARKit's
scene understanding,


194
00:09:59.899 --> 00:10:02.602 line:-1 position:50%
you can enable
realistic simulations


195
00:10:02.602 --> 00:10:06.239 line:-1 position:50%
with people's surroundings
and even their hands


196
00:10:06.239 --> 00:10:09.576 line:-1 position:50%
using RealityKit's
collision components.


197
00:10:09.576 --> 00:10:11.911 line:-1 position:50%
Before we look
at how using RealityKit


198
00:10:11.911 --> 00:10:15.381 line:-1 position:50%
carries over from iOS,
let's do a quick refresher


199
00:10:15.381 --> 00:10:20.587 line:-1 position:50%
on how to work with RealityKit's
Entity Component System.


200
00:10:20,587 --> 00:10:23,723 position:50%
In the Reality Kit
Entity Component System,


201
00:10:23,723 --> 00:10:27,460 position:50%
each entity is a container
for 3D content.


202
00:10:27,460 --> 00:10:29,596 position:50%
Different components
are added to an entity


203
00:10:29,596 --> 00:10:31,831 position:50%
to define its look
and behavior.


204
00:10:31,831 --> 00:10:33,566 position:50%
This can include
a model component


205
00:10:33,566 --> 00:10:35,301 position:50%
for how it should render;


206
00:10:35,301 --> 00:10:36,336 position:50%
a collision component,


207
00:10:36,336 --> 00:10:38,872 position:50%
for how it can collide
with other entities;


208
00:10:38,872 --> 00:10:41,508 position:50%
and many more.


209
00:10:41,508 --> 00:10:43,243 position:50%
You can use
RealityComposer Pro


210
00:10:43,243 --> 00:10:47,113 position:50%
to prepare RealityKit components
like collision components


211
00:10:47,113 --> 00:10:50,450 position:50%
and get them added
to your entities.


212
00:10:50.450 --> 00:10:53.386 line:-1 position:50%
Systems contain code
to act on entities


213
00:10:53.386 --> 00:10:55.755 line:-1 position:50%
that have the required
components.


214
00:10:55.755 --> 00:10:58.858 line:-1 position:50%
For example, the system
required for gesture support


215
00:10:58.858 --> 00:11:02.262 line:-1 position:50%
only operates on entities
that have a CollisionComponent


216
00:11:02.262 --> 00:11:05.965 line:-1 position:50%
and InputTargetComponent.


217
00:11:05.965 --> 00:11:08.968 line:-1 position:50%
A lot of the concepts
used by RealityView


218
00:11:08.968 --> 00:11:14.073 line:-1 position:50%
for spatial computing carry over
from those of ARView on iOS.


219
00:11:14.073 --> 00:11:17.243 line:-1 position:50%
Lets see how these two stack up.


220
00:11:17.243 --> 00:11:20.079 line:-1 position:50%
Both views are event-aware
containers


221
00:11:20.079 --> 00:11:24.551 line:-1 position:50%
to hold the entities
you wish to display in your app.


222
00:11:24.551 --> 00:11:26.853 line:-1 position:50%
You can add Gesture Support
to your views


223
00:11:26.853 --> 00:11:30.590 line:-1 position:50%
to enable selection
and interaction with entities.


224
00:11:30.590 --> 00:11:33.326 line:-1 position:50%
With SwiftUI
for spatial computing,


225
00:11:33.326 --> 00:11:37.797 line:-1 position:50%
you can reach out to select
or drag your entities.


226
00:11:37.797 --> 00:11:40.667 line:-1 position:50%
Both ARView and RealityView


227
00:11:40.667 --> 00:11:43.703 line:-1 position:50%
provide a collection
of your entities.


228
00:11:43.703 --> 00:11:46.539 line:-1 position:50%
ARView uses a Scene for this.


229
00:11:46.539 --> 00:11:50.843 line:-1 position:50%
RealityView has a Content
to add your entities to.


230
00:11:50.843 --> 00:11:52.812 line:-1 position:50%
You can add AnchorEntities
to them,


231
00:11:52.812 --> 00:11:56.416 line:-1 position:50%
allowing you to anchor
your content to the real world.


232
00:11:56.416 --> 00:11:59.052 line:-1 position:50%
On both platforms,
you create an entity


233
00:11:59.052 --> 00:12:03.957 line:-1 position:50%
to load your content model
and an AnchorEntity to place it.


234
00:12:03.957 --> 00:12:06.259 line:-1 position:50%
One main difference
between the platforms


235
00:12:06.259 --> 00:12:09.596 line:-1 position:50%
is in the behavior
of anchor entities.


236
00:12:09,596 --> 00:12:12,966 position:50%
ARView on iOS
uses an ARSession


237
00:12:12,966 --> 00:12:15,969 position:50%
and your app must receive
permission


238
00:12:15,969 --> 00:12:17,870 position:50%
to run scene understanding
algorithms


239
00:12:17,870 --> 00:12:21,074 position:50%
necessary for anchor entities
to work.


240
00:12:21,074 --> 00:12:23,743 position:50%
RealityView is using
System Services


241
00:12:23,743 --> 00:12:26,145 position:50%
to enable anchorEntities.


242
00:12:26,145 --> 00:12:28,514 position:50%
This means
that spatial experiences


243
00:12:28,514 --> 00:12:31,184 position:50%
can anchor content
to your surroundings


244
00:12:31,184 --> 00:12:34,187 position:50%
without requiring permissions.


245
00:12:34,187 --> 00:12:36,789 position:50%
Apps using this approach
do not receive


246
00:12:36,789 --> 00:12:39,058 position:50%
the underlying scene
understanding data


247
00:12:39,058 --> 00:12:41,294 position:50%
or transforms.


248
00:12:41,294 --> 00:12:45,031 position:50%
Not having transform data
for your app to place content


249
00:12:45,031 --> 00:12:48,334 position:50%
has some implications that
Oliver will talk about later


250
00:12:48,334 --> 00:12:50,136 position:50%
in his section.


251
00:12:50.136 --> 00:12:53.473 line:-1 position:50%
As we've seen, there are
many familiar concepts


252
00:12:53.473 --> 00:12:55.975 line:-1 position:50%
that carry over
coming from iOS,


253
00:12:55.975 --> 00:12:58.511 line:-1 position:50%
but there are also
new capabilities


254
00:12:58.511 --> 00:13:01.681 line:-1 position:50%
that RealityKit provides
for spatial computing.


255
00:13:01,681 --> 00:13:04,584 position:50%
We've only scratched
the surface of what's possible


256
00:13:04,584 --> 00:13:07,053 position:50%
with RealityKit
on this new platform,


257
00:13:07,053 --> 00:13:09,555 position:50%
and you may want
to check out the session below


258
00:13:09,555 --> 00:13:12,025 position:50%
to follow up on more.


259
00:13:12.025 --> 00:13:15.862 line:-1 position:50%
Now over to Oliver, who will
talk more about RealityView


260
00:13:15.862 --> 00:13:19.332 line:-1 position:50%
and how to bring in
your content from iOS.


261
00:13:19.332 --> 00:13:20.500 line:-1 position:50%
Oliver Dunkley: Thanks, Omid!


262
00:13:20.500 --> 00:13:22.435 line:-1 position:50%
Let's continue by exploring
the different ways


263
00:13:22.435 --> 00:13:24.303 line:-1 position:50%
you can bring in
your existing content


264
00:13:24.303 --> 00:13:26.406 line:-1 position:50%
to spatial computing.


265
00:13:26.406 --> 00:13:28.508 line:-1 position:50%
Lets start in the Shared Space.


266
00:13:28.508 --> 00:13:30.843 line:-1 position:50%
We can add 3D content
to a window or volume,


267
00:13:30.843 --> 00:13:34.080 line:-1 position:50%
and use system gestures
to interact with it.


268
00:13:34.080 --> 00:13:36.349 line:-1 position:50%
To display your assets,
you just add them directly


269
00:13:36.349 --> 00:13:39.452 line:-1 position:50%
to RealityView's Content.


270
00:13:39.452 --> 00:13:41.688 line:-1 position:50%
You do this
by creating an entity


271
00:13:41.688 --> 00:13:44.290 line:-1 position:50%
to hold your model component
and position it


272
00:13:44.290 --> 00:13:46.325 line:-1 position:50%
by setting the transform
component.


273
00:13:46.325 --> 00:13:48.394 line:-1 position:50%
You can also set up
gesture support


274
00:13:48.394 --> 00:13:51.364 line:-1 position:50%
to modify
the transform component.


275
00:13:51.364 --> 00:13:54.500 line:-1 position:50%
Note that all entities
added to the view's content


276
00:13:54.500 --> 00:13:58.204 line:-1 position:50%
exist in the same space
relative to the space's origin


277
00:13:58.204 --> 00:14:01.274 line:-1 position:50%
and can therefore
interact with one another.


278
00:14:01.274 --> 00:14:02.275 line:-1 position:50%
In the Shared Space,


279
00:14:02.275 --> 00:14:05.778 line:-1 position:50%
content cannot be anchored
to your surroundings.


280
00:14:05.778 --> 00:14:06.879 line:-1 position:50%
Let's consider our options


281
00:14:06.879 --> 00:14:09.982 line:-1 position:50%
if we transition our app
to a Full Space.


282
00:14:09.982 --> 00:14:12.351 line:-1 position:50%
One of the key differences
coming from the Shared Space


283
00:14:12.351 --> 00:14:14.654 line:-1 position:50%
is that apps can now
additionally anchor content


284
00:14:14.654 --> 00:14:16.723 line:-1 position:50%
to people's surroundings.


285
00:14:16.723 --> 00:14:19.759 line:-1 position:50%
Anchoring your content here
can happen in two ways.


286
00:14:19.759 --> 00:14:22.595 line:-1 position:50%
Lets first look at using
RealityKit's AnchorEntity


287
00:14:22.595 --> 00:14:25.264 line:-1 position:50%
to place content without
requiring permissions


288
00:14:25.264 --> 00:14:28.634 line:-1 position:50%
to use ARKit data
in your app.


289
00:14:28.634 --> 00:14:30.236 line:-1 position:50%
RealityKit's AnchorEntities


290
00:14:30.236 --> 00:14:33.139 line:-1 position:50%
allow you to specify a target
for the system to find


291
00:14:33.139 --> 00:14:36.075 line:-1 position:50%
and automatically
anchor your content to.


292
00:14:36.075 --> 00:14:39.112 line:-1 position:50%
So for example, in order
to place a 3D model


293
00:14:39.112 --> 00:14:40.847 line:-1 position:50%
on a table surface
in front of you,


294
00:14:40.847 --> 00:14:43.082 line:-1 position:50%
you can use
a RealityKit AnchorEntity


295
00:14:43.082 --> 00:14:46.119 line:-1 position:50%
with a target set to table.


296
00:14:46.119 --> 00:14:48.921 line:-1 position:50%
Different from iOS,
AnchorEntities can be used


297
00:14:48.921 --> 00:14:52.959 line:-1 position:50%
without having to prompt
for user permissions.


298
00:14:52.959 --> 00:14:55.328 line:-1 position:50%
People's privacy is preserved
by not sharing


299
00:14:55.328 --> 00:14:58.097 line:-1 position:50%
the underlying transforms
of the AnchorEntity


300
00:14:58.097 --> 00:15:00.333 line:-1 position:50%
with your application.


301
00:15:00.333 --> 00:15:03.236 line:-1 position:50%
Note: this implies children
of different anchor entities


302
00:15:03.236 --> 00:15:05.972 line:-1 position:50%
are not aware of one another.


303
00:15:05.972 --> 00:15:08.608 line:-1 position:50%
New to anchorEntities,
you can target hands,


304
00:15:08.608 --> 00:15:10.076 line:-1 position:50%
which opens up
a whole new realm


305
00:15:10.076 --> 00:15:12.912 line:-1 position:50%
of interesting interaction
opportunities.


306
00:15:12.912 --> 00:15:16.115 line:-1 position:50%
For example, you could anchor
content to a person's palm


307
00:15:16.115 --> 00:15:19.085 line:-1 position:50%
and have it follow their hands
as they move them.


308
00:15:19.085 --> 00:15:20.787 line:-1 position:50%
This is all done by the system,


309
00:15:20.787 --> 00:15:25.224 line:-1 position:50%
without telling your app where
the persons hands actually are.


310
00:15:25.224 --> 00:15:26.759 line:-1 position:50%
AnchorEntitys provide a quick,


311
00:15:26.759 --> 00:15:28.928 line:-1 position:50%
privacy-friendly way
for your app


312
00:15:28.928 --> 00:15:32.365 line:-1 position:50%
to anchor content
to people's surroundings.


313
00:15:32.365 --> 00:15:34.133 line:-1 position:50%
Coming back
to a Full Space,


314
00:15:34.133 --> 00:15:36.102 line:-1 position:50%
we can also leverage ARKit
to incorporate


315
00:15:36.102 --> 00:15:39.705 line:-1 position:50%
system-level knowledge
of the people's surroundings.


316
00:15:39.705 --> 00:15:43.176 line:-1 position:50%
This enables you to build
your own custom placement logic.


317
00:15:43.176 --> 00:15:46.212 line:-1 position:50%
Let's take a look
at how this works.


318
00:15:46.212 --> 00:15:49.382 line:-1 position:50%
Similar to iOS, your application
receives anchoring updates


319
00:15:49.382 --> 00:15:51.884 line:-1 position:50%
for scene understanding data.


320
00:15:51.884 --> 00:15:55.087 line:-1 position:50%
You can integrate this anchor
data into your app logic


321
00:15:55.087 --> 00:15:58.224 line:-1 position:50%
to achieve all sorts
of amazing experiences.


322
00:15:58.224 --> 00:16:00.626 line:-1 position:50%
For example, you could
use the bounds of a plane


323
00:16:00.626 --> 00:16:03.529 line:-1 position:50%
to center and distribute
your content onto.


324
00:16:03.529 --> 00:16:06.132 line:-1 position:50%
Or, you could use planes
and their classifications


325
00:16:06.132 --> 00:16:07.800 line:-1 position:50%
to find the corner of a room


326
00:16:07.800 --> 00:16:11.938 line:-1 position:50%
by looking for the intersection
of two walls and a floor.


327
00:16:11.938 --> 00:16:14.040 line:-1 position:50%
Once you've decided
where to place your content,


328
00:16:14.040 --> 00:16:16.676 line:-1 position:50%
you add a world anchor
for ARKit to track


329
00:16:16.676 --> 00:16:20.446 line:-1 position:50%
and use it to update your
entity's transform component.


330
00:16:20.446 --> 00:16:21.647 line:-1 position:50%
This not only
allows your content


331
00:16:21.647 --> 00:16:23.349 line:-1 position:50%
to remain anchored
to the real world,


332
00:16:23.349 --> 00:16:25.551 line:-1 position:50%
as the underlying world map
is updated,


333
00:16:25.551 --> 00:16:28.521 line:-1 position:50%
but it also opens the door
to anchor persistence,


334
00:16:28.521 --> 00:16:31.624 line:-1 position:50%
which we will explore shortly.


335
00:16:31.624 --> 00:16:33.292 line:-1 position:50%
All the entities added
to your space


336
00:16:33.292 --> 00:16:37.029 line:-1 position:50%
can interact with one another as
well as with the surroundings.


337
00:16:37.029 --> 00:16:39.131 line:-1 position:50%
This all works because
scene understanding anchors


338
00:16:39.131 --> 00:16:43.836 line:-1 position:50%
are delivered with transforms
relative to the space's origin.


339
00:16:43.836 --> 00:16:48.975 line:-1 position:50%
User Permission is required
to use ARKit capabilities.


340
00:16:48.975 --> 00:16:52.311 line:-1 position:50%
You just saw how integrating
ARKit data into your app logic


341
00:16:52.311 --> 00:16:54.714 line:-1 position:50%
can enable
more advanced features.


342
00:16:54.714 --> 00:16:57.917 line:-1 position:50%
So far we have talked about
letting your app place content.


343
00:16:57.917 --> 00:17:01.387 line:-1 position:50%
Let's explore how we can
let people guide placement.


344
00:17:01.387 --> 00:17:03.489 line:-1 position:50%
On iOS, you can use raycasting


345
00:17:03.489 --> 00:17:07.360 line:-1 position:50%
to translate 2D input
to a 3D position.


346
00:17:07.360 --> 00:17:08.694 line:-1 position:50%
But with this new platform,


347
00:17:08.694 --> 00:17:11.364 line:-1 position:50%
we don't need
this 2D-3D bridge anymore,


348
00:17:11.364 --> 00:17:12.398 line:-1 position:50%
as we can use hands


349
00:17:12.398 --> 00:17:16.035 line:-1 position:50%
to naturally interact
with experiences directly.


350
00:17:16.035 --> 00:17:17.937 line:-1 position:50%
Raycasting remains powerful;


351
00:17:17.937 --> 00:17:22.174 line:-1 position:50%
it lets people reach out
beyond arms length.


352
00:17:22.174 --> 00:17:25.278 line:-1 position:50%
There are various ways
to set up raycasting.


353
00:17:25.278 --> 00:17:27.380 line:-1 position:50%
Fundamentally, you need
to setup RealityKit's


354
00:17:27.380 --> 00:17:30.650 line:-1 position:50%
collision components
to raycast against.


355
00:17:30.650 --> 00:17:32.685 line:-1 position:50%
Collision components
can also be created


356
00:17:32.685 --> 00:17:34.353 line:-1 position:50%
from ARKit's mesh anchors


357
00:17:34.353 --> 00:17:37.123 line:-1 position:50%
to raycast against
people's surroundings.


358
00:17:37.123 --> 00:17:39.625 line:-1 position:50%
Let's explore two examples
of how to raycast


359
00:17:39.625 --> 00:17:41.661 line:-1 position:50%
for spatial computing:


360
00:17:41.661 --> 00:17:43.696 line:-1 position:50%
first, using system gestures,


361
00:17:43.696 --> 00:17:47.033 line:-1 position:50%
and the second
using hands data.


362
00:17:47.033 --> 00:17:48.768 line:-1 position:50%
After obtaining a position,


363
00:17:48.768 --> 00:17:50.269 line:-1 position:50%
we can place
an ARKit worldAnchor


364
00:17:50.269 --> 00:17:54.507 line:-1 position:50%
to keep our content anchored.


365
00:17:54.507 --> 00:17:56.842 line:-1 position:50%
Let's consider
the following example.


366
00:17:56.842 --> 00:17:58.077 line:-1 position:50%
Imagine our app revolves around


367
00:17:58.077 --> 00:18:01.647 line:-1 position:50%
placing inspirational 3D assets
for modelers.


368
00:18:01.647 --> 00:18:03.916 line:-1 position:50%
Maybe in this particular
scenario,


369
00:18:03.916 --> 00:18:06.585 line:-1 position:50%
a person wants to use our app
to place a virtual ship


370
00:18:06.585 --> 00:18:10.856 line:-1 position:50%
on their workbench
for some modeling project.


371
00:18:10.856 --> 00:18:13.893 line:-1 position:50%
Here is our workbench
we want to place our ship on.


372
00:18:13.893 --> 00:18:17.496 line:-1 position:50%
We'll start with
an empty RealityView.


373
00:18:17.496 --> 00:18:19.832 line:-1 position:50%
ARKit's scene understanding
provides mesh anchors


374
00:18:19.832 --> 00:18:22.501 line:-1 position:50%
that we'll use
to represent the surroundings.


375
00:18:22.501 --> 00:18:26.739 line:-1 position:50%
They provide geometry and
semantic information we can use.


376
00:18:26.739 --> 00:18:29.241 line:-1 position:50%
Remember that meshes
for scene reconstruction data


377
00:18:29.241 --> 00:18:32.478 line:-1 position:50%
are delivered
as a series of chunks.


378
00:18:32.478 --> 00:18:35.748 line:-1 position:50%
We'll create an entity
to represent this mesh chunk,


379
00:18:35.748 --> 00:18:38.951 line:-1 position:50%
and we'll correctly place
this entity in a full space


380
00:18:38.951 --> 00:18:42.254 line:-1 position:50%
using the mesh anchor's
transform.


381
00:18:42.254 --> 00:18:44.423 line:-1 position:50%
Our entity then needs
a collision component


382
00:18:44.423 --> 00:18:46.792 line:-1 position:50%
to hit test against.


383
00:18:46.792 --> 00:18:50.029 line:-1 position:50%
We'll use RealityKit's
ShapeResources method


384
00:18:50.029 --> 00:18:52.631 line:-1 position:50%
to generate a collision shape
from the meshAnchor


385
00:18:52.631 --> 00:18:54.700 line:-1 position:50%
for our entity.


386
00:18:54.700 --> 00:18:56.635 line:-1 position:50%
We'll then add
our correctly placed entity


387
00:18:56.635 --> 00:18:59.372 line:-1 position:50%
which supports hit testing.


388
00:18:59.372 --> 00:19:01.574 line:-1 position:50%
We'll build an entity
and collision component


389
00:19:01.574 --> 00:19:03.075 line:-1 position:50%
for each mesh chunk we receive


390
00:19:03.075 --> 00:19:06.312 line:-1 position:50%
to represent
all the surroundings.


391
00:19:06.312 --> 00:19:08.481 line:-1 position:50%
As scene reconstruction
is refined,


392
00:19:08.481 --> 00:19:11.951 line:-1 position:50%
we may get updates to meshes
or have chunks removed.


393
00:19:11.951 --> 00:19:13.619 line:-1 position:50%
We should be ready
to update our entities


394
00:19:13.619 --> 00:19:16.789 line:-1 position:50%
on these changes as well.


395
00:19:16.789 --> 00:19:18.324 line:-1 position:50%
We now have
a collection of entities


396
00:19:18.324 --> 00:19:20.393 line:-1 position:50%
representing the surroundings.


397
00:19:20.393 --> 00:19:22.561 line:-1 position:50%
All these entities
have collision components


398
00:19:22.561 --> 00:19:25.331 line:-1 position:50%
and can support
a raycast test.


399
00:19:25.331 --> 00:19:28.667 line:-1 position:50%
Let's first explore raycasting
using system gestures,


400
00:19:28.667 --> 00:19:32.671 line:-1 position:50%
and then continue the example
using hands data.


401
00:19:32.671 --> 00:19:35.341 line:-1 position:50%
We can raycast and get
a position to place our ship


402
00:19:35.341 --> 00:19:37.443 line:-1 position:50%
using system gestures.


403
00:19:37.443 --> 00:19:39.712 line:-1 position:50%
Gestures can only
interact with entities


404
00:19:39.712 --> 00:19:42.948 line:-1 position:50%
that have both Collision
and InputTarget components,


405
00:19:42.948 --> 00:19:46.552 line:-1 position:50%
so we add one
to each of our mesh entities.


406
00:19:46.552 --> 00:19:49.321 line:-1 position:50%
By adding a SpatialTapGesture
to the RealityView,


407
00:19:49.321 --> 00:19:53.359 line:-1 position:50%
people can raycast by
looking at entities and tapping.


408
00:19:53.359 --> 00:19:56.062 line:-1 position:50%
This resulting event
holds a position in world space


409
00:19:56.062 --> 00:20:00.166 line:-1 position:50%
representing the place
people looked at when tapping.


410
00:20:00.166 --> 00:20:01.667 line:-1 position:50%
Instead of
using system gestures,


411
00:20:01.667 --> 00:20:03.769 line:-1 position:50%
we could also have
used ARKit's hand anchors


412
00:20:03.769 --> 00:20:05.137 line:-1 position:50%
to build a ray.


413
00:20:05.137 --> 00:20:08.307 line:-1 position:50%
Lets take a step back
and explore this option.


414
00:20:08.307 --> 00:20:09.442 line:-1 position:50%
To know where people point,


415
00:20:09.442 --> 00:20:12.745 line:-1 position:50%
we first need a representation
of the person's hand.


416
00:20:12.745 --> 00:20:16.515 line:-1 position:50%
ARkit's new hand anchors
gives us everything we need.


417
00:20:16.515 --> 00:20:19.585 line:-1 position:50%
We can use finger joint
information to build the origin


418
00:20:19.585 --> 00:20:23.289 line:-1 position:50%
and direction of the ray
for our query.


419
00:20:23.289 --> 00:20:25.624 line:-1 position:50%
Now that we have the origin
and direction of our ray,


420
00:20:25.624 --> 00:20:30.196 line:-1 position:50%
we can do a raycast against
the entities in our scene.


421
00:20:30,196 --> 00:20:31,997 position:50%
The resulting CollisionCastHit


422
00:20:31,997 --> 00:20:33,866 position:50%
provides the entity
that was hit,


423
00:20:33,866 --> 00:20:37,870 position:50%
along with its position
and a surface normal.


424
00:20:37.870 --> 00:20:41.006 line:-1 position:50%
Once we identify a position in
the world to place our content,


425
00:20:41.006 --> 00:20:42.208 line:-1 position:50%
we'll add a world anchor


426
00:20:42.208 --> 00:20:46.512 line:-1 position:50%
for ARKit to continuously
track this position for us.


427
00:20:46.512 --> 00:20:48.547 line:-1 position:50%
ARKit will update
this world anchor's transform


428
00:20:48.547 --> 00:20:51.484 line:-1 position:50%
as the world map is refined.


429
00:20:51.484 --> 00:20:54.487 line:-1 position:50%
We can create a new entity
to load our ship's model,


430
00:20:54.487 --> 00:20:57.456 line:-1 position:50%
and set its transform
using the world anchor update,


431
00:20:57.456 --> 00:21:00.092 line:-1 position:50%
positioning it
where the user wanted.


432
00:21:00.092 --> 00:21:02.795 line:-1 position:50%
Finally, we can add the entity
to our content


433
00:21:02.795 --> 00:21:05.931 line:-1 position:50%
to render it
over the workbench.


434
00:21:05.931 --> 00:21:08.300 line:-1 position:50%
Whenever ARKit updates
the world anchor we added,


435
00:21:08.300 --> 00:21:11.470 line:-1 position:50%
we update the transform
component of our ship entity,


436
00:21:11.470 --> 00:21:15.040 line:-1 position:50%
making sure it stays anchored
to the real world.


437
00:21:15.040 --> 00:21:16.075 line:-1 position:50%
And that's it!


438
00:21:16.075 --> 00:21:19.311 line:-1 position:50%
We used our hands to point to
a location in our surroundings


439
00:21:19.311 --> 00:21:21.447 line:-1 position:50%
and placed content there.


440
00:21:21.447 --> 00:21:24.049 line:-1 position:50%
Raycasting is not only useful
for placing content,


441
00:21:24.049 --> 00:21:26.519 line:-1 position:50%
but also for
interacting with it.


442
00:21:26.519 --> 00:21:27.620 line:-1 position:50%
Let's see what it takes


443
00:21:27.620 --> 00:21:30.723 line:-1 position:50%
to raycast against
our virtual ship.


444
00:21:30.723 --> 00:21:33.559 line:-1 position:50%
RealityKit collision components
are very powerful.


445
00:21:33.559 --> 00:21:35.961 line:-1 position:50%
We can let the ship entity
participate in collisions


446
00:21:35.961 --> 00:21:38.731 line:-1 position:50%
by simply adding an appropriate
collision component to it,


447
00:21:38.731 --> 00:21:42.835 line:-1 position:50%
which Reality Composer Pro
can help us with.


448
00:21:42,835 --> 00:21:45,070 position:50%
After enabling the ship's
collision component


449
00:21:45,070 --> 00:21:48,340 position:50%
and building a new ray from
the latest hand joint positions,


450
00:21:48,340 --> 00:21:49,742 position:50%
we can do another raycast


451
00:21:49,742 --> 00:21:53,846 position:50%
and tell if the user is pointing
at the ship or the table.


452
00:21:53.846 --> 00:21:55.414 line:-1 position:50%
The previous examples
demonstrated


453
00:21:55.414 --> 00:21:59.018 line:-1 position:50%
the power and versatility of
combining RealityKit's features


454
00:21:59.018 --> 00:22:00.686 line:-1 position:50%
with ARKit's
scene understanding


455
00:22:00.686 --> 00:22:03.522 line:-1 position:50%
to build truly compelling
experiences.


456
00:22:03.522 --> 00:22:08.427 line:-1 position:50%
Lets see how using ARkit has
changed for spatial computing.


457
00:22:08.427 --> 00:22:10.796 line:-1 position:50%
Fundamentally,
just as on iOS,


458
00:22:10.796 --> 00:22:12.798 line:-1 position:50%
ARKit still works
by running a session


459
00:22:12.798 --> 00:22:15.201 line:-1 position:50%
to receive anchor updates.


460
00:22:15.201 --> 00:22:17.069 line:-1 position:50%
How you configure
and run your session,


461
00:22:17.069 --> 00:22:19.805 line:-1 position:50%
receive anchors updates,
and persist world anchors


462
00:22:19.805 --> 00:22:21.941 line:-1 position:50%
has changed
on this new platform.


463
00:22:21.941 --> 00:22:23.943 line:-1 position:50%
Let's take a look!


464
00:22:23.943 --> 00:22:28.380 line:-1 position:50%
On iOS, ARKit provides different
configurations to chose from.


465
00:22:28.380 --> 00:22:30.482 line:-1 position:50%
Each configuration
bundles capabilities


466
00:22:30.482 --> 00:22:33.485 line:-1 position:50%
necessary for your experience.


467
00:22:33.485 --> 00:22:34.687 line:-1 position:50%
Here, for example,


468
00:22:34.687 --> 00:22:37.323 line:-1 position:50%
we selected
ARWorldTrackingConfiguration,


469
00:22:37.323 --> 00:22:39.925 line:-1 position:50%
and will enable
sceneReconstruction for meshes


470
00:22:39.925 --> 00:22:43.095 line:-1 position:50%
and planeDetection
for planes.


471
00:22:43.095 --> 00:22:45.698 line:-1 position:50%
We can then
create our ARSession


472
00:22:45.698 --> 00:22:49.868 line:-1 position:50%
and run it with
the selected configuration.


473
00:22:49.868 --> 00:22:53.539 line:-1 position:50%
On this new platform, ARKit
now exposes a data provider


474
00:22:53.539 --> 00:22:56.775 line:-1 position:50%
for each scene understanding
capability.


475
00:22:56.775 --> 00:23:00.112 line:-1 position:50%
Hand tracking is a new
capability offered by ARKit


476
00:23:00.112 --> 00:23:02.781 line:-1 position:50%
and gets its own provider
as well.


477
00:23:02.781 --> 00:23:05.284 line:-1 position:50%
Each data provider's initializer
takes the parameters


478
00:23:05.284 --> 00:23:08.621 line:-1 position:50%
needed to configure
that provider instance.


479
00:23:08.621 --> 00:23:11.156 line:-1 position:50%
Now instead of choosing
from a catalogue


480
00:23:11.156 --> 00:23:14.560 line:-1 position:50%
of preset configurations,
you get an à la carte selection


481
00:23:14.560 --> 00:23:18.097 line:-1 position:50%
of the providers you need
for your application.


482
00:23:18.097 --> 00:23:21.533 line:-1 position:50%
Here for example, we choose
a SceneReconstructionProvider


483
00:23:21.533 --> 00:23:23.135 line:-1 position:50%
to receive mesh anchors


484
00:23:23.135 --> 00:23:26.839 line:-1 position:50%
and a PlaneDetectionProvider
to receive plane anchors.


485
00:23:26.839 --> 00:23:28.107 line:-1 position:50%
We create the providers


486
00:23:28.107 --> 00:23:29.975 line:-1 position:50%
and initialize
the mesh classification


487
00:23:29.975 --> 00:23:32.945 line:-1 position:50%
and plane types
we wish to receive.


488
00:23:32.945 --> 00:23:35.414 line:-1 position:50%
Then we create
an ARKitSession


489
00:23:35.414 --> 00:23:40.352 line:-1 position:50%
and run it with
the instantiated providers.


490
00:23:40.352 --> 00:23:42.187 line:-1 position:50%
Now that we have seen
how configuring your session


491
00:23:42.187 --> 00:23:44.690 line:-1 position:50%
has been simplified,
let's go and understand


492
00:23:44.690 --> 00:23:46.191 line:-1 position:50%
which way
these new data providers


493
00:23:46.191 --> 00:23:50.029 line:-1 position:50%
change the way your app
actually receives ARKit data.


494
00:23:50.029 --> 00:23:52.031 line:-1 position:50%
On iOS,
a single delegate


495
00:23:52.031 --> 00:23:54.667 line:-1 position:50%
receives anchor
and frame updates.


496
00:23:54.667 --> 00:23:57.536 line:-1 position:50%
Anchors are aggregated
and delivered with ARFrames


497
00:23:57.536 --> 00:24:00.873 line:-1 position:50%
to keep camera frames
and anchors in sync.


498
00:24:00.873 --> 00:24:02.408 line:-1 position:50%
Applications are responsible


499
00:24:02.408 --> 00:24:04.410 line:-1 position:50%
for displaying
the camera pixel buffer,


500
00:24:04.410 --> 00:24:05.711 line:-1 position:50%
and using camera transforms


501
00:24:05.711 --> 00:24:09.848 line:-1 position:50%
to register and render
tracked virtual content.


502
00:24:09.848 --> 00:24:12.551 line:-1 position:50%
Mesh and plane anchors
are delivered as base anchors,


503
00:24:12.551 --> 00:24:14.887 line:-1 position:50%
and it is up to you
to disambiguate them


504
00:24:14.887 --> 00:24:17.990 line:-1 position:50%
to figure out which is which.


505
00:24:17.990 --> 00:24:19.391 line:-1 position:50%
On our new platform,


506
00:24:19.391 --> 00:24:22.728 line:-1 position:50%
it is the data providers
that deliver anchor updates.


507
00:24:22.728 --> 00:24:25.597 line:-1 position:50%
Here are the providers
we previously configured.


508
00:24:25.597 --> 00:24:27.599 line:-1 position:50%
Once you run an ARKitSession,


509
00:24:27.599 --> 00:24:29.535 line:-1 position:50%
each provider
will immediately begin


510
00:24:29.535 --> 00:24:33.605 line:-1 position:50%
asynchronously
publishing anchor updates.


511
00:24:33.605 --> 00:24:36.342 line:-1 position:50%
The SceneReconstructionProvider
gives meshAnchors,


512
00:24:36.342 --> 00:24:39.044 line:-1 position:50%
and the planeDetectionProvider
gives us PlaneAnchors.


513
00:24:39.044 --> 00:24:42.247 line:-1 position:50%
No disambiguation
is necessary!


514
00:24:42.247 --> 00:24:44.550 line:-1 position:50%
Anchor updates come
as soon as they're available,


515
00:24:44.550 --> 00:24:48.287 line:-1 position:50%
and are decoupled from updates
of other data providers.


516
00:24:48.287 --> 00:24:52.324 line:-1 position:50%
It is important to note that
ARFrames are no longer provided.


517
00:24:52.324 --> 00:24:55.861 line:-1 position:50%
Spatial computing applications
do not need frame or camera data


518
00:24:55.861 --> 00:24:57.062 line:-1 position:50%
to display content,


519
00:24:57.062 --> 00:25:00.532 line:-1 position:50%
since this is now done
automatically by the system.


520
00:25:00.532 --> 00:25:03.702 line:-1 position:50%
Without having to package
anchor updates with an ARFrame,


521
00:25:03.702 --> 00:25:06.004 line:-1 position:50%
ARKit can now deliver them
immediately,


522
00:25:06.004 --> 00:25:08.507 line:-1 position:50%
reducing latency,
allowing your application


523
00:25:08.507 --> 00:25:12.945 line:-1 position:50%
to quickly react to updates
in the person's surroundings.


524
00:25:12.945 --> 00:25:16.148 line:-1 position:50%
Next, let's talk about
worldAnchor persistence.


525
00:25:16.148 --> 00:25:18.050 line:-1 position:50%
You will love these changes!


526
00:25:18.050 --> 00:25:19.585 line:-1 position:50%
In our raycasting examples,


527
00:25:19.585 --> 00:25:22.521 line:-1 position:50%
we used world anchors to place
and anchor virtual content


528
00:25:22.521 --> 00:25:24.189 line:-1 position:50%
to real-world positions.


529
00:25:24.189 --> 00:25:26.458 line:-1 position:50%
Your app can persist
these anchors,


530
00:25:26.458 --> 00:25:28.994 line:-1 position:50%
enabling it to automatically
receive them again,


531
00:25:28.994 --> 00:25:32.064 line:-1 position:50%
when the device returns
to the same surroundings.


532
00:25:32.064 --> 00:25:37.169 line:-1 position:50%
Let's first quickly recap
how persistence worked on iOS.


533
00:25:37.169 --> 00:25:40.305 line:-1 position:50%
On iOS, it is the application's
responsibility


534
00:25:40.305 --> 00:25:43.175 line:-1 position:50%
to handle world map
and anchor persistence.


535
00:25:43.175 --> 00:25:46.311 line:-1 position:50%
This included requesting
and saving ARKit's world map


536
00:25:46.311 --> 00:25:48.147 line:-1 position:50%
with your added anchor,


537
00:25:48.147 --> 00:25:50.516 line:-1 position:50%
adding logic to reload
the correct world map


538
00:25:50.516 --> 00:25:54.753 line:-1 position:50%
at the right time, then waiting
for relocalization to finish


539
00:25:54.753 --> 00:25:57.356 line:-1 position:50%
before receiving previously
persisted anchors


540
00:25:57.356 --> 00:26:00.993 line:-1 position:50%
and continuing
the application experience.


541
00:26:00.993 --> 00:26:03.962 line:-1 position:50%
On this new platform,
the system continuously persists


542
00:26:03.962 --> 00:26:05.731 line:-1 position:50%
the world map
in the background,


543
00:26:05.731 --> 00:26:08.033 line:-1 position:50%
seamlessly loading,
unloading,


544
00:26:08.033 --> 00:26:10.903 line:-1 position:50%
creating, and relocalizing
to existing maps


545
00:26:10.903 --> 00:26:13.072 line:-1 position:50%
as people move around.


546
00:26:13.072 --> 00:26:15.307 line:-1 position:50%
Your application does not
have to handle maps anymore,


547
00:26:15.307 --> 00:26:18.177 line:-1 position:50%
the system now
does it for you!


548
00:26:18.177 --> 00:26:20.345 line:-1 position:50%
You simply focus
on using world anchors


549
00:26:20.345 --> 00:26:24.550 line:-1 position:50%
to persist locations
of virtual content.


550
00:26:24.550 --> 00:26:25.884 line:-1 position:50%
When placing content,


551
00:26:25.884 --> 00:26:28.153 line:-1 position:50%
you'll be using the new
WorldTrackingProvider


552
00:26:28.153 --> 00:26:31.089 line:-1 position:50%
to add WorldAnchors
to the world map.


553
00:26:31.089 --> 00:26:34.760 line:-1 position:50%
The system will automatically
save these for you.


554
00:26:34.760 --> 00:26:37.329 line:-1 position:50%
The WorldTrackingProvider
will update the tracking status


555
00:26:37.329 --> 00:26:40.032 line:-1 position:50%
and transforms
of these world anchors.


556
00:26:40.032 --> 00:26:42.000 line:-1 position:50%
You can use
the WorldAnchor identifier


557
00:26:42.000 --> 00:26:47.906 line:-1 position:50%
to load or unload the
corresponding virtual content.


558
00:26:47.906 --> 00:26:50.676 line:-1 position:50%
We just highlighted a few
updates to the ARKit principles


559
00:26:50.676 --> 00:26:52.111 line:-1 position:50%
that you knew from iOS,


560
00:26:52.111 --> 00:26:54.680 line:-1 position:50%
but there is so much more
to explore!


561
00:26:54,680 --> 00:26:56,615 position:50%
For a deeper dive,
with code examples,


562
00:26:56,615 --> 00:26:58,150 position:50%
we recommend you watch


563
00:26:58,150 --> 00:27:01,086 position:50%
"Meet ARKit for
spatial computing."


564
00:27:01,086 --> 00:27:02,888 position:50%
Let's conclude this session!


565
00:27:02.888 --> 00:27:05.491 line:-1 position:50%
In this session, we provided
a high-level understanding


566
00:27:05.491 --> 00:27:07.793 line:-1 position:50%
of how ARKit
and RealityKit concepts


567
00:27:07.793 --> 00:27:09.795 line:-1 position:50%
have evolved from iOS,


568
00:27:09.795 --> 00:27:11.330 line:-1 position:50%
the changes
you need to consider,


569
00:27:11.330 --> 00:27:14.299 line:-1 position:50%
and which sessions
to watch for more details.


570
00:27:14.299 --> 00:27:15.834 line:-1 position:50%
This platform
takes on many tasks


571
00:27:15.834 --> 00:27:19.338 line:-1 position:50%
your iOS app had to handle,
allowing you to really focus


572
00:27:19.338 --> 00:27:21.874 line:-1 position:50%
on building beautiful
content and experiences


573
00:27:21.874 --> 00:27:26.278 line:-1 position:50%
using frameworks and concepts
you're already familiar with.


574
00:27:26.278 --> 00:27:29.081 line:-1 position:50%
We are thrilled to see how you
leverage spatial computing


575
00:27:29.081 --> 00:27:32.484 line:-1 position:50%
and all of its amazing
capabilities to evolve your app!


576
00:27:32.484 --> 00:27:33.652 line:-1 position:50%
Thank you for watching!


577
00:27:33,652 --> 00:27:36,855 line:0 position:90% size:2%
♪

