1
00:00:00,506 --> 00:00:05,460
[ Music ]


2
00:00:07,096 --> 00:00:07,976
>> Good afternoon.


3
00:00:08,516 --> 00:00:14,716
[ Applause ]


4
00:00:15,216 --> 00:00:16,085
Hello, everyone.


5
00:00:16,346 --> 00:00:18,076
And welcome to our session on


6
00:00:18,076 --> 00:00:19,636
introducing ARKit 3.


7
00:00:20,046 --> 00:00:21,286
My name is Andreas.


8
00:00:21,606 --> 00:00:23,106
I'm an engineer on the ARKit


9
00:00:23,146 --> 00:00:23,466
team.


10
00:00:23,926 --> 00:00:25,196
And I couldn't be more excited


11
00:00:25,196 --> 00:00:26,896
to be here today to tell you all


12
00:00:26,896 --> 00:00:29,666
about the third major release of


13
00:00:29,666 --> 00:00:30,016
ARKit.


14
00:00:32,046 --> 00:00:33,926
When we introduced ARKit in


15
00:00:33,926 --> 00:00:37,196
2017, we made iOS the largest AR


16
00:00:37,196 --> 00:00:39,976
platform in the world, bringing


17
00:00:39,976 --> 00:00:42,116
AR to hundreds of millions of


18
00:00:42,216 --> 00:00:43,076
iOS devices.


19
00:00:43,236 --> 00:00:45,496
And this is really significant


20
00:00:45,496 --> 00:00:47,516
for you because that's the wide


21
00:00:47,516 --> 00:00:49,506
audience that you can reach with


22
00:00:49,506 --> 00:00:50,966
the apps and the games you


23
00:00:50,966 --> 00:00:52,896
write.


24
00:00:53,176 --> 00:00:54,626
Our mission has been from the


25
00:00:54,626 --> 00:00:55,946
beginning to make it really


26
00:00:56,066 --> 00:00:58,056
simple for you to write your


27
00:00:58,056 --> 00:01:00,106
first augmented reality app even


28
00:01:00,106 --> 00:01:01,186
if you're a new developer.


29
00:01:01,186 --> 00:01:03,626
But we also want to give you the


30
00:01:03,696 --> 00:01:05,906
tools at hand that you need to


31
00:01:05,906 --> 00:01:07,726
create really advanced and


32
00:01:07,726 --> 00:01:09,126
sophisticated experiences.


33
00:01:09,126 --> 00:01:12,116
So if you look at the App Store


34
00:01:12,116 --> 00:01:14,716
today, we can see that you did


35
00:01:14,716 --> 00:01:15,896
an amazing job.


36
00:01:15,896 --> 00:01:17,216
You built great apps and great


37
00:01:17,216 --> 00:01:17,606
games.


38
00:01:18,126 --> 00:01:19,426
So let's just have a look at a


39
00:01:19,426 --> 00:01:22,956
few of them now.


40
00:01:22,956 --> 00:01:25,086
Bringing your game idea into AR


41
00:01:25,086 --> 00:01:26,866
can make them so much more


42
00:01:26,866 --> 00:01:28,626
engaging and physical, like


43
00:01:28,626 --> 00:01:29,426
Angry Birds.


44
00:01:29,776 --> 00:01:31,226
You can now play it in AR,


45
00:01:31,226 --> 00:01:32,446
actually work around those


46
00:01:32,446 --> 00:01:35,146
structures and identify the best


47
00:01:35,146 --> 00:01:38,076
spot to shoot and then have to


48
00:01:38,076 --> 00:01:39,436
slingshot right into your own


49
00:01:39,436 --> 00:01:41,616
hand and fire off those angry


50
00:01:41,676 --> 00:01:41,926
birds.


51
00:01:45,196 --> 00:01:47,426
ARKit also works really well for


52
00:01:47,426 --> 00:01:48,806
large scale use cases.


53
00:01:49,416 --> 00:01:51,476
iScape is a tool for outdoor


54
00:01:51,476 --> 00:01:52,236
landscaping.


55
00:01:52,236 --> 00:01:54,856
We can place bushes and trees


56
00:01:54,856 --> 00:01:56,116
and watch your next garden


57
00:01:56,116 --> 00:01:57,776
remodeling project come to life


58
00:01:57,776 --> 00:02:00,426
in AR right in your own garden


59
00:02:00,506 --> 00:02:02,976
or backyard.


60
00:02:03,056 --> 00:02:05,706
Last year, with ARKit 2, we


61
00:02:05,706 --> 00:02:08,746
introduced USDZ, a new 3D file


62
00:02:08,746 --> 00:02:11,766
format for exchanging formats


63
00:02:11,766 --> 00:02:13,106
right made for AR.


64
00:02:14,126 --> 00:02:16,116
Wayfair uses this to place


65
00:02:16,116 --> 00:02:17,876
virtual furniture right in your


66
00:02:17,876 --> 00:02:18,136
home.


67
00:02:18,136 --> 00:02:21,266
But leveraging ARKit's advanced


68
00:02:21,366 --> 00:02:22,806
scene understanding features


69
00:02:22,806 --> 00:02:24,226
like environment texturing,


70
00:02:24,396 --> 00:02:25,646
these objects really blend


71
00:02:25,646 --> 00:02:27,626
nicely with your living room.


72
00:02:27,776 --> 00:02:31,236
And Lego is making use of


73
00:02:31,236 --> 00:02:33,026
ARKit's 3D object detection


74
00:02:33,026 --> 00:02:33,836
capabilities.


75
00:02:35,326 --> 00:02:37,006
It finds your physical Lego set


76
00:02:37,006 --> 00:02:38,696
and enhances it in AR.


77
00:02:38,696 --> 00:02:40,706
And thanks to ARKit's multi-user


78
00:02:40,706 --> 00:02:42,456
support, you can even play


79
00:02:42,456 --> 00:02:44,906
together with your friends.


80
00:02:45,396 --> 00:02:47,486
So these are just a few examples


81
00:02:47,486 --> 00:02:48,686
of what you have created.


82
00:02:49,476 --> 00:02:52,366
ARKit helps you to take care all


83
00:02:52,786 --> 00:02:54,296
of the technical details, do the


84
00:02:54,296 --> 00:02:56,266
heavy-lifting for you to make


85
00:02:56,266 --> 00:02:57,886
augmented reality work so that


86
00:02:58,016 --> 00:02:59,886
you can really focus on creating


87
00:03:00,186 --> 00:03:01,676
the great experiences around


88
00:03:01,916 --> 00:03:02,216
them.


89
00:03:03,016 --> 00:03:04,986
Let's do a quick recap of the


90
00:03:05,186 --> 00:03:06,256
three main pillars of


91
00:03:06,256 --> 00:03:08,036
functionality that ARKit


92
00:03:08,166 --> 00:03:09,046
provides to you.


93
00:03:10,806 --> 00:03:12,376
First, there is tracking.


94
00:03:12,376 --> 00:03:16,206
Tracking determines where your


95
00:03:16,206 --> 00:03:17,886
device is with regard to the


96
00:03:17,886 --> 00:03:19,886
environment so that virtual


97
00:03:19,966 --> 00:03:21,596
content can be positioned


98
00:03:21,706 --> 00:03:24,026
accurately and updated correctly


99
00:03:24,026 --> 00:03:25,516
on top of the camera image in


100
00:03:25,516 --> 00:03:26,046
real time.


101
00:03:27,186 --> 00:03:28,876
This creates then the illusion


102
00:03:29,136 --> 00:03:30,876
that virtual content is actually


103
00:03:30,996 --> 00:03:32,736
placed in the real world.


104
00:03:33,906 --> 00:03:35,576
ARKit also provides you with


105
00:03:35,636 --> 00:03:37,116
different tracking technologies


106
00:03:37,116 --> 00:03:38,766
such as road World Tracking,


107
00:03:39,136 --> 00:03:42,306
face tracking or image tracking.


108
00:03:42,996 --> 00:03:44,736
On top of tracking, we have


109
00:03:44,886 --> 00:03:45,766
scene understanding.


110
00:03:47,606 --> 00:03:49,036
With scene understanding, you


111
00:03:49,036 --> 00:03:51,696
can identify surfaces, images,


112
00:03:51,826 --> 00:03:54,266
and 3D objects in the scene and


113
00:03:54,266 --> 00:03:56,006
attach virtual content right on


114
00:03:56,076 --> 00:03:56,566
top of them.


115
00:03:58,196 --> 00:03:59,696
Scene understanding also learns


116
00:03:59,696 --> 00:04:01,416
about the lighting and even


117
00:04:01,456 --> 00:04:03,556
textures in the environment to


118
00:04:03,556 --> 00:04:04,976
help make your content look


119
00:04:05,346 --> 00:04:06,226
really realistic.


120
00:04:06,486 --> 00:04:09,016
And finally, rendering.


121
00:04:09,146 --> 00:04:10,836
It brings your 3D content to


122
00:04:10,836 --> 00:04:11,266
life.


123
00:04:12,806 --> 00:04:14,066
We've been supporting different


124
00:04:14,066 --> 00:04:15,686
renderers like SceneKit,


125
00:04:15,986 --> 00:04:17,406
SpriteKit, and Metal.


126
00:04:17,446 --> 00:04:19,906
And now, this year also


127
00:04:20,166 --> 00:04:22,176
RealityKit, designed from the


128
00:04:22,176 --> 00:04:24,226
ground up with augmented reality


129
00:04:24,226 --> 00:04:24,666
in mind.


130
00:04:27,416 --> 00:04:29,046
So with this year's release of


131
00:04:29,046 --> 00:04:30,886
ARKit, they're making a huge


132
00:04:30,886 --> 00:04:31,536
leap forward.


133
00:04:32,416 --> 00:04:34,526
Not only are your experiences


134
00:04:34,766 --> 00:04:36,236
going to look even better and


135
00:04:36,316 --> 00:04:38,616
feel more natural, you will also


136
00:04:38,616 --> 00:04:40,886
be able to create entirely new


137
00:04:40,886 --> 00:04:43,396
experiences for use cases that


138
00:04:43,396 --> 00:04:44,936
haven't been possible before.


139
00:04:45,056 --> 00:04:47,646
Thanks to many new features


140
00:04:47,756 --> 00:04:49,706
we're bringing with ARKit 3,


141
00:04:50,796 --> 00:04:53,116
like people occlusion, motion


142
00:04:53,116 --> 00:04:55,286
capture, collaborative sessions,


143
00:04:55,826 --> 00:04:57,396
simultaneous use of the front


144
00:04:57,396 --> 00:04:59,246
and back camera, tracking


145
00:04:59,246 --> 00:05:00,836
multiple faces, and many more.


146
00:05:01,616 --> 00:05:02,966
We've got a lot to cover so


147
00:05:02,966 --> 00:05:05,586
let's dive right in and we're


148
00:05:05,586 --> 00:05:07,986
starting with people occlusion.


149
00:05:07,986 --> 00:05:11,806
Let's have a look at this scene


150
00:05:11,856 --> 00:05:12,066
here.


151
00:05:13,356 --> 00:05:14,406
So in order to create a


152
00:05:14,406 --> 00:05:16,846
convincing AR experience, it's


153
00:05:16,846 --> 00:05:18,586
important to position virtual


154
00:05:18,736 --> 00:05:21,226
content accurately and also to


155
00:05:21,226 --> 00:05:22,536
match the real world lighting.


156
00:05:23,626 --> 00:05:25,076
So let's bring in a virtual


157
00:05:25,406 --> 00:05:26,966
espresso machine here and put it


158
00:05:26,966 --> 00:05:27,746
right on the table.


159
00:05:29,146 --> 00:05:31,066
But wait, when people are in the


160
00:05:31,176 --> 00:05:32,616
frame, as in this example, it


161
00:05:32,646 --> 00:05:34,356
can easily break the illusion,


162
00:05:34,996 --> 00:05:35,956
because you would expect the


163
00:05:36,086 --> 00:05:37,996
person on the front to actually


164
00:05:38,136 --> 00:05:39,356
cover the espresso machine.


165
00:05:41,266 --> 00:05:43,186
So with ARKit 3 and people


166
00:05:43,306 --> 00:05:45,336
occlusion, you can now solve


167
00:05:45,406 --> 00:05:45,996
that problem.


168
00:05:47,106 --> 00:05:49,106
[ Applause ]


169
00:05:49,236 --> 00:05:49,526
Thank you.


170
00:05:51,766 --> 00:05:53,326
So let's have a look at how this


171
00:05:53,326 --> 00:05:53,666
is done.


172
00:05:55,546 --> 00:05:57,356
Virtual content by default is


173
00:05:57,356 --> 00:05:59,256
rendered on top of the camera


174
00:05:59,256 --> 00:05:59,736
image.


175
00:05:59,826 --> 00:06:03,096
As you can see here, for pure


176
00:06:03,236 --> 00:06:04,726
tabletop experience, this is


177
00:06:04,726 --> 00:06:06,296
fine, but if any people in the


178
00:06:06,296 --> 00:06:07,626
frame are in front of that


179
00:06:07,626 --> 00:06:09,806
object, the augmentation doesn't


180
00:06:09,806 --> 00:06:10,816
look correct anymore.


181
00:06:12,046 --> 00:06:14,566
So what ARKit 3 now does for you


182
00:06:15,456 --> 00:06:17,396
is, thanks to machine learning,


183
00:06:17,676 --> 00:06:19,316
recognized people present in the


184
00:06:19,316 --> 00:06:21,266
frame and then create a separate


185
00:06:21,266 --> 00:06:23,506
layer that has only the pixels


186
00:06:24,026 --> 00:06:25,176
containing these people.


187
00:06:25,556 --> 00:06:26,976
We call that segmentation.


188
00:06:28,286 --> 00:06:30,226
Then we can render that layer on


189
00:06:30,226 --> 00:06:31,216
top of everything else.


190
00:06:31,456 --> 00:06:33,806
Let's have a look at the


191
00:06:33,806 --> 00:06:34,776
composite image.


192
00:06:35,416 --> 00:06:36,696
It's looking much better now.


193
00:06:37,676 --> 00:06:38,786
But if you look closely, it's


194
00:06:38,786 --> 00:06:39,996
still not entirely correct.


195
00:06:41,206 --> 00:06:42,836
So the person the front is now


196
00:06:43,146 --> 00:06:45,106
occluding the espresso machine.


197
00:06:46,246 --> 00:06:47,506
But if you zoom in here, then


198
00:06:47,506 --> 00:06:49,086
you can see that also the person


199
00:06:49,086 --> 00:06:51,066
in the back is rendered on top


200
00:06:51,066 --> 00:06:53,206
of the virtual object, although


201
00:06:53,306 --> 00:06:54,876
she is actually standing behind


202
00:06:54,876 --> 00:06:55,376
the table.


203
00:06:55,856 --> 00:06:57,616
So the virtual model in that


204
00:06:57,706 --> 00:06:59,756
case should occlude her, not the


205
00:06:59,756 --> 00:07:00,406
other way around.


206
00:07:00,406 --> 00:07:04,446
Now, that happened because we


207
00:07:04,446 --> 00:07:06,396
did not take the distance of


208
00:07:06,516 --> 00:07:07,886
people from the camera into


209
00:07:08,036 --> 00:07:08,316
account.


210
00:07:10,786 --> 00:07:12,816
When ARKit 3 uses advanced


211
00:07:12,816 --> 00:07:14,246
machine learning to do an


212
00:07:14,246 --> 00:07:15,716
additional depth estimation


213
00:07:15,716 --> 00:07:18,446
step, with this estimate, how


214
00:07:18,446 --> 00:07:19,886
far does segmented people are


215
00:07:19,886 --> 00:07:22,306
away from the camera, we can now


216
00:07:22,346 --> 00:07:23,786
correct the rendering order and


217
00:07:23,786 --> 00:07:26,016
make sure to render only people


218
00:07:26,016 --> 00:07:27,656
upfront if they are actually


219
00:07:27,686 --> 00:07:28,586
closer to the camera.


220
00:07:28,746 --> 00:07:31,286
And thanks to the power of Apple


221
00:07:31,286 --> 00:07:33,606
Neural Engine, we are able to do


222
00:07:33,606 --> 00:07:35,826
this on every frame in real


223
00:07:35,966 --> 00:07:36,206
time.


224
00:07:36,206 --> 00:07:40,736
So let's have a look at the


225
00:07:40,736 --> 00:07:41,876
composite image now.


226
00:07:42,636 --> 00:07:43,436
You see that people are


227
00:07:43,536 --> 00:07:44,946
occluding the virtual content


228
00:07:45,246 --> 00:07:47,186
just as you would expect


229
00:07:47,186 --> 00:07:49,016
resulting in a convincing AR


230
00:07:49,016 --> 00:07:49,676
experience.


231
00:07:50,356 --> 00:07:52,356
[ Applause ]


232
00:07:52,696 --> 00:07:53,316
That is great.


233
00:07:53,566 --> 00:07:53,976
Thank you.


234
00:07:58,146 --> 00:08:00,946
So people occlusion enables


235
00:08:00,946 --> 00:08:02,346
virtual content to be rendered


236
00:08:02,376 --> 00:08:03,186
behind people.


237
00:08:04,296 --> 00:08:05,856
It works also for multiple


238
00:08:05,906 --> 00:08:08,276
people in the scene, and it even


239
00:08:08,276 --> 00:08:09,726
works if people are only


240
00:08:09,796 --> 00:08:11,936
partially visible like in the


241
00:08:11,936 --> 00:08:13,696
example before the woman behind


242
00:08:13,696 --> 00:08:15,766
the table actually was not


243
00:08:15,766 --> 00:08:17,806
visible with the full body but


244
00:08:17,806 --> 00:08:18,726
it still is working.


245
00:08:19,816 --> 00:08:21,196
Now, this is really significant


246
00:08:21,196 --> 00:08:23,166
because it does not only make


247
00:08:23,166 --> 00:08:25,226
your experiences look way more


248
00:08:25,226 --> 00:08:27,686
realistic than before, it also


249
00:08:27,976 --> 00:08:29,356
means for you that you can now


250
00:08:29,356 --> 00:08:31,386
create experiences that haven't


251
00:08:31,386 --> 00:08:32,556
been impossible before.


252
00:08:33,446 --> 00:08:34,566
For example, think of a


253
00:08:34,566 --> 00:08:36,655
multiplayer game where you have


254
00:08:36,966 --> 00:08:38,556
people in the frame together


255
00:08:38,856 --> 00:08:40,666
with your virtual content.


256
00:08:42,976 --> 00:08:44,786
People occlusion is integrated


257
00:08:44,876 --> 00:08:48,106
right in ARView and ARSCNView as


258
00:08:48,106 --> 00:08:48,396
well.


259
00:08:48,396 --> 00:08:51,616
And thanks to depth estimation,


260
00:08:51,736 --> 00:08:53,326
we can provide you with an


261
00:08:53,326 --> 00:08:55,456
approximation of the distance of


262
00:08:55,516 --> 00:08:57,506
the people detected with regard


263
00:08:57,706 --> 00:09:00,436
to the camera.


264
00:09:00,436 --> 00:09:02,106
We're using Apple Neural Engine


265
00:09:02,556 --> 00:09:03,266
to do that work.


266
00:09:03,796 --> 00:09:05,236
So people occlusion will work on


267
00:09:05,236 --> 00:09:08,076
devices with an A12 processor or


268
00:09:08,076 --> 00:09:08,346
later.


269
00:09:08,346 --> 00:09:12,706
So let's have a look at how to


270
00:09:12,706 --> 00:09:13,806
turn this on in API.


271
00:09:14,716 --> 00:09:16,996
We have a new property on


272
00:09:17,146 --> 00:09:18,966
ARConfiguration called


273
00:09:18,966 --> 00:09:20,106
FrameSemantics.


274
00:09:21,256 --> 00:09:23,086
This will give you different


275
00:09:23,146 --> 00:09:25,036
kinds of semantic information of


276
00:09:25,096 --> 00:09:26,256
what's in the current frame.


277
00:09:27,686 --> 00:09:29,416
You can also check if a certain


278
00:09:29,416 --> 00:09:31,336
semantic is available on the


279
00:09:31,516 --> 00:09:33,616
specific configuration or device


280
00:09:34,246 --> 00:09:36,326
with an additional method on the


281
00:09:36,326 --> 00:09:37,176
ARConfiguration.


282
00:09:38,646 --> 00:09:40,306
Specific for people occlusion,


283
00:09:40,426 --> 00:09:41,936
there are two methods available


284
00:09:41,936 --> 00:09:46,096
that you can use.


285
00:09:46,316 --> 00:09:47,756
One option is person


286
00:09:47,756 --> 00:09:48,476
segmentation.


287
00:09:49,186 --> 00:09:51,096
This will-- you provide just


288
00:09:51,356 --> 00:09:52,986
with the segmentation of people


289
00:09:53,436 --> 00:09:55,146
rendered on top of the camera


290
00:09:55,146 --> 00:09:55,606
image.


291
00:09:56,826 --> 00:09:58,096
That's the best choice if you


292
00:09:58,096 --> 00:09:59,826
know that people will always be


293
00:09:59,826 --> 00:10:01,736
standing upfront and your


294
00:10:01,736 --> 00:10:03,676
virtual content will always be


295
00:10:03,856 --> 00:10:04,826
behind those people.


296
00:10:05,476 --> 00:10:07,386
For example, a green screen use


297
00:10:07,386 --> 00:10:09,026
case just that you don't need a


298
00:10:09,026 --> 00:10:10,346
green screen anymore now.


299
00:10:10,616 --> 00:10:13,486
And the other option is person


300
00:10:13,486 --> 00:10:15,316
segmentation with depth.


301
00:10:16,016 --> 00:10:17,266
This will provide you with


302
00:10:17,406 --> 00:10:19,666
additional depth estimation of


303
00:10:19,816 --> 00:10:21,616
how far those people are away


304
00:10:21,616 --> 00:10:22,326
from the camera.


305
00:10:23,266 --> 00:10:24,866
That's the best choice if people


306
00:10:24,866 --> 00:10:26,546
might be visible together with


307
00:10:26,586 --> 00:10:29,236
virtual content either behind or


308
00:10:29,236 --> 00:10:33,286
in front of that content.


309
00:10:33,386 --> 00:10:34,876
If you do your own rendering


310
00:10:35,046 --> 00:10:37,506
using Metal or for advanced used


311
00:10:37,506 --> 00:10:39,616
cases, you can also directly


312
00:10:39,616 --> 00:10:42,106
access the pixel buffers with


313
00:10:42,106 --> 00:10:43,576
the segmentation and the


314
00:10:43,576 --> 00:10:45,996
estimated depth data on every


315
00:10:45,996 --> 00:10:46,516
ARFrame.


316
00:10:47,756 --> 00:10:50,446
So now, let me show you how


317
00:10:50,446 --> 00:10:52,196
people occlusion works in a live


318
00:10:52,286 --> 00:10:52,976
demo.


319
00:10:56,516 --> 00:11:02,056
[ Applause ]


320
00:11:02,556 --> 00:11:04,356
So here in Xcode, I have a small


321
00:11:04,356 --> 00:11:06,706
sample project using the new


322
00:11:06,706 --> 00:11:07,656
RealityKit API.


323
00:11:08,816 --> 00:11:10,256
And let me quickly walk you


324
00:11:10,256 --> 00:11:11,456
through what it does.


325
00:11:12,806 --> 00:11:15,296
So in our viewDidLoad method,


326
00:11:15,636 --> 00:11:17,666
we're creating an AnchorEntity


327
00:11:18,456 --> 00:11:19,926
looking for horizontal planes


328
00:11:20,486 --> 00:11:21,936
and we're adding this anchor


329
00:11:21,936 --> 00:11:25,146
entity to the scene.


330
00:11:25,146 --> 00:11:27,386
Then, we're retrieving a URL of


331
00:11:27,386 --> 00:11:30,816
a model to load and load it


332
00:11:31,186 --> 00:11:33,136
using ModelEntity's asynchronous


333
00:11:33,136 --> 00:11:34,536
mode loading API.


334
00:11:34,536 --> 00:11:38,386
We add the entity as a child of


335
00:11:38,386 --> 00:11:42,466
our anchor, and also install


336
00:11:42,466 --> 00:11:44,216
gestures so that I will be able


337
00:11:44,266 --> 00:11:46,276
to drag the object around on the


338
00:11:46,326 --> 00:11:46,736
plane.


339
00:11:47,976 --> 00:11:49,996
So what this does, thanks to


340
00:11:49,996 --> 00:11:51,886
RealityKit, is automatically


341
00:11:52,006 --> 00:11:53,126
setup a World Tracking


342
00:11:53,126 --> 00:11:55,406
configuration because we know


343
00:11:55,406 --> 00:11:57,476
that we need World Tracking for


344
00:11:57,646 --> 00:11:58,536
plane estimation.


345
00:11:59,116 --> 00:12:00,916
And then, as soon as a plane is


346
00:12:00,956 --> 00:12:02,366
detected, the content will


347
00:12:02,366 --> 00:12:03,586
automatically be placed.


348
00:12:04,636 --> 00:12:07,596
Now, this is not using people


349
00:12:07,596 --> 00:12:08,296
occlusion yet.


350
00:12:09,516 --> 00:12:11,386
But I have already a stop to


351
00:12:11,386 --> 00:12:12,106
turn this on.


352
00:12:12,866 --> 00:12:13,266
It's called


353
00:12:13,266 --> 00:12:14,986
togglePeopleOcclusion and what I


354
00:12:14,986 --> 00:12:16,556
want to implement is a method


355
00:12:16,786 --> 00:12:18,326
that lets me switch people


356
00:12:18,326 --> 00:12:19,946
occlusion on and off when the


357
00:12:19,946 --> 00:12:21,136
user taps the screen.


358
00:12:22,256 --> 00:12:23,986
So let's go ahead and implement


359
00:12:23,986 --> 00:12:24,376
it now.


360
00:12:25,046 --> 00:12:28,306
So the first thing I'm going to


361
00:12:28,436 --> 00:12:31,756
do is actually check if my World


362
00:12:31,756 --> 00:12:34,076
Tracking configuration supports


363
00:12:34,656 --> 00:12:35,946
the person segmentation with


364
00:12:36,046 --> 00:12:37,296
depth frame semantics.


365
00:12:38,176 --> 00:12:39,406
It's recommended that you do


366
00:12:39,406 --> 00:12:42,776
that always because if the code


367
00:12:42,776 --> 00:12:44,846
is run on a device that it does


368
00:12:44,846 --> 00:12:47,166
not have Apple Neural Engine and


369
00:12:47,286 --> 00:12:48,426
this frame semantic is not


370
00:12:48,426 --> 00:12:50,646
supported, we want to gracefully


371
00:12:50,646 --> 00:12:53,396
handle that case.


372
00:12:53,606 --> 00:12:56,066
So if that's the case, we would


373
00:12:56,066 --> 00:12:57,186
just display a message to the


374
00:12:57,186 --> 00:12:58,506
user that people occlusion is


375
00:12:58,546 --> 00:13:00,496
not available on that device.


376
00:13:03,456 --> 00:13:05,186
And let's actually move on and


377
00:13:05,186 --> 00:13:06,826
implement the toggle.


378
00:13:08,056 --> 00:13:09,386
I'm going to do a switch


379
00:13:09,386 --> 00:13:11,436
statement on the frameSemantics


380
00:13:11,506 --> 00:13:13,026
property of our configuration


381
00:13:13,026 --> 00:13:13,276
here.


382
00:13:13,916 --> 00:13:14,496
And if


383
00:13:14,546 --> 00:13:16,586
personSegmentationWithDepth is


384
00:13:16,586 --> 00:13:19,036
part of the frameSemantics, then


385
00:13:19,036 --> 00:13:22,116
we remove it and tell the user


386
00:13:22,116 --> 00:13:23,306
that people occlusion is now


387
00:13:23,406 --> 00:13:23,786
turned off.


388
00:13:24,446 --> 00:13:28,486
And I just need to implement the


389
00:13:28,486 --> 00:13:29,436
other case as well.


390
00:13:30,046 --> 00:13:30,986
If you don't have person


391
00:13:30,986 --> 00:13:32,756
segmentation enable then, we


392
00:13:32,756 --> 00:13:35,416
insert the frameSemantics into


393
00:13:36,356 --> 00:13:38,096
different semantics property and


394
00:13:38,096 --> 00:13:39,696
display a message that we now


395
00:13:39,786 --> 00:13:42,766
turn people occlusion on.


396
00:13:43,346 --> 00:13:45,856
Now, I need to rerun the updated


397
00:13:45,886 --> 00:13:47,326
configuration on my session.


398
00:13:48,516 --> 00:13:51,086
So let me retrieve the session


399
00:13:51,806 --> 00:13:55,136
from the ARView and call run


400
00:13:55,676 --> 00:13:57,196
with a configuration that I've


401
00:13:57,256 --> 00:13:58,826
just updated here.


402
00:14:00,036 --> 00:14:01,786
So now, the implementation of my


403
00:14:01,786 --> 00:14:03,936
togglePeopleOcclusion method is


404
00:14:03,936 --> 00:14:06,446
finished, now I need to make


405
00:14:06,446 --> 00:14:08,116
sure that it's actually called


406
00:14:08,776 --> 00:14:10,146
when the user taps the screen.


407
00:14:11,336 --> 00:14:12,696
I already installed a tap


408
00:14:12,696 --> 00:14:14,786
gesture recognizer and here in


409
00:14:14,786 --> 00:14:18,476
my onTap method, I just call


410
00:14:19,796 --> 00:14:20,736
togglePeopleOcclusion.


411
00:14:21,396 --> 00:14:24,266
And that's all I need to do.


412
00:14:25,376 --> 00:14:27,236
And now, let me go ahead and


413
00:14:28,556 --> 00:14:31,466
build that code and run it on my


414
00:14:31,466 --> 00:14:33,086
device here.


415
00:14:34,346 --> 00:14:36,876
And we already see that the


416
00:14:36,926 --> 00:14:39,166
plane was detected and the


417
00:14:39,166 --> 00:14:40,066
content placed.


418
00:14:40,066 --> 00:14:41,856
I can also move it around,


419
00:14:41,976 --> 00:14:43,856
thanks to the gestures that I've


420
00:14:43,856 --> 00:14:44,226
added.


421
00:14:45,026 --> 00:14:46,886
You see that RealityKit has


422
00:14:46,886 --> 00:14:48,966
added a nice grounding shadow.


423
00:14:49,726 --> 00:14:52,806
And now, let's actually have a


424
00:14:52,806 --> 00:14:53,806
look at people occlusion.


425
00:14:53,946 --> 00:14:54,966
Right now, it's still turned


426
00:14:54,966 --> 00:14:55,176
off.


427
00:14:55,176 --> 00:14:57,366
So if I bring in my hand now,


428
00:14:57,486 --> 00:14:58,676
you see that the content is


429
00:14:58,676 --> 00:15:00,106
always rendered on top.


430
00:15:00,426 --> 00:15:01,736
This is the behavior that you


431
00:15:01,736 --> 00:15:04,616
know from ARKit 2.


432
00:15:04,856 --> 00:15:06,856
Now, let me turn it on and bring


433
00:15:06,856 --> 00:15:07,736
in my hand again.


434
00:15:07,736 --> 00:15:10,156
And you'll see that now


435
00:15:10,206 --> 00:15:10,966
[applause] the virtual object is


436
00:15:11,176 --> 00:15:12,866
actually covered.


437
00:15:14,516 --> 00:15:18,466
[ Applause ]


438
00:15:18,966 --> 00:15:21,646
And that's people occlusion in


439
00:15:21,646 --> 00:15:22,236
the ARKit 3.


440
00:15:27,786 --> 00:15:32,276
[Applause] Thank you.


441
00:15:32,276 --> 00:15:34,656
So, let's talk about another


442
00:15:34,656 --> 00:15:36,666
exciting new feature of ARKit 3


443
00:15:37,116 --> 00:15:38,616
which is motion capture.


444
00:15:39,746 --> 00:15:42,736
With motion capture, you can


445
00:15:42,786 --> 00:15:45,056
track the body of a person which


446
00:15:45,056 --> 00:15:46,696
can then for example be mapped


447
00:15:46,696 --> 00:15:48,666
to a virtual character in real


448
00:15:48,706 --> 00:15:49,046
time.


449
00:15:49,966 --> 00:15:51,156
Now, this could only be done


450
00:15:51,156 --> 00:15:52,626
with external setup and special


451
00:15:52,626 --> 00:15:53,776
equipment before.


452
00:15:54,466 --> 00:15:56,636
Now, with the ARKit 3, it just


453
00:15:56,706 --> 00:15:58,566
takes few lines of code and


454
00:15:58,566 --> 00:16:00,456
works right on your iPad or


455
00:16:00,456 --> 00:16:00,806
iPhone.


456
00:16:03,476 --> 00:16:06,086
So, motion capture let's you


457
00:16:06,126 --> 00:16:08,876
track a human body both in 2D


458
00:16:09,156 --> 00:16:10,026
and in 3D.


459
00:16:10,586 --> 00:16:12,976
And it provides you with a


460
00:16:12,976 --> 00:16:15,186
skeleton representation of that


461
00:16:15,186 --> 00:16:15,676
person.


462
00:16:16,286 --> 00:16:19,786
This, for example, enables to


463
00:16:19,786 --> 00:16:21,256
drive a virtual character.


464
00:16:22,036 --> 00:16:23,796
And this is made possible by


465
00:16:23,796 --> 00:16:24,856
advanced machine learning


466
00:16:24,856 --> 00:16:26,716
algorithms running on Apple


467
00:16:26,716 --> 00:16:27,426
Neural Engine.


468
00:16:28,116 --> 00:16:29,586
So it's available on devices


469
00:16:29,586 --> 00:16:31,496
with an A12 or later processor.


470
00:16:32,656 --> 00:16:34,606
Let's look at 2D body detection


471
00:16:34,606 --> 00:16:35,076
first.


472
00:16:35,676 --> 00:16:39,286
How to turn this on?


473
00:16:40,336 --> 00:16:42,186
We have a new frame semantics


474
00:16:42,186 --> 00:16:43,866
option called bodyDetection.


475
00:16:44,746 --> 00:16:46,396
This is supported on the World


476
00:16:46,396 --> 00:16:48,666
Tracking configuration and on


477
00:16:48,666 --> 00:16:50,206
image and orientation tracking


478
00:16:50,206 --> 00:16:51,416
configurations as well.


479
00:16:52,276 --> 00:16:53,506
So you'll simply add this to


480
00:16:53,506 --> 00:16:55,336
your frame semantics and call


481
00:16:55,336 --> 00:16:56,036
run on the session.


482
00:16:57,636 --> 00:16:58,486
Now, let's have a look at the


483
00:16:58,486 --> 00:17:02,696
data we will be getting back.


484
00:17:02,696 --> 00:17:05,116
Every ARFrame delivers an object


485
00:17:05,165 --> 00:17:07,945
of type ARBody2D in the


486
00:17:07,945 --> 00:17:10,596
detectedBody property if a


487
00:17:10,596 --> 00:17:12,506
person was detected.


488
00:17:13,376 --> 00:17:15,425
This object contains a 2D


489
00:17:15,425 --> 00:17:15,996
skeleton.


490
00:17:17,455 --> 00:17:19,106
And the skeleton will provide


491
00:17:19,106 --> 00:17:21,016
you with all the joint landmarks


492
00:17:21,076 --> 00:17:22,896
in normalized image space.


493
00:17:23,876 --> 00:17:24,896
They are being returned in a


494
00:17:24,896 --> 00:17:26,896
flat hierarchy in an array


495
00:17:27,026 --> 00:17:28,246
because this is most efficient


496
00:17:28,286 --> 00:17:29,036
for processing.


497
00:17:29,846 --> 00:17:31,386
But you will also be getting a


498
00:17:31,386 --> 00:17:32,486
skeleton definition.


499
00:17:33,406 --> 00:17:34,706
And thanks to the skeleton


500
00:17:34,706 --> 00:17:36,416
definition, you have all the


501
00:17:36,416 --> 00:17:38,786
information how to interpret the


502
00:17:38,786 --> 00:17:39,656
skeleton data.


503
00:17:40,446 --> 00:17:41,826
In particular, it contains


504
00:17:41,826 --> 00:17:43,356
information about the hierarchy


505
00:17:43,356 --> 00:17:45,426
of joints, for example, the fact


506
00:17:45,676 --> 00:17:47,466
that the hand joint is a child


507
00:17:47,466 --> 00:17:48,386
of the elbow joint.


508
00:17:49,786 --> 00:17:51,176
And you will also be provided


509
00:17:51,266 --> 00:17:53,486
with names for joints that can


510
00:17:53,486 --> 00:17:55,266
then be used for easier access.


511
00:17:55,936 --> 00:17:57,796
So let's have a look at how this


512
00:17:57,826 --> 00:17:58,256
looks like.


513
00:17:59,056 --> 00:18:00,406
Here's a person we detected in


514
00:18:00,406 --> 00:18:01,006
our frame.


515
00:18:01,926 --> 00:18:03,526
And that's the 2D skeleton


516
00:18:03,526 --> 00:18:05,426
provided by ARKit.


517
00:18:06,016 --> 00:18:08,256
As mentioned before, important


518
00:18:08,256 --> 00:18:10,056
joints are named to make it easy


519
00:18:10,056 --> 00:18:11,986
for you to find out the position


520
00:18:11,986 --> 00:18:13,036
of a particular one you're


521
00:18:13,036 --> 00:18:14,846
interested in, for example, the


522
00:18:14,846 --> 00:18:17,446
head or the right hand.


523
00:18:18,636 --> 00:18:19,996
So this was 2D.


524
00:18:20,626 --> 00:18:22,646
Now, let's have a look at 3D


525
00:18:22,646 --> 00:18:24,166
motion capture.


526
00:18:25,216 --> 00:18:26,766
3D motion capture let's you


527
00:18:26,766 --> 00:18:29,286
track a human body in 3D space


528
00:18:29,936 --> 00:18:31,046
and it provides you with a


529
00:18:31,266 --> 00:18:32,496
three-dimensional skeleton


530
00:18:32,496 --> 00:18:33,376
representation.


531
00:18:34,656 --> 00:18:36,496
It also provides you scale


532
00:18:36,496 --> 00:18:38,516
estimation to let you determine


533
00:18:38,516 --> 00:18:40,016
the size of the person that is


534
00:18:40,076 --> 00:18:40,696
being tracked.


535
00:18:41,746 --> 00:18:43,866
And the 3D skeleton is anchored


536
00:18:44,186 --> 00:18:45,256
in world coordinates.


537
00:18:46,516 --> 00:18:48,186
Let's see how to use this in


538
00:18:48,186 --> 00:18:48,496
API.


539
00:18:52,006 --> 00:18:53,276
We are introducing a new


540
00:18:53,336 --> 00:18:55,226
configuration called


541
00:18:55,226 --> 00:18:56,806
ARBodyTrackingConfiguration.


542
00:18:58,426 --> 00:19:00,346
So this lets you use 3D body


543
00:19:00,346 --> 00:19:02,856
tracking but it also provides


544
00:19:03,186 --> 00:19:05,476
the 2D body detection that we


545
00:19:05,476 --> 00:19:06,476
have seen before.


546
00:19:06,916 --> 00:19:09,346
So the frame semantics is turned


547
00:19:09,346 --> 00:19:10,706
on by default in that


548
00:19:10,706 --> 00:19:11,486
configuration.


549
00:19:12,686 --> 00:19:14,626
In addition, this configuration


550
00:19:14,686 --> 00:19:16,236
also tracks the device's


551
00:19:16,296 --> 00:19:18,666
position and orientation and


552
00:19:18,666 --> 00:19:20,166
provides selected World Tracking


553
00:19:20,166 --> 00:19:21,866
features such as plane


554
00:19:21,866 --> 00:19:23,926
estimation or image detection.


555
00:19:24,746 --> 00:19:26,756
So with that, you have even more


556
00:19:26,756 --> 00:19:28,666
possibilities in what you can do


557
00:19:29,316 --> 00:19:31,226
using body tracking in your AR


558
00:19:32,016 --> 00:19:32,086
app.


559
00:19:33,516 --> 00:19:35,716
In order to set it up, you


560
00:19:35,716 --> 00:19:37,426
simply create the body tracking


561
00:19:37,426 --> 00:19:39,846
configuration and run it on your


562
00:19:39,846 --> 00:19:40,256
session.


563
00:19:41,396 --> 00:19:43,096
Note that we also have API to


564
00:19:43,146 --> 00:19:45,076
check if that configuration is


565
00:19:45,076 --> 00:19:49,136
supported on the current device.


566
00:19:49,306 --> 00:19:51,746
So, now, when ARKit is running


567
00:19:52,076 --> 00:19:54,026
and detects a person, it will


568
00:19:54,026 --> 00:19:57,286
add a new type of anchor, an


569
00:19:57,286 --> 00:19:58,026
ARBodyAnchor.


570
00:20:00,096 --> 00:20:02,366
This will be provided to you in


571
00:20:02,366 --> 00:20:03,546
the session that that anchor's


572
00:20:03,546 --> 00:20:05,186
called back just like any other


573
00:20:05,186 --> 00:20:06,436
anchor types that you know.


574
00:20:07,786 --> 00:20:09,526
And just like any anchor, it


575
00:20:09,526 --> 00:20:11,796
also has a transform providing


576
00:20:11,796 --> 00:20:13,066
you with a position and


577
00:20:13,066 --> 00:20:15,136
orientation of the detected


578
00:20:15,206 --> 00:20:17,306
person in world coordinates.


579
00:20:17,856 --> 00:20:19,576
In addition, you will be getting


580
00:20:19,686 --> 00:20:22,296
the scale factor and a reference


581
00:20:22,676 --> 00:20:23,826
to the 3D skeleton.


582
00:20:24,446 --> 00:20:28,636
Let's have a look at how this


583
00:20:28,636 --> 00:20:29,176
looks like.


584
00:20:29,846 --> 00:20:31,056
You see already that it's much


585
00:20:31,056 --> 00:20:32,916
more detailed than the 2D


586
00:20:32,916 --> 00:20:33,406
skeleton.


587
00:20:34,436 --> 00:20:35,626
So the yellow joints are the


588
00:20:35,626 --> 00:20:37,036
ones that will be delivered to


589
00:20:37,036 --> 00:20:38,976
the user with motion capture


590
00:20:38,976 --> 00:20:39,316
data.


591
00:20:40,396 --> 00:20:42,516
The white ones are leaf joints


592
00:20:42,876 --> 00:20:44,326
that our additionally available


593
00:20:44,326 --> 00:20:45,046
in the skeleton.


594
00:20:46,096 --> 00:20:47,936
These are not actively tracked


595
00:20:48,246 --> 00:20:49,806
so the transform is static with


596
00:20:49,806 --> 00:20:51,156
regard to the tracked parent.


597
00:20:51,846 --> 00:20:53,486
But of course, you can direct


598
00:20:53,486 --> 00:20:55,666
the access each of those joints


599
00:20:56,006 --> 00:20:58,016
and retrieve their road


600
00:20:58,016 --> 00:20:58,766
coordinates.


601
00:21:00,016 --> 00:21:01,606
Again, we also have labels for


602
00:21:01,606 --> 00:21:04,056
the most important once, an API


603
00:21:04,326 --> 00:21:06,066
to query them by name so that


604
00:21:06,066 --> 00:21:08,826
you can easily find out about a


605
00:21:08,826 --> 00:21:10,076
particular joint you're


606
00:21:10,076 --> 00:21:11,456
interested in.


607
00:21:12,016 --> 00:21:13,686
Now, I'm sure that you can come


608
00:21:13,686 --> 00:21:15,486
up with a lot of great use cases


609
00:21:15,746 --> 00:21:18,376
for this new API, but I want to


610
00:21:18,376 --> 00:21:20,026
talk about one particular use


611
00:21:20,056 --> 00:21:21,866
case that might be interesting


612
00:21:21,866 --> 00:21:24,256
for many of you, which is


613
00:21:24,346 --> 00:21:26,036
animating 3D characters.


614
00:21:27,696 --> 00:21:30,256
By using ARKit in combination


615
00:21:30,256 --> 00:21:32,736
with RealityKit, you can drive a


616
00:21:32,736 --> 00:21:34,986
model based on the 3D skeleton


617
00:21:34,986 --> 00:21:36,646
pose and it's really simple to


618
00:21:36,646 --> 00:21:36,836
do.


619
00:21:37,936 --> 00:21:41,486
All you need is a rigged mesh.


620
00:21:42,586 --> 00:21:43,916
You can find an example for that


621
00:21:43,916 --> 00:21:45,506
in one of our sample apps that


622
00:21:45,506 --> 00:21:46,646
you can download on the session


623
00:21:46,646 --> 00:21:48,606
home page, but of course, you're


624
00:21:48,606 --> 00:21:50,586
also free to make your own in a


625
00:21:50,586 --> 00:21:51,886
content creation tool of your


626
00:21:51,886 --> 00:21:52,296
choice.


627
00:21:52,826 --> 00:21:55,836
Let's see how easy it is to do


628
00:21:55,836 --> 00:21:56,576
that in code.


629
00:21:56,856 --> 00:21:58,406
It's built right in RealityKit


630
00:21:58,406 --> 00:21:58,756
API.


631
00:21:59,736 --> 00:22:01,036
And the major class we will be


632
00:22:01,036 --> 00:22:03,686
using is the BodyTrackedEntity.


633
00:22:04,296 --> 00:22:08,596
So here is a code example using


634
00:22:08,596 --> 00:22:09,486
RealityKit API.


635
00:22:09,486 --> 00:22:12,056
The first thing you're going to


636
00:22:12,056 --> 00:22:14,676
do is create an AnchorEntity of


637
00:22:14,736 --> 00:22:16,966
type body and add this anchor to


638
00:22:16,966 --> 00:22:17,376
the scene.


639
00:22:18,786 --> 00:22:21,016
Next, you're going to load the


640
00:22:21,016 --> 00:22:21,736
model.


641
00:22:22,036 --> 00:22:23,496
In our case, it's called robot.


642
00:22:24,606 --> 00:22:26,136
We're using the asynchronous


643
00:22:26,136 --> 00:22:27,386
loading API for that.


644
00:22:27,826 --> 00:22:29,076
And in the completion handler,


645
00:22:29,516 --> 00:22:30,776
you will be getting the


646
00:22:30,776 --> 00:22:32,806
BodyTrackedEntity that we now


647
00:22:32,866 --> 00:22:35,076
just need to add as a child to


648
00:22:35,076 --> 00:22:35,906
our bodyAnchor.


649
00:22:36,896 --> 00:22:38,196
And that's already all you need


650
00:22:38,196 --> 00:22:39,126
to do.


651
00:22:39,666 --> 00:22:42,516
So as soon as ARKit now adds the


652
00:22:42,516 --> 00:22:44,536
AR body anchor to the session,


653
00:22:45,846 --> 00:22:48,176
the 3D pose of the skeleton will


654
00:22:48,176 --> 00:22:50,386
automatically be applied to the


655
00:22:50,386 --> 00:22:52,276
virtual model in real time.


656
00:22:53,466 --> 00:22:54,916
And that's how simple it is to


657
00:22:54,916 --> 00:22:56,896
do motion capture with ARKit 3.


658
00:22:58,316 --> 00:23:03,936
[Applause] Thank you.


659
00:23:03,936 --> 00:23:06,436
So, now, let's talk about


660
00:23:06,436 --> 00:23:09,226
simultaneous front and back


661
00:23:10,116 --> 00:23:10,526
camera.


662
00:23:10,526 --> 00:23:12,166
ARKit lets you to World Tracking


663
00:23:12,166 --> 00:23:14,316
with the back-facing camera and


664
00:23:14,316 --> 00:23:16,056
face tracking with the 2Depth


665
00:23:16,056 --> 00:23:17,326
camera system on the front.


666
00:23:18,176 --> 00:23:20,116
Now, one really highly requested


667
00:23:20,116 --> 00:23:22,386
feature from you was to enable


668
00:23:22,386 --> 00:23:24,896
user experiences with the front


669
00:23:24,896 --> 00:23:26,456
and the back camera together.


670
00:23:27,316 --> 00:23:29,706
Now, with ARKit 3, you can do


671
00:23:30,506 --> 00:23:30,666
that.


672
00:23:31,956 --> 00:23:33,426
So with this new feature, you


673
00:23:33,426 --> 00:23:35,596
can build AR experiences using


674
00:23:35,676 --> 00:23:37,696
both cameras at the same time.


675
00:23:37,996 --> 00:23:39,336
And what this means for you is,


676
00:23:39,866 --> 00:23:41,666
that you can now build two new


677
00:23:41,716 --> 00:23:42,866
types of use cases.


678
00:23:43,896 --> 00:23:45,976
First, you can create World


679
00:23:45,976 --> 00:23:47,106
Tracking experiences.


680
00:23:47,106 --> 00:23:48,816
So, using the back-facing


681
00:23:48,886 --> 00:23:51,156
camera, but also benefit from


682
00:23:51,156 --> 00:23:53,446
face data captured with the


683
00:23:53,446 --> 00:23:54,096
front camera.


684
00:23:55,106 --> 00:23:56,746
And you can create Face Tracking


685
00:23:56,746 --> 00:23:59,086
experiences that make use of the


686
00:23:59,216 --> 00:24:00,826
full device orientation and


687
00:24:00,826 --> 00:24:02,496
position in 6 degrees of


688
00:24:02,586 --> 00:24:02,956
freedom.


689
00:24:03,406 --> 00:24:06,896
All of this is supported on A12


690
00:24:06,896 --> 00:24:08,546
devices and later.


691
00:24:08,806 --> 00:24:10,956
Let's see an example.


692
00:24:11,426 --> 00:24:12,586
So here we're running World


693
00:24:12,586 --> 00:24:14,196
Tracking with plane estimation


694
00:24:14,806 --> 00:24:16,836
but we also placed a face mesh


695
00:24:16,836 --> 00:24:18,656
on top of the plane and are


696
00:24:18,656 --> 00:24:20,586
updating it in real time with


697
00:24:20,636 --> 00:24:23,186
the facial expressions captured


698
00:24:23,186 --> 00:24:24,216
through the front camera.


699
00:24:24,216 --> 00:24:27,436
So let' see how to use


700
00:24:27,516 --> 00:24:29,266
concurrent front and back camera


701
00:24:30,296 --> 00:24:31,336
in API.


702
00:24:31,546 --> 00:24:33,026
First, let's create a World


703
00:24:33,026 --> 00:24:34,066
Tracking configuration.


704
00:24:34,416 --> 00:24:36,416
Now, the configuration that I


705
00:24:36,506 --> 00:24:38,706
choose determines which camera


706
00:24:38,706 --> 00:24:40,406
stream is actually displayed on


707
00:24:40,406 --> 00:24:40,926
the screen.


708
00:24:41,206 --> 00:24:43,196
So in that case, it would be the


709
00:24:43,196 --> 00:24:44,256
back-facing camera.


710
00:24:45,666 --> 00:24:47,756
Now I'm turning on the new


711
00:24:47,916 --> 00:24:50,386
userFaceTrackingEnabled property


712
00:24:50,586 --> 00:24:52,046
and run the session.


713
00:24:52,366 --> 00:24:57,046
This will cause that I receive


714
00:24:57,176 --> 00:24:58,156
face anchors.


715
00:24:58,276 --> 00:25:01,326
And I can then use any


716
00:25:01,326 --> 00:25:02,876
information from that anchors


717
00:25:03,066 --> 00:25:05,496
like the face mesh, land shapes,


718
00:25:05,926 --> 00:25:08,666
or the anchors transform itself.


719
00:25:09,236 --> 00:25:11,496
Now, note, since we are working


720
00:25:11,666 --> 00:25:13,646
with world coordinates here, the


721
00:25:13,646 --> 00:25:15,256
user face transfer will be


722
00:25:15,256 --> 00:25:17,646
placed behind the camera, which


723
00:25:17,646 --> 00:25:19,196
means that in order to visualize


724
00:25:19,256 --> 00:25:20,486
the face, you would need to


725
00:25:20,616 --> 00:25:22,156
translate it to a location


726
00:25:22,156 --> 00:25:25,636
somewhere in front of the


727
00:25:25,846 --> 00:25:26,366
camera.


728
00:25:26,366 --> 00:25:28,316
Now, let's also look at the face


729
00:25:28,366 --> 00:25:29,486
tracking configuration.


730
00:25:30,366 --> 00:25:31,746
You create your face tracking


731
00:25:31,746 --> 00:25:33,646
configuration just as you would


732
00:25:33,686 --> 00:25:35,676
do it always and set


733
00:25:36,056 --> 00:25:37,496
worldTrackingEnabled to true.


734
00:25:38,226 --> 00:25:40,276
And then, after you run the


735
00:25:40,276 --> 00:25:42,876
configuration, you can access in


736
00:25:42,876 --> 00:25:45,216
every frame, for example, in the


737
00:25:45,216 --> 00:25:46,696
session that update frame


738
00:25:46,856 --> 00:25:50,386
callback the transform of the


739
00:25:50,386 --> 00:25:51,586
current camera position.


740
00:25:52,216 --> 00:25:54,506
And you can then use that for


741
00:25:54,616 --> 00:25:55,746
whatever use case you have in


742
00:25:55,746 --> 00:25:56,116
mind.


743
00:25:56,326 --> 00:25:58,626
And that's simultaneous front


744
00:25:58,626 --> 00:26:00,076
and back camera in ARKit 3.


745
00:26:01,036 --> 00:26:02,376
We think that you will be able


746
00:26:02,526 --> 00:26:04,516
to enable many great new use


747
00:26:04,516 --> 00:26:06,126
cases with this new API.


748
00:26:07,281 --> 00:26:09,281
[ Applause ]


749
00:26:09,546 --> 00:26:09,746
Thank you.


750
00:26:10,421 --> 00:26:12,421
[ Applause ]


751
00:26:12,826 --> 00:26:14,446
And now, let me hand it over


752
00:26:14,446 --> 00:26:16,506
Thomas who will tell you all


753
00:26:16,506 --> 00:26:17,816
about collaborative sessions.


754
00:26:20,516 --> 00:26:22,636
[ Applause ]


755
00:26:23,136 --> 00:26:23,426
>> Thank you.


756
00:26:24,366 --> 00:26:25,056
Thank you, Andreas.


757
00:26:25,416 --> 00:26:26,386
Good afternoon, everyone.


758
00:26:26,736 --> 00:26:28,156
My name is Thomas and I'm part


759
00:26:28,156 --> 00:26:28,876
of the ARKit team.


760
00:26:29,506 --> 00:26:30,526
So let's talk about


761
00:26:30,526 --> 00:26:31,686
collaborative sessions.


762
00:26:32,216 --> 00:26:35,276
In ARKit 2, you could create


763
00:26:35,666 --> 00:26:37,786
multiuser experiences with the


764
00:26:37,786 --> 00:26:39,606
ability to save and load world


765
00:26:39,606 --> 00:26:40,086
maps.


766
00:26:40,726 --> 00:26:42,046
You had to save a map on one


767
00:26:42,046 --> 00:26:43,686
device and send it to another


768
00:26:43,686 --> 00:26:45,756
one in order for your users to


769
00:26:45,756 --> 00:26:47,036
jump into the same experience


770
00:26:47,036 --> 00:26:47,276
again.


771
00:26:48,586 --> 00:26:50,556
It was a one map -- one-time map


772
00:26:50,556 --> 00:26:52,026
sharing experience, and after


773
00:26:52,026 --> 00:26:54,306
that point, most of the users


774
00:26:54,306 --> 00:26:55,186
wouldn't be in the same--


775
00:26:55,296 --> 00:26:56,616
wouldn't share the same


776
00:26:56,616 --> 00:26:57,506
information anymore.


777
00:26:58,006 --> 00:26:59,786
Well, with collaborative


778
00:26:59,786 --> 00:27:02,816
sessions in ARKit 3, we now


779
00:27:02,816 --> 00:27:04,416
continuously share your mapping


780
00:27:04,416 --> 00:27:05,886
information across the network.


781
00:27:07,226 --> 00:27:09,016
This allows you to create ad hoc


782
00:27:09,016 --> 00:27:11,366
multiuser experiences where your


783
00:27:11,366 --> 00:27:13,986
users are more easily accessing


784
00:27:13,986 --> 00:27:14,686
the same session.


785
00:27:15,896 --> 00:27:18,076
Additionally, we also allow you


786
00:27:18,826 --> 00:27:20,626
to share or we actually share


787
00:27:20,626 --> 00:27:22,556
ARAnchors across all devices.


788
00:27:23,186 --> 00:27:24,226
All those anchors are


789
00:27:24,226 --> 00:27:26,016
identifiable with anchor's


790
00:27:26,016 --> 00:27:28,216
session IDs on those ones.


791
00:27:29,536 --> 00:27:31,386
Note that at this point, most--


792
00:27:31,386 --> 00:27:32,846
all the coordinate systems are


793
00:27:32,846 --> 00:27:34,016
independent from each others


794
00:27:34,016 --> 00:27:35,116
even though we still share the


795
00:27:35,116 --> 00:27:37,206
information under the hood.


796
00:27:37,406 --> 00:27:38,976
Let me show you how this works.


797
00:27:41,566 --> 00:27:44,156
So in this video here, we can


798
00:27:44,156 --> 00:27:44,956
see two users.


799
00:27:45,016 --> 00:27:46,246
Pay attention to the colors.


800
00:27:46,246 --> 00:27:47,656
One user will be showing feature


801
00:27:47,656 --> 00:27:49,826
points in green and another user


802
00:27:49,826 --> 00:27:51,016
will be showing feature points


803
00:27:51,016 --> 00:27:52,896
in red.


804
00:27:53,086 --> 00:27:53,976
As they move around the


805
00:27:53,976 --> 00:27:56,976
environment, they're starting to


806
00:27:56,976 --> 00:27:59,556
map the environment and add more


807
00:27:59,556 --> 00:28:00,986
feature points do it.


808
00:28:01,976 --> 00:28:03,746
At this point, and this is the


809
00:28:03,746 --> 00:28:05,316
internal representation of their


810
00:28:05,316 --> 00:28:07,186
internal maps, they don't know


811
00:28:07,186 --> 00:28:08,246
about each other's maps.


812
00:28:08,506 --> 00:28:13,616
As they move around, they gather


813
00:28:13,616 --> 00:28:14,576
more feature points.


814
00:28:17,796 --> 00:28:19,026
When they gather more feature


815
00:28:19,026 --> 00:28:20,776
points in the scene and you can


816
00:28:20,776 --> 00:28:21,876
see the internal maps, and pay


817
00:28:21,876 --> 00:28:23,426
attention to the color and their


818
00:28:23,426 --> 00:28:25,416
final matching point, then those


819
00:28:25,416 --> 00:28:27,256
internal map will then merge


820
00:28:27,256 --> 00:28:28,806
into each others and will only


821
00:28:28,806 --> 00:28:30,716
form one map only, which means


822
00:28:30,716 --> 00:28:32,366
that each users will now


823
00:28:32,366 --> 00:28:34,996
understand each others and scene


824
00:28:34,996 --> 00:28:36,056
understanding.


825
00:28:36,056 --> 00:28:40,206
As they move around, they map


826
00:28:40,206 --> 00:28:41,346
even more information.


827
00:28:42,606 --> 00:28:43,786
And they continue sharing that


828
00:28:43,786 --> 00:28:45,106
under the hood.


829
00:28:46,316 --> 00:28:48,766
Additionally, ARKit 3 provides


830
00:28:48,766 --> 00:28:51,016
you with like AR participant


831
00:28:51,016 --> 00:28:52,596
anchors that allows you to


832
00:28:53,056 --> 00:28:54,786
understand where another user is


833
00:28:54,786 --> 00:28:55,906
in real time in your


834
00:28:55,906 --> 00:28:56,386
environment.


835
00:28:57,226 --> 00:28:59,106
This is really handy if you want


836
00:28:59,106 --> 00:29:00,646
to display for example an icon


837
00:29:00,646 --> 00:29:01,796
or something to represent that


838
00:29:01,796 --> 00:29:02,066
user.


839
00:29:02,616 --> 00:29:07,436
As mentioned earlier, ARKit 3


840
00:29:07,436 --> 00:29:09,076
also share ARAnchors under the


841
00:29:09,076 --> 00:29:11,656
hood, meaning that if you share


842
00:29:11,656 --> 00:29:13,406
or add an anchor on one device,


843
00:29:13,406 --> 00:29:15,026
it will automatically show up on


844
00:29:15,026 --> 00:29:15,696
the other device.


845
00:29:17,106 --> 00:29:18,126
Let's now have a look at how it


846
00:29:18,126 --> 00:29:18,846
works in code.


847
00:29:20,576 --> 00:29:22,066
As Andreas mentioned earlier,


848
00:29:22,096 --> 00:29:23,596
ARKit is really well integrated


849
00:29:23,596 --> 00:29:24,406
with RealityKit.


850
00:29:25,386 --> 00:29:26,876
If you want to enable the


851
00:29:26,876 --> 00:29:27,916
collaborative session with


852
00:29:27,916 --> 00:29:29,366
RealityKit, it's pretty simple.


853
00:29:30,276 --> 00:29:31,556
You first need to setup your


854
00:29:31,556 --> 00:29:33,126
Multipeer Connectivity session.


855
00:29:33,506 --> 00:29:36,326
Multipeer Connectivity framework


856
00:29:36,326 --> 00:29:37,346
is an Apple framework that


857
00:29:37,346 --> 00:29:38,676
allows you for discovery and


858
00:29:38,676 --> 00:29:40,196
peer-to-peer connections.


859
00:29:40,196 --> 00:29:43,126
Then, you need to pass this


860
00:29:43,126 --> 00:29:45,116
Multipeer Connectivity session


861
00:29:45,116 --> 00:29:47,416
to the AR scenes view


862
00:29:47,746 --> 00:29:48,826
synchronization service.


863
00:29:49,316 --> 00:29:53,806
And finally, as every ARKit


864
00:29:53,806 --> 00:29:55,456
experiences, you have to setup


865
00:29:55,506 --> 00:29:55,786
your


866
00:29:55,786 --> 00:29:57,286
ARWorldTrackingConfiguration,


867
00:29:57,436 --> 00:29:59,576
set the isCollaborationEnabled


868
00:29:59,576 --> 00:30:01,566
flag to true and run that


869
00:30:01,566 --> 00:30:02,706
configuration on the session.


870
00:30:03,236 --> 00:30:03,836
And that's it.


871
00:30:06,696 --> 00:30:08,196
So what's going to happen then?


872
00:30:09,416 --> 00:30:11,046
So ARKit when sending up the


873
00:30:11,046 --> 00:30:13,286
isCollaborationEnabled flag to


874
00:30:13,796 --> 00:30:16,336
true will essentially -- and


875
00:30:16,336 --> 00:30:17,466
running that configuration on


876
00:30:17,466 --> 00:30:18,836
the session -- will essentially


877
00:30:18,836 --> 00:30:22,506
create a new method on the


878
00:30:22,506 --> 00:30:24,006
ARSessionDelegate for you to be


879
00:30:24,006 --> 00:30:24,936
transferring that data.


880
00:30:25,576 --> 00:30:27,016
In the RealityKit use case,


881
00:30:27,016 --> 00:30:28,686
we'll take care of it, but if


882
00:30:28,686 --> 00:30:29,906
you're using ARKit in another


883
00:30:29,906 --> 00:30:31,646
renderer, then we'll-- you'll


884
00:30:31,646 --> 00:30:33,056
have to send that data across


885
00:30:33,056 --> 00:30:33,596
the network.


886
00:30:35,576 --> 00:30:36,916
This data is called AR


887
00:30:36,916 --> 00:30:37,756
collaboration data.


888
00:30:39,006 --> 00:30:41,036
ARKit can create at any point in


889
00:30:41,036 --> 00:30:43,226
time an AR collaboration data


890
00:30:43,226 --> 00:30:45,106
package that you then have to


891
00:30:45,106 --> 00:30:47,526
forward again to other users.


892
00:30:47,886 --> 00:30:49,396
This is not limited to only two


893
00:30:49,396 --> 00:30:49,886
users.


894
00:30:49,886 --> 00:30:51,916
You can have a large amount of


895
00:30:51,916 --> 00:30:53,456
users in that session.


896
00:30:54,676 --> 00:30:57,676
During that process, ARKit will


897
00:30:57,676 --> 00:30:59,196
generate additional AR


898
00:30:59,196 --> 00:31:00,746
collaboration data that you will


899
00:31:00,746 --> 00:31:02,776
have to forward to other devices


900
00:31:02,776 --> 00:31:03,816
and broadcast that data.


901
00:31:07,876 --> 00:31:09,406
Let's see how that works in


902
00:31:09,906 --> 00:31:10,006
code.


903
00:31:11,296 --> 00:31:13,236
So you first need to setup your


904
00:31:13,236 --> 00:31:14,736
multipeer connectivity in this


905
00:31:14,736 --> 00:31:17,236
example, or you can also setup


906
00:31:17,236 --> 00:31:18,606
any framework-- any network


907
00:31:18,606 --> 00:31:19,716
framework of your choice and


908
00:31:19,716 --> 00:31:20,856
make sure that your devices are


909
00:31:20,856 --> 00:31:21,866
sharing the same session.


910
00:31:22,336 --> 00:31:25,676
When they do, then you need to


911
00:31:25,676 --> 00:31:26,366
enable the


912
00:31:26,366 --> 00:31:27,826
ARWorldTrackingConfiguration


913
00:31:27,826 --> 00:31:29,686
with this isCollaborationEnabled


914
00:31:29,686 --> 00:31:30,396
flag to true.


915
00:31:31,706 --> 00:31:33,466
When this is the case, you need


916
00:31:33,466 --> 00:31:34,436
to run-- then run the


917
00:31:34,436 --> 00:31:35,216
configuration.


918
00:31:36,296 --> 00:31:38,856
At this point, you will then


919
00:31:38,856 --> 00:31:40,196
have a new method available


920
00:31:40,196 --> 00:31:41,806
under delegate for you where you


921
00:31:41,806 --> 00:31:43,356
will be receiving some


922
00:31:43,356 --> 00:31:44,296
collaboration data.


923
00:31:44,776 --> 00:31:49,066
Upon receiving that data, you


924
00:31:49,066 --> 00:31:50,296
need to make sure to broadcast


925
00:31:50,296 --> 00:31:52,406
it on the network to other users


926
00:31:52,726 --> 00:31:53,546
that are also in this


927
00:31:53,576 --> 00:31:54,496
collaboration session.


928
00:31:54,976 --> 00:31:58,376
Upon the reception of that data


929
00:31:58,826 --> 00:32:00,516
on the other devices, you need


930
00:32:00,516 --> 00:32:02,936
to update URL session so that it


931
00:32:02,936 --> 00:32:04,016
knows about this new data.


932
00:32:04,406 --> 00:32:05,566
And that's it.


933
00:32:07,436 --> 00:32:09,266
This collaboration session data


934
00:32:10,276 --> 00:32:12,456
automatically exchange all the


935
00:32:12,456 --> 00:32:14,226
user created ARAnchors.


936
00:32:15,566 --> 00:32:18,126
Each anchors is identifiable by


937
00:32:18,126 --> 00:32:20,506
a session ID so that you can


938
00:32:20,506 --> 00:32:21,706
make sure to understand from


939
00:32:21,706 --> 00:32:23,686
which device or which AR session


940
00:32:24,136 --> 00:32:25,316
the anchor is coming from.


941
00:32:25,836 --> 00:32:28,426
As mentioned earlier, the


942
00:32:28,426 --> 00:32:30,386
ARParticipantAnchor represents


943
00:32:30,386 --> 00:32:32,626
in real time the participant


944
00:32:32,626 --> 00:32:34,586
position which can be very handy


945
00:32:34,586 --> 00:32:35,706
in some of your use cases.


946
00:32:39,296 --> 00:32:40,536
So this is how you create


947
00:32:40,536 --> 00:32:41,436
collaborative sessions.


948
00:32:42,516 --> 00:32:48,546
[ Applause ]


949
00:32:49,046 --> 00:32:50,906
Let's now talk about coaching.


950
00:32:51,636 --> 00:32:53,316
When you create an experience,


951
00:32:53,556 --> 00:32:55,586
an AR experience, coaching is


952
00:32:55,586 --> 00:32:56,436
really important.


953
00:32:57,056 --> 00:32:58,056
You really want to guide your


954
00:32:58,056 --> 00:33:00,336
users whether they are new or


955
00:33:00,336 --> 00:33:02,546
returning users into your AR


956
00:33:02,546 --> 00:33:03,176
experience.


957
00:33:04,036 --> 00:33:05,256
It's not a trivial process.


958
00:33:05,656 --> 00:33:07,216
And sometimes, it's hard for you


959
00:33:07,216 --> 00:33:09,256
to understand or even to guide


960
00:33:09,256 --> 00:33:11,766
the user to that new experience.


961
00:33:13,136 --> 00:33:14,626
Throughout that process, you


962
00:33:14,626 --> 00:33:15,726
have to react to certain


963
00:33:15,726 --> 00:33:16,656
tracking events.


964
00:33:16,846 --> 00:33:17,956
Sometimes the tracking gets


965
00:33:17,956 --> 00:33:19,216
limited because the user moves


966
00:33:19,216 --> 00:33:21,836
way too fast.


967
00:33:21,836 --> 00:33:23,266
So far, we've been providing you


968
00:33:23,266 --> 00:33:25,336
with human interface guideline


969
00:33:25,976 --> 00:33:28,276
that allowed you to provide some


970
00:33:28,276 --> 00:33:29,536
guidelines for the onboarding


971
00:33:29,536 --> 00:33:30,166
experiences.


972
00:33:31,736 --> 00:33:33,346
Well this year, we're embedding


973
00:33:33,346 --> 00:33:34,836
that in the UI view


974
00:33:38,206 --> 00:33:39,686
and we call it the AR Coaching


975
00:33:39,686 --> 00:33:39,906
View.


976
00:33:40,406 --> 00:33:43,746
This is a built-in overlay that


977
00:33:43,746 --> 00:33:45,186
you can directly embed in your


978
00:33:45,186 --> 00:33:46,106
AR applications.


979
00:33:46,746 --> 00:33:48,386
It guides your users to a really


980
00:33:48,386 --> 00:33:49,646
good tracking experience.


981
00:33:50,176 --> 00:33:53,146
It provides a consistent design


982
00:33:53,676 --> 00:33:55,096
throughout your applications so


983
00:33:55,096 --> 00:33:56,406
that your users are very


984
00:33:56,406 --> 00:33:57,846
familiar with it.


985
00:33:58,496 --> 00:33:59,886
You actually may have seen that


986
00:33:59,886 --> 00:34:00,726
design before.


987
00:34:00,946 --> 00:34:02,836
We have it AR Quick Look and in


988
00:34:03,346 --> 00:34:03,496
Measure.


989
00:34:05,296 --> 00:34:07,906
This new UI overlay


990
00:34:07,906 --> 00:34:09,906
automatically activates and


991
00:34:09,906 --> 00:34:11,446
deactivate base on the different


992
00:34:11,446 --> 00:34:12,246
tracking events.


993
00:34:13,036 --> 00:34:14,826
And you can also adjust certain


994
00:34:14,826 --> 00:34:15,576
coaching goals.


995
00:34:16,255 --> 00:34:17,146
Let's have a look at some of


996
00:34:17,146 --> 00:34:17,686
those overlays.


997
00:34:17,686 --> 00:34:21,235
So in the AR Coaching View, we


998
00:34:21,985 --> 00:34:23,206
have multiple overlays.


999
00:34:24,036 --> 00:34:25,846
The onboarding UI provides the


1000
00:34:25,846 --> 00:34:28,065
users the ability to understand


1001
00:34:28,065 --> 00:34:29,235
what you're looking for, and in


1002
00:34:29,235 --> 00:34:30,246
this case, surfaces.


1003
00:34:30,835 --> 00:34:32,106
Most of the time your experience


1004
00:34:32,106 --> 00:34:33,335
require a surface to place


1005
00:34:33,335 --> 00:34:34,235
content on to it.


1006
00:34:34,235 --> 00:34:36,556
So if you enable plane detection


1007
00:34:36,556 --> 00:34:37,956
on your configuration, then this


1008
00:34:37,956 --> 00:34:39,146
overlay will automatically show


1009
00:34:39,146 --> 00:34:39,946
up.


1010
00:34:41,335 --> 00:34:42,846
Secondly, we have another


1011
00:34:42,846 --> 00:34:45,576
overlay that provides the user


1012
00:34:45,576 --> 00:34:46,936
with the ability to understand


1013
00:34:46,936 --> 00:34:48,005
that they have to move around a


1014
00:34:48,005 --> 00:34:49,126
little bit more to gather


1015
00:34:49,126 --> 00:34:50,746
additional features so that


1016
00:34:50,746 --> 00:34:52,565
tracking can works best.


1017
00:34:53,216 --> 00:34:55,426
And then finally, we have


1018
00:34:55,426 --> 00:34:57,536
another overlay which helps your


1019
00:34:57,536 --> 00:34:59,156
user relocalize against certain


1020
00:34:59,156 --> 00:35:00,766
environments in case of your


1021
00:35:00,766 --> 00:35:03,116
lost tracking for example, or if


1022
00:35:03,116 --> 00:35:03,936
the app went into the


1023
00:35:03,936 --> 00:35:04,446
background.


1024
00:35:05,066 --> 00:35:08,306
Let's look at one example.


1025
00:35:11,346 --> 00:35:12,876
So, in this example, we're


1026
00:35:12,876 --> 00:35:14,646
asking a user to move the device


1027
00:35:14,646 --> 00:35:16,466
around to find a new plane, and


1028
00:35:16,466 --> 00:35:17,886
as soon as the user is moving


1029
00:35:17,886 --> 00:35:19,566
around and gathering more


1030
00:35:19,566 --> 00:35:21,636
feature, then the content can be


1031
00:35:21,706 --> 00:35:23,446
placed and the view deactivates


1032
00:35:23,446 --> 00:35:24,116
automatically.


1033
00:35:24,596 --> 00:35:25,586
So you don't have to do


1034
00:35:25,586 --> 00:35:26,086
anything.


1035
00:35:26,086 --> 00:35:26,836
Everything is handled


1036
00:35:26,836 --> 00:35:27,526
automatically.


1037
00:35:28,516 --> 00:35:32,966
[ Applause ]


1038
00:35:33,466 --> 00:35:36,166
Let's have a look at how you can


1039
00:35:36,166 --> 00:35:36,646
set that up.


1040
00:35:37,376 --> 00:35:38,886
Again, this is really easy.


1041
00:35:39,636 --> 00:35:41,686
As is it's a simple UI view, you


1042
00:35:41,686 --> 00:35:43,306
have to set it up as a child of


1043
00:35:43,346 --> 00:35:44,506
another UI view.


1044
00:35:44,766 --> 00:35:46,386
Ideally, you set it as a child


1045
00:35:46,386 --> 00:35:47,826
of the AR view.


1046
00:35:48,436 --> 00:35:50,676
Then, you need to connect this


1047
00:35:50,676 --> 00:35:53,036
session to the coaching view so


1048
00:35:53,036 --> 00:35:54,076
that the coaching view knows


1049
00:35:54,076 --> 00:35:57,386
what events to react to.


1050
00:35:57,386 --> 00:35:58,296
Or you need to connect the


1051
00:35:58,296 --> 00:36:00,866
session provider outlet of the


1052
00:36:00,866 --> 00:36:02,016
coaching view to the session


1053
00:36:02,016 --> 00:36:04,326
provider itself if you're using


1054
00:36:04,326 --> 00:36:07,556
a storyboard for example.


1055
00:36:07,626 --> 00:36:09,416
Optionally, you can set a bunch


1056
00:36:09,416 --> 00:36:10,556
of delegates if you want to


1057
00:36:10,556 --> 00:36:12,336
react to certain events that the


1058
00:36:12,336 --> 00:36:13,156
view is giving you.


1059
00:36:13,686 --> 00:36:18,146
And finally, you can also


1060
00:36:18,146 --> 00:36:19,706
provide a set of specific


1061
00:36:19,706 --> 00:36:21,606
coaching goals if you want to


1062
00:36:21,606 --> 00:36:23,166
disable certain functionalities.


1063
00:36:23,686 --> 00:36:26,596
Let's look at some of those


1064
00:36:26,596 --> 00:36:26,976
delegates.


1065
00:36:30,296 --> 00:36:32,666
So we have three new methods on


1066
00:36:32,666 --> 00:36:33,806
the AR Coaching View--


1067
00:36:33,806 --> 00:36:35,346
CoachingOverlayViewDelegate.


1068
00:36:35,826 --> 00:36:37,146
Two of them can react to


1069
00:36:37,146 --> 00:36:38,516
activation and deactivation.


1070
00:36:38,516 --> 00:36:40,476
So you can choose if you want to


1071
00:36:40,476 --> 00:36:41,636
still enable that throughout the


1072
00:36:41,636 --> 00:36:43,016
experience or if you think, for


1073
00:36:43,016 --> 00:36:45,376
example, that if a user had that


1074
00:36:45,466 --> 00:36:47,306
once, it doesn't need to know


1075
00:36:47,306 --> 00:36:47,696
anymore.


1076
00:36:49,106 --> 00:36:50,916
Additionally, you can react to


1077
00:36:50,916 --> 00:36:52,866
certain relocalization abort


1078
00:36:52,866 --> 00:36:56,946
requests by default the UI view,


1079
00:36:56,946 --> 00:36:58,506
the coaching view will actually


1080
00:36:58,506 --> 00:37:01,106
give you or give your users a


1081
00:37:01,106 --> 00:37:02,946
new UI bottom that-- where they


1082
00:37:03,066 --> 00:37:04,266
can actually relocalize and


1083
00:37:04,266 --> 00:37:05,636
restart the session or reset the


1084
00:37:05,636 --> 00:37:06,786
tracking for example.


1085
00:37:07,806 --> 00:37:09,676
So this new view is really handy


1086
00:37:09,756 --> 00:37:11,746
for your applications so that


1087
00:37:11,746 --> 00:37:12,596
you can make sure that you've


1088
00:37:12,596 --> 00:37:14,006
got that consistent design and


1089
00:37:14,006 --> 00:37:14,676
help your users.


1090
00:37:15,306 --> 00:37:18,196
Let's now talk about Face


1091
00:37:18,196 --> 00:37:18,596
Tracking.


1092
00:37:19,316 --> 00:37:21,996
In ARKit 1, we enabled Face


1093
00:37:21,996 --> 00:37:23,306
Tracking with the ability to


1094
00:37:23,306 --> 00:37:24,386
track one face.


1095
00:37:25,396 --> 00:37:27,456
While in ARKit 2, we have the


1096
00:37:27,456 --> 00:37:28,786
ability to the multi-face


1097
00:37:28,786 --> 00:37:30,186
tracking up to three faces


1098
00:37:30,216 --> 00:37:31,516
concurrently.


1099
00:37:32,536 --> 00:37:36,476
Additionally, you can also make


1100
00:37:36,476 --> 00:37:38,196
sure to recognize the person


1101
00:37:38,196 --> 00:37:39,576
when he leaves the frame and


1102
00:37:39,576 --> 00:37:41,466
comes back again giving you the


1103
00:37:41,466 --> 00:37:43,656
same face anchor ID again.


1104
00:37:47,516 --> 00:37:52,546
[ Applause ]


1105
00:37:53,046 --> 00:37:55,796
So multi-Face Tracking tracks up


1106
00:37:55,796 --> 00:37:59,036
to three faces concurrently and


1107
00:37:59,186 --> 00:38:00,826
provides you with a persistent


1108
00:38:00,826 --> 00:38:02,986
face anchor ID so that you can


1109
00:38:02,986 --> 00:38:04,496
make sure to recognize one user


1110
00:38:04,496 --> 00:38:05,376
throughout the session.


1111
00:38:06,016 --> 00:38:07,616
If you restart a new session,


1112
00:38:08,436 --> 00:38:09,846
then this ID disappears and a


1113
00:38:09,846 --> 00:38:11,206
new one comes up.


1114
00:38:12,536 --> 00:38:14,266
To enable this, this is really


1115
00:38:14,266 --> 00:38:14,776
easy.


1116
00:38:15,446 --> 00:38:17,036
We have two new properties on


1117
00:38:17,036 --> 00:38:18,856
the ARFaceTrackingConfiguration.


1118
00:38:20,206 --> 00:38:21,386
The first one allows you to


1119
00:38:21,386 --> 00:38:23,756
query how many multiple faces


1120
00:38:23,756 --> 00:38:25,386
can be tracked concurrently in


1121
00:38:25,386 --> 00:38:26,646
one session on that specific


1122
00:38:26,646 --> 00:38:27,286
device.


1123
00:38:27,846 --> 00:38:29,416
And the other one allows you to


1124
00:38:29,416 --> 00:38:31,046
set the number of track faces


1125
00:38:31,096 --> 00:38:31,766
that you want to track


1126
00:38:31,766 --> 00:38:32,356
concurrently.


1127
00:38:32,856 --> 00:38:35,976
And that's Multi-Face Tracking.


1128
00:38:36,516 --> 00:38:40,956
[ Applause ]


1129
00:38:41,456 --> 00:38:43,096
Let's now talk about a new


1130
00:38:43,246 --> 00:38:45,306
tracking configuration that we


1131
00:38:45,806 --> 00:38:47,506
called ARPositional


1132
00:38:47,506 --> 00:38:48,556
TrackingConfiguration.


1133
00:38:49,486 --> 00:38:50,606
So this new tracking


1134
00:38:50,606 --> 00:38:53,416
configuration is intended for


1135
00:38:53,416 --> 00:38:54,906
tracking only use cases.


1136
00:38:55,846 --> 00:38:58,096
You often had a use case where


1137
00:38:58,096 --> 00:39:00,066
it didn't really need the camera


1138
00:39:00,096 --> 00:39:01,416
backdrop to be rendered for


1139
00:39:01,416 --> 00:39:01,846
example.


1140
00:39:03,206 --> 00:39:04,786
Well, this is made for that use


1141
00:39:04,826 --> 00:39:05,146
case.


1142
00:39:06,376 --> 00:39:07,816
We can achieve a low power


1143
00:39:07,816 --> 00:39:09,956
consumption with the ability to


1144
00:39:09,956 --> 00:39:13,006
lower the capture frame rate and


1145
00:39:13,006 --> 00:39:15,426
also the camera resolution by


1146
00:39:15,426 --> 00:39:16,596
still keeping your rendering


1147
00:39:16,596 --> 00:39:18,546
rate at 60 hertz.


1148
00:39:21,956 --> 00:39:24,536
Next, let's talk about some of


1149
00:39:24,566 --> 00:39:25,376
the scene understanding


1150
00:39:25,376 --> 00:39:27,626
improvements we've made this


1151
00:39:28,476 --> 00:39:28,636
year.


1152
00:39:29,966 --> 00:39:31,166
Image detection and image


1153
00:39:31,166 --> 00:39:32,216
tracking has been around for


1154
00:39:32,216 --> 00:39:33,366
some time now.


1155
00:39:34,086 --> 00:39:35,816
We can now this year detect up


1156
00:39:35,816 --> 00:39:38,056
to 100 images at the same time.


1157
00:39:38,616 --> 00:39:42,226
We also provide you with the


1158
00:39:42,316 --> 00:39:44,316
ability to detect the scale of


1159
00:39:44,316 --> 00:39:46,306
an printed image for example.


1160
00:39:47,386 --> 00:39:49,016
Oftentimes, when new application


1161
00:39:49,016 --> 00:39:51,346
require a user to use an image


1162
00:39:51,346 --> 00:39:52,696
to place content and scale that


1163
00:39:52,696 --> 00:39:55,216
content accordingly, the image


1164
00:39:55,216 --> 00:39:56,226
might be printed with a


1165
00:39:56,226 --> 00:39:57,456
different size for example or


1166
00:39:57,456 --> 00:39:58,726
different paper size.


1167
00:39:59,426 --> 00:40:01,086
With this automatic scale


1168
00:40:01,086 --> 00:40:03,516
estimation, you can now detect


1169
00:40:03,516 --> 00:40:05,986
the physical size and adjust the


1170
00:40:05,986 --> 00:40:06,736
scale accordingly.


1171
00:40:06,736 --> 00:40:10,786
We also have the ability to


1172
00:40:10,786 --> 00:40:14,356
query at run time the quality of


1173
00:40:14,356 --> 00:40:15,666
an image that you're passing to


1174
00:40:15,666 --> 00:40:17,196
ARKit when you want to create a


1175
00:40:17,196 --> 00:40:19,096
new AR reference image.


1176
00:40:21,516 --> 00:40:23,376
We've also made improvements to


1177
00:40:23,376 --> 00:40:24,806
our object detection algorithms.


1178
00:40:25,966 --> 00:40:27,756
With machine learning, we can


1179
00:40:27,756 --> 00:40:29,536
enhance that object detection


1180
00:40:29,536 --> 00:40:32,656
algorithms and provide you with


1181
00:40:32,656 --> 00:40:34,896
a faster recognition, and also


1182
00:40:34,896 --> 00:40:36,626
in more robust environments, in


1183
00:40:36,626 --> 00:40:37,466
more-- in different


1184
00:40:37,466 --> 00:40:38,006
environments.


1185
00:40:38,326 --> 00:40:39,976
Oftentimes, you had to scan a


1186
00:40:40,286 --> 00:40:41,396
specific object in an


1187
00:40:41,396 --> 00:40:42,346
environment so that it works


1188
00:40:42,346 --> 00:40:43,456
perfectly in another one.


1189
00:40:44,226 --> 00:40:45,116
Now, this is a bit more


1190
00:40:45,116 --> 00:40:45,586
flexible.


1191
00:40:48,456 --> 00:40:51,166
Finally, another area of scene


1192
00:40:51,166 --> 00:40:52,246
understanding which is really


1193
00:40:52,246 --> 00:40:54,616
important is plane estimation.


1194
00:40:55,376 --> 00:40:56,536
Oftentimes, you need plane


1195
00:40:56,536 --> 00:40:58,706
estimation to place content.


1196
00:40:58,856 --> 00:40:59,926
Well, with machine learning,


1197
00:41:00,476 --> 00:41:01,686
we're actually making that even


1198
00:41:01,686 --> 00:41:04,006
more accurate and we're making


1199
00:41:04,006 --> 00:41:05,956
that even more robust to detect


1200
00:41:05,956 --> 00:41:06,996
planes and faster.


1201
00:41:07,566 --> 00:41:10,356
Let's have a look at an example.


1202
00:41:10,986 --> 00:41:16,406
With machine learning, not only


1203
00:41:16,406 --> 00:41:18,756
we can extend those planes on


1204
00:41:18,756 --> 00:41:19,956
the ground when features are


1205
00:41:19,956 --> 00:41:21,886
detected but the flow is


1206
00:41:21,886 --> 00:41:23,466
actually going even further.


1207
00:41:23,746 --> 00:41:26,056
But we can also with the ability


1208
00:41:26,056 --> 00:41:27,526
to detect-- we also have the


1209
00:41:27,526 --> 00:41:29,676
ability detect walls on the side


1210
00:41:30,026 --> 00:41:31,276
when no feature points our


1211
00:41:31,276 --> 00:41:31,676
present.


1212
00:41:32,166 --> 00:41:33,576
And this is thanks to machine


1213
00:41:34,076 --> 00:41:34,276
learning.


1214
00:41:36,516 --> 00:41:41,376
[ Applause ]


1215
00:41:41,876 --> 00:41:44,056
As you can see here-- As you


1216
00:41:44,056 --> 00:41:45,146
can-- You saw on the previous


1217
00:41:45,146 --> 00:41:46,646
video, we had a couple of


1218
00:41:46,696 --> 00:41:48,326
classifications on the planes.


1219
00:41:49,026 --> 00:41:50,166
Well, this is done again with


1220
00:41:50,166 --> 00:41:50,886
machine learning.


1221
00:41:50,996 --> 00:41:52,336
And last year, we introduced


1222
00:41:52,816 --> 00:41:54,446
five different classifications,


1223
00:41:55,136 --> 00:41:56,956
wall, floor, ceiling, table, and


1224
00:41:56,956 --> 00:41:57,366
seat.


1225
00:41:58,466 --> 00:41:59,806
Well, this year, we're adding


1226
00:41:59,936 --> 00:42:01,026
two additional ones.


1227
00:42:01,406 --> 00:42:03,106
We are adding the ability to


1228
00:42:03,106 --> 00:42:05,886
detect doors and windows.


1229
00:42:07,296 --> 00:42:09,406
As mentioned earlier, plane


1230
00:42:09,406 --> 00:42:10,676
classification is really


1231
00:42:10,676 --> 00:42:12,526
important-- Or plane estimation


1232
00:42:12,526 --> 00:42:13,566
is really important to place


1233
00:42:13,566 --> 00:42:14,696
content on your-- in the world.


1234
00:42:14,696 --> 00:42:16,586
This is actually ideal for


1235
00:42:16,586 --> 00:42:17,256
object placement.


1236
00:42:17,526 --> 00:42:18,716
You always your objects to be


1237
00:42:18,716 --> 00:42:19,566
placed on a surface.


1238
00:42:20,896 --> 00:42:21,916
Well, this year with the new


1239
00:42:21,916 --> 00:42:25,126
raycasting API, you can now even


1240
00:42:25,126 --> 00:42:26,556
easier place your content like


1241
00:42:26,746 --> 00:42:28,976
more precisely and is even more


1242
00:42:28,976 --> 00:42:29,506
flexible.


1243
00:42:30,866 --> 00:42:32,486
It supports any kind of surface


1244
00:42:32,486 --> 00:42:32,956
alignment.


1245
00:42:33,286 --> 00:42:34,566
So you're not always bound to


1246
00:42:34,566 --> 00:42:36,116
vertical and horizontal anymore.


1247
00:42:38,156 --> 00:42:40,036
But also, you can track your


1248
00:42:40,036 --> 00:42:40,966
raycast all the time.


1249
00:42:42,716 --> 00:42:44,466
Meaning that as ARKit-- or as


1250
00:42:44,466 --> 00:42:45,486
you move your device around,


1251
00:42:45,486 --> 00:42:47,066
ARKit detects more information


1252
00:42:47,066 --> 00:42:49,456
about the environment, it can


1253
00:42:49,666 --> 00:42:51,486
accurately place your object on


1254
00:42:51,486 --> 00:42:52,986
top of the physical surface such


1255
00:42:52,986 --> 00:42:54,026
as those planes are evolving.


1256
00:42:54,546 --> 00:42:57,016
Let's see how you can enable


1257
00:42:57,016 --> 00:42:57,806
that in ARKit.


1258
00:43:00,336 --> 00:43:01,906
Well, it sounds we're creating a


1259
00:43:01,906 --> 00:43:02,816
raycast query.


1260
00:43:03,526 --> 00:43:05,806
A raycast query has three


1261
00:43:05,806 --> 00:43:06,466
parameters.


1262
00:43:06,596 --> 00:43:08,096
The first one decides from where


1263
00:43:08,096 --> 00:43:08,956
you want to perform that


1264
00:43:08,956 --> 00:43:09,536
raycast.


1265
00:43:10,106 --> 00:43:11,246
In this example, we're doing


1266
00:43:11,246 --> 00:43:12,266
this from the screenCenter.


1267
00:43:12,266 --> 00:43:15,896
Then you need to tell it like


1268
00:43:16,126 --> 00:43:18,686
what you want to allow in order


1269
00:43:18,686 --> 00:43:20,606
to place that content or get the


1270
00:43:20,606 --> 00:43:21,366
transforms back.


1271
00:43:21,936 --> 00:43:25,406
And additionally, you need tell


1272
00:43:25,406 --> 00:43:26,566
it which alignment you want.


1273
00:43:26,746 --> 00:43:29,296
It can be horizontal, vertical,


1274
00:43:29,566 --> 00:43:30,686
or any.


1275
00:43:32,616 --> 00:43:34,756
Then you need to pass that query


1276
00:43:35,076 --> 00:43:36,616
on to the trackedRaycast method


1277
00:43:36,616 --> 00:43:38,206
on your AR session.


1278
00:43:40,056 --> 00:43:41,976
This method has a callback that


1279
00:43:41,976 --> 00:43:43,836
will allow you to react to the


1280
00:43:43,836 --> 00:43:45,406
new transform and result giving


1281
00:43:45,406 --> 00:43:47,746
to you with that raycast so that


1282
00:43:47,746 --> 00:43:49,026
you can adjust your content or


1283
00:43:49,026 --> 00:43:49,996
your anchors accordingly.


1284
00:43:50,456 --> 00:43:53,436
And then finally, when you are


1285
00:43:53,436 --> 00:43:55,146
done with this raycast, you can


1286
00:43:55,146 --> 00:43:55,696
just stop it.


1287
00:43:56,346 --> 00:43:58,966
And those are some of the


1288
00:43:58,966 --> 00:44:00,236
scene-- Those are some of the


1289
00:44:00,236 --> 00:44:01,856
raycasting improvements that


1290
00:44:01,886 --> 00:44:02,466
we've done this year.


1291
00:44:07,256 --> 00:44:08,666
Let's move on with some of the


1292
00:44:08,666 --> 00:44:10,366
visual coherence enhancement


1293
00:44:10,416 --> 00:44:11,346
we've made.


1294
00:44:12,216 --> 00:44:16,456
So this year, we have this new


1295
00:44:16,456 --> 00:44:18,696
ARView that allows you to


1296
00:44:18,696 --> 00:44:19,986
activate and deactivate


1297
00:44:20,216 --> 00:44:22,116
different render options on


1298
00:44:22,116 --> 00:44:22,456
demand.


1299
00:44:23,396 --> 00:44:24,766
It also can also be


1300
00:44:25,056 --> 00:44:26,876
automatically deactivated and


1301
00:44:26,876 --> 00:44:28,556
activated based on your device


1302
00:44:28,556 --> 00:44:29,226
capability.


1303
00:44:30,076 --> 00:44:31,636
Let's look at an example of


1304
00:44:31,636 --> 00:44:31,936
this.


1305
00:44:32,266 --> 00:44:34,826
You've probably seen that video


1306
00:44:34,826 --> 00:44:35,496
before.


1307
00:44:35,496 --> 00:44:37,016
Look at how the Quadrocopter


1308
00:44:37,016 --> 00:44:38,426
actually moves around and how


1309
00:44:38,426 --> 00:44:39,646
the objects are moving as well


1310
00:44:39,646 --> 00:44:42,066
on the surface and how real all


1311
00:44:42,066 --> 00:44:43,646
of these looks like.


1312
00:44:44,276 --> 00:44:45,716
When everything disappears, you


1313
00:44:47,256 --> 00:44:49,156
actually don't even realize that


1314
00:44:49,156 --> 00:44:50,176
those objects were virtual.


1315
00:44:50,666 --> 00:44:52,726
Let's look at some of those


1316
00:44:52,726 --> 00:44:54,866
visual coherence enhancements


1317
00:44:54,866 --> 00:44:55,196
we've made.


1318
00:44:55,736 --> 00:44:57,956
Let's look again at the


1319
00:44:57,956 --> 00:44:59,396
beginning of that video and


1320
00:44:59,396 --> 00:45:00,936
let's pause for a second when


1321
00:45:00,936 --> 00:45:02,316
those balls are rolling on the


1322
00:45:02,796 --> 00:45:02,986
table.


1323
00:45:04,376 --> 00:45:05,926
Here you can see the depth of


1324
00:45:06,006 --> 00:45:08,256
field effect which is a new


1325
00:45:08,256 --> 00:45:09,736
feature of RealityKit.


1326
00:45:11,226 --> 00:45:13,216
Your AR experience are designed


1327
00:45:13,216 --> 00:45:15,226
for, you know, small and big


1328
00:45:15,226 --> 00:45:15,746
rooms.


1329
00:45:16,316 --> 00:45:19,196
The camera on your iOS device


1330
00:45:19,196 --> 00:45:20,696
always adjust their focus to the


1331
00:45:20,696 --> 00:45:23,266
environment, while the depth of


1332
00:45:23,266 --> 00:45:25,436
field feature allows to adjust


1333
00:45:25,436 --> 00:45:26,776
the focus on the virtual


1334
00:45:26,776 --> 00:45:27,886
contents so that it matches


1335
00:45:27,946 --> 00:45:29,596
perfectly with your physical


1336
00:45:29,596 --> 00:45:31,626
one, so that the object blends


1337
00:45:31,676 --> 00:45:36,306
perfectly in the environment.


1338
00:45:36,306 --> 00:45:37,416
Additionally, when you move the


1339
00:45:37,416 --> 00:45:39,026
camera quickly or when the


1340
00:45:39,026 --> 00:45:41,566
object moves quickly, you can


1341
00:45:41,566 --> 00:45:43,356
see that blurriness occurring.


1342
00:45:44,496 --> 00:45:46,516
Well-- And then most of the time


1343
00:45:46,516 --> 00:45:48,626
when you have a regular renderer


1344
00:45:48,626 --> 00:45:50,986
and no motion blur effect, then


1345
00:45:50,986 --> 00:45:52,896
your virtual content stands out.


1346
00:45:54,346 --> 00:45:55,386
And it doesn't really blend


1347
00:45:55,386 --> 00:45:56,436
nicely in the environment.


1348
00:45:57,096 --> 00:45:58,356
Well, thanks to VIO camera


1349
00:45:58,356 --> 00:46:01,226
motion and the sense of


1350
00:46:01,226 --> 00:46:03,386
parameters, then we can


1351
00:46:03,386 --> 00:46:06,116
synthesize the motion blur.


1352
00:46:06,436 --> 00:46:07,746
And we can apply it on the


1353
00:46:07,746 --> 00:46:10,506
visual object so that it blends


1354
00:46:10,586 --> 00:46:11,696
perfectly in your environment.


1355
00:46:11,696 --> 00:46:15,506
This is a variable on ARSCNView


1356
00:46:15,506 --> 00:46:16,596
and on the ARView as well.


1357
00:46:16,716 --> 00:46:20,446
Let's look again at this example


1358
00:46:20,446 --> 00:46:21,686
where everything looks really


1359
00:46:21,686 --> 00:46:21,876
good.


1360
00:46:55,616 --> 00:46:56,996
Two additional APIs that we're


1361
00:46:57,056 --> 00:46:58,646
making available for visual


1362
00:46:58,646 --> 00:47:00,306
coherence enhancement this year


1363
00:47:00,616 --> 00:47:03,176
are HDR environment textures and


1364
00:47:03,176 --> 00:47:03,756
camera grain.


1365
00:47:06,286 --> 00:47:07,996
When you place your content, you


1366
00:47:07,996 --> 00:47:08,956
really want that content to


1367
00:47:08,956 --> 00:47:10,026
reflect the real world.


1368
00:47:11,296 --> 00:47:13,206
Well, with high-dynamic-range,


1369
00:47:13,986 --> 00:47:16,226
you can capture even in bright


1370
00:47:16,226 --> 00:47:17,236
light environment those


1371
00:47:17,356 --> 00:47:18,746
highlights or higher highlights


1372
00:47:19,376 --> 00:47:20,576
that makes your contents more


1373
00:47:20,576 --> 00:47:21,076
vibrant.


1374
00:47:22,626 --> 00:47:23,846
Well, with ARKit this year, we


1375
00:47:23,846 --> 00:47:25,386
can actually request for those


1376
00:47:25,386 --> 00:47:26,666
HDR environment textures so that


1377
00:47:26,666 --> 00:47:28,576
your content looks even better.


1378
00:47:29,896 --> 00:47:31,916
Additionally, we also have a


1379
00:47:31,916 --> 00:47:32,846
camera grain API.


1380
00:47:33,786 --> 00:47:36,236
You'll probably notice when you


1381
00:47:36,236 --> 00:47:38,026
have an AR experience in a very


1382
00:47:38,026 --> 00:47:39,506
low light environment how your


1383
00:47:39,806 --> 00:47:41,856
other content looks really shiny


1384
00:47:42,176 --> 00:47:43,046
compared to the camera.


1385
00:47:44,906 --> 00:47:46,476
Every camera produce some grain.


1386
00:47:46,786 --> 00:47:48,056
And especially in low lights,


1387
00:47:48,126 --> 00:47:51,216
this grain can be a bit heavier.


1388
00:47:51,856 --> 00:47:53,336
Where with this new camera grain


1389
00:47:53,336 --> 00:47:55,906
API, we can make sure to apply


1390
00:47:56,716 --> 00:47:58,686
the same grain patterns on your


1391
00:47:58,686 --> 00:48:00,576
virtual content so that it


1392
00:48:00,576 --> 00:48:02,386
blends nicely and doesn't stand


1393
00:48:03,316 --> 00:48:03,386
out.


1394
00:48:04,516 --> 00:48:06,296
So those are some of the visual


1395
00:48:06,296 --> 00:48:07,746
coherence enhancement for this


1396
00:48:07,746 --> 00:48:07,916
year.


1397
00:48:07,916 --> 00:48:10,966
But we didn't stop there.


1398
00:48:11,776 --> 00:48:13,156
I think there's one feature that


1399
00:48:13,156 --> 00:48:13,866
a lot of you have been


1400
00:48:13,866 --> 00:48:14,456
requesting.


1401
00:48:16,146 --> 00:48:17,076
When you develop an AR


1402
00:48:17,076 --> 00:48:19,076
experience, you'll always have


1403
00:48:19,116 --> 00:48:21,506
or often in order to prototype


1404
00:48:21,506 --> 00:48:23,236
or to test your experience go to


1405
00:48:23,236 --> 00:48:24,266
a certain location.


1406
00:48:26,356 --> 00:48:28,386
And most of the time when you go


1407
00:48:28,386 --> 00:48:29,386
there, you'd come back to your


1408
00:48:29,386 --> 00:48:30,186
desk, you develop your


1409
00:48:30,186 --> 00:48:31,146
experience, you want to come


1410
00:48:31,146 --> 00:48:31,696
back again.


1411
00:48:31,866 --> 00:48:34,216
Well, this year with the Reality


1412
00:48:34,216 --> 00:48:36,876
Composer app, you can now record


1413
00:48:36,966 --> 00:48:38,976
an experience or a sequence.


1414
00:48:40,076 --> 00:48:41,216
Meaning that you can go to your


1415
00:48:41,266 --> 00:48:42,936
favorite place so where-- the


1416
00:48:43,006 --> 00:48:44,556
place where your experience will


1417
00:48:44,556 --> 00:48:47,366
be happening to capture the


1418
00:48:47,366 --> 00:48:48,206
environment.


1419
00:48:48,706 --> 00:48:50,376
ARKit will make sure to save the


1420
00:48:50,376 --> 00:48:52,596
sensor data alongside the video


1421
00:48:52,596 --> 00:48:56,436
stream into a movie file


1422
00:48:56,436 --> 00:48:58,906
container so that you can take


1423
00:48:58,996 --> 00:49:02,946
it with you and put it in Xcode.


1424
00:49:03,076 --> 00:49:06,256
At that point, the Xcode scheme


1425
00:49:06,256 --> 00:49:08,156
settings now have like one new


1426
00:49:08,156 --> 00:49:10,126
additional feature or a new


1427
00:49:10,226 --> 00:49:12,256
field that allows you to select


1428
00:49:12,256 --> 00:49:13,456
that file.


1429
00:49:16,106 --> 00:49:18,336
When that file is selected and


1430
00:49:18,336 --> 00:49:19,976
then you press Run on the device


1431
00:49:19,976 --> 00:49:23,906
that is attached to Xcode, then


1432
00:49:24,046 --> 00:49:25,656
you can replay that experience


1433
00:49:25,656 --> 00:49:26,316
at your desk.


1434
00:49:26,936 --> 00:49:28,636
This is ideal for prototyping,


1435
00:49:29,256 --> 00:49:31,336
and even better for tweaking


1436
00:49:31,336 --> 00:49:32,736
your AR configuration with


1437
00:49:32,736 --> 00:49:34,136
different parameters and trying


1438
00:49:34,136 --> 00:49:35,366
to see how the experience looks


1439
00:49:35,366 --> 00:49:35,526
like.


1440
00:49:36,076 --> 00:49:37,676
You can even react to certain


1441
00:49:37,676 --> 00:49:37,976
tracking--


1442
00:49:38,516 --> 00:49:46,456
[ Applause ]


1443
00:49:46,956 --> 00:49:48,376
So this is great.


1444
00:49:48,376 --> 00:49:50,136
I think you've got a lot of


1445
00:49:50,136 --> 00:49:51,776
different tools this year in


1446
00:49:51,776 --> 00:49:54,456
ARKit 3 where you can enhance


1447
00:49:54,456 --> 00:49:56,296
your multiuser experiences with


1448
00:49:56,356 --> 00:49:57,556
collaborative sessions,


1449
00:49:57,996 --> 00:49:59,246
multiple-face tracking.


1450
00:50:00,096 --> 00:50:02,356
You can improve the realism of


1451
00:50:02,406 --> 00:50:03,986
all your AR apps with


1452
00:50:03,986 --> 00:50:06,746
RealityKit's new coherence


1453
00:50:06,746 --> 00:50:08,676
effect, and also the new visual


1454
00:50:08,676 --> 00:50:09,616
effects on the


1455
00:50:09,616 --> 00:50:10,926
ARWorldTrackingConfiguration.


1456
00:50:12,396 --> 00:50:13,916
You can enable new use cases


1457
00:50:14,996 --> 00:50:18,086
with the new motion capture, and


1458
00:50:18,086 --> 00:50:19,706
also the simultaneous face and


1459
00:50:19,706 --> 00:50:20,396
back camera.


1460
00:50:21,536 --> 00:50:23,396
And, of course, there are lots


1461
00:50:23,396 --> 00:50:25,086
of improvements under the hood


1462
00:50:25,356 --> 00:50:26,416
on existing features.


1463
00:50:26,756 --> 00:50:28,526
As an example, with object


1464
00:50:28,526 --> 00:50:29,676
detection and machine learning.


1465
00:50:30,266 --> 00:50:33,456
And least-- last but not the


1466
00:50:33,456 --> 00:50:36,676
least, the record and replay


1467
00:50:36,676 --> 00:50:38,446
workflow I think will make your


1468
00:50:38,446 --> 00:50:39,966
design and your experience


1469
00:50:39,966 --> 00:50:41,246
prototyping even better than


1470
00:50:41,246 --> 00:50:41,716
before.


1471
00:50:41,716 --> 00:50:44,956
So I'm really looking forward


1472
00:50:44,956 --> 00:50:46,296
for you to go and download our


1473
00:50:46,296 --> 00:50:47,526
samples on the website.


1474
00:50:49,036 --> 00:50:50,556
We also have a couple of labs,


1475
00:50:50,946 --> 00:50:52,476
one tomorrow and one on


1476
00:50:52,576 --> 00:50:54,416
Thursday, where I hope you will


1477
00:50:54,416 --> 00:50:55,466
be coming and asking us the


1478
00:50:55,526 --> 00:50:57,756
questions you have or just to


1479
00:50:57,756 --> 00:50:58,016
chat.


1480
00:50:59,166 --> 00:51:01,136
And then we also have two


1481
00:51:01,136 --> 00:51:03,016
in-depth sessions, one of which


1482
00:51:03,016 --> 00:51:04,856
is around bringing people into


1483
00:51:04,856 --> 00:51:07,046
AR which we'll talk more about


1484
00:51:07,046 --> 00:51:08,166
people occlusion and motion


1485
00:51:08,166 --> 00:51:08,616
capture.


1486
00:51:09,086 --> 00:51:10,676
And the second one is about


1487
00:51:10,746 --> 00:51:12,146
collaborative AR experiences.


1488
00:51:12,806 --> 00:51:15,416
I hope you enjoy the rest of the


1489
00:51:15,416 --> 00:51:15,946
conference.


1490
00:51:17,336 --> 00:51:18,096
Have a great day.


1491
00:51:18,416 --> 00:51:18,616
Bye.


1492
00:51:19,516 --> 00:51:22,500
[ Applause ]

