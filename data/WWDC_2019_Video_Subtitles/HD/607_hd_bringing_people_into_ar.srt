1
00:00:01,516 --> 00:00:04,500
[ Music ]


2
00:00:11,276 --> 00:00:12,886
>> Hello everyone.


3
00:00:13,016 --> 00:00:14,756
[ Applause ]


4
00:00:14,756 --> 00:00:15,786
Thank you so much for coming.


5
00:00:18,096 --> 00:00:19,676
This is a deep dive session for


6
00:00:19,676 --> 00:00:20,376
ARKit.


7
00:00:20,376 --> 00:00:21,446
We'll be showing you how we're


8
00:00:21,446 --> 00:00:22,836
bringing people into AR.


9
00:00:24,216 --> 00:00:25,226
My name is Adrian.


10
00:00:25,226 --> 00:00:26,496
And, I'll be joined onstage by


11
00:00:26,496 --> 00:00:26,966
Tanmay.


12
00:00:31,666 --> 00:00:33,216
Earlier this week, Apple


13
00:00:33,216 --> 00:00:34,526
announced the RealityKit


14
00:00:34,526 --> 00:00:34,896
framework.


15
00:00:35,536 --> 00:00:36,736
This is a new framework for


16
00:00:36,736 --> 00:00:37,896
rendering photorealistic


17
00:00:37,896 --> 00:00:38,366
content.


18
00:00:39,166 --> 00:00:40,706
It has also been built from the


19
00:00:40,706 --> 00:00:42,136
ground up to support AR.


20
00:00:42,816 --> 00:00:46,036
We also held a What's New


21
00:00:46,036 --> 00:00:47,806
session for ARKit 3, where we


22
00:00:47,806 --> 00:00:49,166
showed you many of the cool


23
00:00:49,166 --> 00:00:50,156
features that we're bringing


24
00:00:50,156 --> 00:00:51,176
into ARKit this year.


25
00:00:52,036 --> 00:00:53,446
And, for this deep dive, we're


26
00:00:53,446 --> 00:00:54,806
going to focus on two of these.


27
00:00:56,176 --> 00:00:57,536
First, I'll be showing you how


28
00:00:57,536 --> 00:00:58,496
people occlusion works.


29
00:00:58,786 --> 00:00:59,686
And then, I'm going to hand it


30
00:00:59,686 --> 00:01:01,686
over to Tanmay to give you the


31
00:01:01,686 --> 00:01:03,666
deep dive of how motion capture


32
00:01:03,666 --> 00:01:03,956
works.


33
00:01:05,436 --> 00:01:06,036
So, let's begin.


34
00:01:07,456 --> 00:01:08,636
What is people occlusion?


35
00:01:09,276 --> 00:01:12,346
In ARKit today, we're already


36
00:01:12,346 --> 00:01:15,036
able to position rendered


37
00:01:15,036 --> 00:01:16,246
content in the real world.


38
00:01:16,686 --> 00:01:18,076
However, if we look at the video


39
00:01:18,076 --> 00:01:19,146
behind me, we can see that


40
00:01:19,146 --> 00:01:20,196
something is clearly wrong.


41
00:01:21,226 --> 00:01:23,476
What we would expect, is for the


42
00:01:23,476 --> 00:01:25,356
person closer to the camera to


43
00:01:25,356 --> 00:01:27,416
occlude the rendered content as


44
00:01:27,416 --> 00:01:28,846
it moves behind him.


45
00:01:28,846 --> 00:01:31,616
And, with people occlusion,


46
00:01:32,116 --> 00:01:33,906
we're bringing just that.


47
00:01:33,906 --> 00:01:36,346
We're enabling rendered content


48
00:01:36,346 --> 00:01:38,296
and people to correctly occlude


49
00:01:38,296 --> 00:01:39,276
each other in the scene.


50
00:01:39,666 --> 00:01:41,696
And, to understand what we need


51
00:01:41,696 --> 00:01:42,936
to do in order to enable that,


52
00:01:43,296 --> 00:01:46,456
I'm going to break down a frame.


53
00:01:46,676 --> 00:01:47,986
So, here we have the camera


54
00:01:47,986 --> 00:01:49,996
image, and we have two people


55
00:01:49,996 --> 00:01:51,136
standing around a table.


56
00:01:51,826 --> 00:01:53,556
And, we want to place a rendered


57
00:01:53,586 --> 00:01:55,046
object on top of this table.


58
00:01:56,576 --> 00:01:58,936
So far, for ARKit 2, the way we


59
00:01:58,936 --> 00:02:01,786
position rendered content is by


60
00:02:01,786 --> 00:02:03,196
simply overlaying it on the


61
00:02:03,196 --> 00:02:03,676
image.


62
00:02:04,076 --> 00:02:05,696
And, when we do it this way, we


63
00:02:05,696 --> 00:02:07,376
see that we end up with an


64
00:02:07,376 --> 00:02:08,596
incorrect occlusion.


65
00:02:09,686 --> 00:02:10,705
This is not what we would


66
00:02:10,705 --> 00:02:11,246
expect.


67
00:02:11,386 --> 00:02:12,966
And, we want to turn that red


68
00:02:12,966 --> 00:02:14,566
cross into a green checkmark.


69
00:02:15,266 --> 00:02:16,676
So, the way we do that is, we


70
00:02:16,676 --> 00:02:18,126
need to understand there's a


71
00:02:18,126 --> 00:02:19,676
person closer to the camera than


72
00:02:19,676 --> 00:02:20,496
the rendered object.


73
00:02:20,496 --> 00:02:23,236
And, when we do so, we correctly


74
00:02:23,346 --> 00:02:24,376
make sure that the rendered


75
00:02:24,376 --> 00:02:25,456
object gets occluded.


76
00:02:25,866 --> 00:02:29,316
And, this is essentially a depth


77
00:02:29,376 --> 00:02:30,306
ordering problem.


78
00:02:30,496 --> 00:02:32,526
And, in order to understand how


79
00:02:32,526 --> 00:02:33,796
we're going to solve this, I'm


80
00:02:33,796 --> 00:02:35,016
going to decompose the scene.


81
00:02:35,646 --> 00:02:38,756
So, here's the same image in an


82
00:02:38,756 --> 00:02:41,076
oblique angle, and let's explode


83
00:02:41,336 --> 00:02:42,336
the scene and look at the


84
00:02:42,336 --> 00:02:44,806
different depth planes that we


85
00:02:44,806 --> 00:02:45,046
have.


86
00:02:46,076 --> 00:02:46,966
We can see that we have


87
00:02:46,966 --> 00:02:48,106
different things in different


88
00:02:48,106 --> 00:02:50,006
depth planes, mixing real and


89
00:02:50,216 --> 00:02:50,666
rendered.


90
00:02:51,146 --> 00:02:53,006
And, here we have each person in


91
00:02:53,006 --> 00:02:55,776
their own depth plane, with the


92
00:02:55,776 --> 00:02:57,656
rendered object in between.


93
00:02:59,996 --> 00:03:02,006
For rendered content, the


94
00:03:02,006 --> 00:03:03,696
graphics pipeline already knows


95
00:03:03,696 --> 00:03:05,696
exactly where it is, simply by


96
00:03:05,696 --> 00:03:06,796
using a depth buffer.


97
00:03:07,676 --> 00:03:09,366
And, in order for us to also do


98
00:03:09,366 --> 00:03:10,756
the same thing for the real


99
00:03:10,756 --> 00:03:12,326
content, we need to understand


100
00:03:12,666 --> 00:03:13,706
where the people are in the


101
00:03:13,706 --> 00:03:14,126
scene.


102
00:03:14,296 --> 00:03:16,166
And, in order to enable this,


103
00:03:16,526 --> 00:03:18,206
we're adding two new buffers.


104
00:03:19,616 --> 00:03:21,486
We're adding the segmentation


105
00:03:21,486 --> 00:03:23,136
buffer, which tells you, per


106
00:03:23,136 --> 00:03:24,996
pixel, where a person is in this


107
00:03:24,996 --> 00:03:25,296
scene.


108
00:03:26,296 --> 00:03:27,996
And, we're also giving you a


109
00:03:28,026 --> 00:03:30,516
corresponding depth buffer which


110
00:03:30,516 --> 00:03:31,966
tells you where that person is


111
00:03:32,156 --> 00:03:32,766
in depth.


112
00:03:33,306 --> 00:03:36,316
Now, the amazing thing about


113
00:03:36,316 --> 00:03:39,006
this feature is that the way


114
00:03:39,006 --> 00:03:40,446
we're generating these buffers


115
00:03:40,746 --> 00:03:42,316
is by leveraging the power of


116
00:03:42,316 --> 00:03:44,616
the A12 chip and using machine


117
00:03:44,616 --> 00:03:45,916
learning in order to generate


118
00:03:45,916 --> 00:03:47,576
these buffers, using only the


119
00:03:47,576 --> 00:03:48,366
camera image.


120
00:03:48,886 --> 00:03:51,576
And so, these two new buffers


121
00:03:52,686 --> 00:03:54,766
will be exposed on the ARFrame


122
00:03:54,826 --> 00:03:56,586
as two new properties, the


123
00:03:56,586 --> 00:03:58,156
segmentationBuffer, and the


124
00:03:58,156 --> 00:03:59,976
estimatedDepthData.


125
00:04:03,186 --> 00:04:04,306
Since we want to use these


126
00:04:04,306 --> 00:04:06,346
buffers for people occlusion, we


127
00:04:06,346 --> 00:04:08,146
also have to have them generated


128
00:04:08,206 --> 00:04:09,796
at the same cadence as the


129
00:04:09,796 --> 00:04:10,506
camera frame.


130
00:04:11,146 --> 00:04:12,716
So, when your camera frame is


131
00:04:12,716 --> 00:04:14,266
running at 60 frames a second,


132
00:04:15,076 --> 00:04:16,696
we are also able to generate


133
00:04:16,696 --> 00:04:18,426
these buffers for you at 60


134
00:04:18,426 --> 00:04:22,076
frames a second.


135
00:04:22,076 --> 00:04:23,806
We would also like these buffers


136
00:04:23,846 --> 00:04:25,606
to be at the same resolution as


137
00:04:25,636 --> 00:04:26,466
the camera image.


138
00:04:27,596 --> 00:04:29,526
However, in order to enable this


139
00:04:29,526 --> 00:04:31,966
in real time, the neural network


140
00:04:31,966 --> 00:04:33,776
only sees a smaller image.


141
00:04:33,986 --> 00:04:36,096
And so, if you take the output


142
00:04:36,096 --> 00:04:38,106
of the neural network, and we


143
00:04:38,106 --> 00:04:39,766
magnify it, we will see that


144
00:04:39,766 --> 00:04:41,146
there's a lot of detail that the


145
00:04:41,146 --> 00:04:42,426
neural network just simply did


146
00:04:42,426 --> 00:04:43,026
not see.


147
00:04:44,456 --> 00:04:46,166
So, in order to compensate for


148
00:04:46,166 --> 00:04:48,076
this, we're doing some


149
00:04:48,076 --> 00:04:49,236
additional processing.


150
00:04:50,436 --> 00:04:51,756
We're applying matting.


151
00:04:52,136 --> 00:04:55,376
And, what matting does is, it's


152
00:04:55,376 --> 00:04:56,616
basically using the


153
00:04:56,616 --> 00:04:58,456
segmentationBuffer as a guide,


154
00:04:58,456 --> 00:05:00,316
and then looking at the camera


155
00:05:00,316 --> 00:05:01,616
image in order to figure out


156
00:05:01,616 --> 00:05:02,746
what the missing detail was.


157
00:05:03,846 --> 00:05:05,986
And now, with a matted image,


158
00:05:05,986 --> 00:05:07,846
we're able to correctly extract


159
00:05:07,846 --> 00:05:09,686
the people from the scene, and


160
00:05:09,686 --> 00:05:10,376
together with the


161
00:05:10,376 --> 00:05:12,636
estimatedDepthData, we can then


162
00:05:12,636 --> 00:05:13,996
position them in the correct


163
00:05:13,996 --> 00:05:14,626
depth plane.


164
00:05:15,376 --> 00:05:16,876
Finally letting us solve the


165
00:05:16,876 --> 00:05:18,526
depth ordering problem, and we


166
00:05:18,526 --> 00:05:20,316
can then recompose the scene.


167
00:05:20,966 --> 00:05:23,076
Now, this is a lot of


168
00:05:23,076 --> 00:05:25,226
technology, and we want to make


169
00:05:25,226 --> 00:05:27,216
it as easy as possible for you


170
00:05:27,216 --> 00:05:28,526
developers to adopt this.


171
00:05:28,996 --> 00:05:30,446
So, we're enabling this feature


172
00:05:30,586 --> 00:05:32,406
in three different ways.


173
00:05:33,076 --> 00:05:35,726
First, we have RealityKit, the


174
00:05:35,726 --> 00:05:37,276
new framework that we announced


175
00:05:37,506 --> 00:05:38,626
this Dub-dub.


176
00:05:39,056 --> 00:05:40,216
However, if you've already been


177
00:05:40,216 --> 00:05:42,646
using SceneKit, and we also


178
00:05:42,646 --> 00:05:43,946
added support for people


179
00:05:43,946 --> 00:05:45,916
occlusion using the ARCSNView.


180
00:05:46,656 --> 00:05:47,836
And, in case you have your own


181
00:05:47,836 --> 00:05:49,846
renderer, or you're working with


182
00:05:49,846 --> 00:05:51,626
a third-party rendering, we're


183
00:05:51,626 --> 00:05:53,166
giving you the building blocks


184
00:05:53,636 --> 00:05:55,436
to enable incorporating people


185
00:05:55,436 --> 00:05:57,206
occlusion into your own app


186
00:05:57,256 --> 00:05:58,356
using the power of Metal.


187
00:05:59,746 --> 00:06:00,986
So, let's look at how we'd do it


188
00:06:00,986 --> 00:06:01,706
in RealityKit.


189
00:06:03,846 --> 00:06:06,586
RealityKit is the recommended


190
00:06:06,586 --> 00:06:07,976
way if you're going to build a


191
00:06:07,976 --> 00:06:09,756
new AR app.


192
00:06:10,046 --> 00:06:11,756
It has a new UI element called


193
00:06:11,756 --> 00:06:14,626
the ARView, which enables you--


194
00:06:14,876 --> 00:06:16,366
gives you an easy-to-use API


195
00:06:16,366 --> 00:06:19,166
that brings photorealism to AR,


196
00:06:19,706 --> 00:06:21,496
further blending the border


197
00:06:21,496 --> 00:06:24,036
between the real and the


198
00:06:24,036 --> 00:06:24,866
rendered content.


199
00:06:25,516 --> 00:06:28,856
It also has built-in support for


200
00:06:28,856 --> 00:06:29,626
people occlusion.


201
00:06:30,276 --> 00:06:31,476
And, if you attended the What's


202
00:06:31,476 --> 00:06:32,966
New session, you have already


203
00:06:32,966 --> 00:06:35,076
seen a live demo of how you can


204
00:06:35,076 --> 00:06:36,506
enable people occlusion in


205
00:06:36,506 --> 00:06:36,876
ARView.


206
00:06:37,496 --> 00:06:38,786
And so, for this deep dive,


207
00:06:38,876 --> 00:06:39,966
we'll do a quick recap.


208
00:06:40,606 --> 00:06:42,226
Let's look at some code.


209
00:06:43,296 --> 00:06:45,466
So, here I have my viewDidLoad


210
00:06:45,466 --> 00:06:47,136
function in my view controller.


211
00:06:47,616 --> 00:06:48,716
And, the first thing I need to


212
00:06:48,716 --> 00:06:50,256
do is to make sure that it's


213
00:06:50,296 --> 00:06:51,186
future supported.


214
00:06:52,396 --> 00:06:53,936
I do that by checking my


215
00:06:53,936 --> 00:06:55,996
configuration, shown as my


216
00:06:55,996 --> 00:06:57,216
WorldTrackingConfiguration.


217
00:06:58,076 --> 00:06:59,506
And, I need to look for a new


218
00:06:59,506 --> 00:07:01,286
property called FrameSemantics.


219
00:07:01,536 --> 00:07:02,696
And, the FrameSemantics we're


220
00:07:02,696 --> 00:07:03,916
going to use to enable people


221
00:07:03,916 --> 00:07:04,686
occlusion is


222
00:07:04,726 --> 00:07:06,396
personSegmentationWithDepth.


223
00:07:07,756 --> 00:07:08,896
Once I know that this is


224
00:07:08,976 --> 00:07:11,836
supported on my configuration,


225
00:07:12,706 --> 00:07:14,246
the only thing I have to do is


226
00:07:14,246 --> 00:07:15,966
set the FrameSemantics on my


227
00:07:15,966 --> 00:07:18,236
configuration, and then, when


228
00:07:18,236 --> 00:07:19,886
the session starts running, the


229
00:07:19,886 --> 00:07:21,716
ARView will automatically pick


230
00:07:21,716 --> 00:07:23,886
this up, and enable people


231
00:07:23,886 --> 00:07:25,516
occlusion for my AR application.


232
00:07:26,976 --> 00:07:29,056
So, all we had to do was really


233
00:07:29,056 --> 00:07:30,446
to change our configuration,


234
00:07:30,446 --> 00:07:31,696
using the new property that


235
00:07:31,696 --> 00:07:33,046
we're introducing for this year,


236
00:07:33,846 --> 00:07:34,696
FrameSemantics.


237
00:07:35,106 --> 00:07:36,546
And, as you saw in the previous


238
00:07:36,546 --> 00:07:37,676
example, I was using


239
00:07:37,676 --> 00:07:39,356
personSegmentationWithDepth.


240
00:07:39,916 --> 00:07:41,136
But, we can also do only


241
00:07:41,166 --> 00:07:42,536
PersonSegmentation in case you


242
00:07:42,536 --> 00:07:43,886
want to enable a weatherman-like


243
00:07:43,886 --> 00:07:44,506
experience.


244
00:07:45,896 --> 00:07:48,256
Now, ARView is the recommended


245
00:07:48,256 --> 00:07:50,806
way for using people occlusion.


246
00:07:51,196 --> 00:07:53,156
And, the reason is, it has a


247
00:07:53,226 --> 00:07:54,706
deep renderer integration.


248
00:07:54,956 --> 00:07:56,596
And, what that means is the


249
00:07:56,596 --> 00:07:58,306
entire graphics pipeline is


250
00:07:58,306 --> 00:07:59,996
aware that there are people in


251
00:07:59,996 --> 00:08:00,516
the scene.


252
00:08:00,996 --> 00:08:02,836
And, it's therefore able to also


253
00:08:02,836 --> 00:08:04,666
handle transparent objects when


254
00:08:04,666 --> 00:08:05,626
you're using this feature.


255
00:08:06,676 --> 00:08:08,486
It's doing this while also being


256
00:08:08,486 --> 00:08:10,186
built for optimal performance.


257
00:08:10,566 --> 00:08:12,646
And, if you're wondering what


258
00:08:12,646 --> 00:08:14,216
kind of experiences you can


259
00:08:14,216 --> 00:08:16,126
enable using people occlusion


260
00:08:16,296 --> 00:08:17,946
and RealityKit, I would like to


261
00:08:18,026 --> 00:08:19,906
play a short video for you.


262
00:08:21,516 --> 00:08:35,500
[ Music ]


263
00:08:38,046 --> 00:08:39,135
This is Swiftstrike.


264
00:08:39,746 --> 00:08:41,405
It's a really cool demo that


265
00:08:41,405 --> 00:08:42,395
we're showing right here in


266
00:08:42,395 --> 00:08:44,716
Dub-dub, and I urge everyone to


267
00:08:44,716 --> 00:08:46,126
check it out if you haven't done


268
00:08:46,126 --> 00:08:46,726
so already.


269
00:08:48,076 --> 00:08:49,186
So, what if you've been using


270
00:08:49,256 --> 00:08:49,696
SceneKit?


271
00:08:50,306 --> 00:08:52,606
Well, let's look at how I would


272
00:08:52,606 --> 00:08:53,746
enable people occlusion for


273
00:08:53,746 --> 00:08:54,176
SceneKit.


274
00:08:56,436 --> 00:08:57,816
If you've already been using


275
00:08:57,936 --> 00:08:59,776
ARSCNView, we're enabling people


276
00:08:59,776 --> 00:09:01,686
occlusion in very much the same


277
00:09:01,686 --> 00:09:02,636
way we're enabling for


278
00:09:02,636 --> 00:09:03,306
RealityKit.


279
00:09:04,056 --> 00:09:05,376
All we have to do is set the


280
00:09:05,376 --> 00:09:07,056
FrameSemantics on our


281
00:09:07,056 --> 00:09:09,176
configuration, and the ARSCNView


282
00:09:09,176 --> 00:09:10,606
will automatically pick this up.


283
00:09:11,736 --> 00:09:13,146
However, there's a difference


284
00:09:13,146 --> 00:09:14,366
between the SceneKit


285
00:09:14,366 --> 00:09:15,896
implementation and RealityKit,


286
00:09:16,236 --> 00:09:18,216
where SceneKit does a


287
00:09:18,356 --> 00:09:19,896
post-processing composition.


288
00:09:20,296 --> 00:09:21,206
And, what that means for you


289
00:09:21,206 --> 00:09:23,416
concretely is it may not work as


290
00:09:23,416 --> 00:09:24,826
well with transparent objects,


291
00:09:24,826 --> 00:09:26,016
depending on how you write


292
00:09:26,846 --> 00:09:26,966
depth.


293
00:09:28,196 --> 00:09:30,206
Finally, what if I have my own


294
00:09:30,206 --> 00:09:31,056
rendering engine?


295
00:09:32,276 --> 00:09:33,946
Well, we want to enable you to


296
00:09:33,946 --> 00:09:35,646
incorporate people occlusion


297
00:09:35,646 --> 00:09:37,006
into your own rendering engine,


298
00:09:37,616 --> 00:09:38,756
or a third-party rendering


299
00:09:38,756 --> 00:09:39,156
engine.


300
00:09:39,756 --> 00:09:42,096
And, what this gives you is


301
00:09:42,096 --> 00:09:43,286
complete control over the


302
00:09:43,286 --> 00:09:43,896
composition.


303
00:09:43,896 --> 00:09:47,126
We want to give you as much


304
00:09:47,126 --> 00:09:49,086
flexibility as possible, while


305
00:09:49,086 --> 00:09:50,866
still giving you easy-to-use API


306
00:09:51,616 --> 00:09:52,876
in order to incorporate this


307
00:09:52,876 --> 00:09:53,426
great feature.


308
00:09:54,216 --> 00:09:55,566
So, before I show you how we do


309
00:09:55,566 --> 00:09:57,256
this, let's do a quick review.


310
00:09:58,496 --> 00:10:01,116
We had the segmentationBuffer


311
00:10:01,116 --> 00:10:02,136
that came out of the neural


312
00:10:02,136 --> 00:10:04,386
network, that was working on a


313
00:10:04,386 --> 00:10:04,976
smaller image.


314
00:10:05,346 --> 00:10:06,816
And then, we applied matting in


315
00:10:06,816 --> 00:10:08,196
order to recapture some of that


316
00:10:08,196 --> 00:10:08,906
missing detail.


317
00:10:10,306 --> 00:10:11,646
Well, when we do our custom


318
00:10:11,646 --> 00:10:13,936
composition, we're providing a


319
00:10:13,936 --> 00:10:16,086
new class that will generate


320
00:10:16,086 --> 00:10:17,346
this matte for you, using the


321
00:10:17,386 --> 00:10:19,076
power of Metal, giving you the


322
00:10:19,076 --> 00:10:20,596
matte as a texture for you to


323
00:10:20,596 --> 00:10:21,596
incorporate into your own


324
00:10:21,596 --> 00:10:22,016
pipeline.


325
00:10:22,686 --> 00:10:24,296
Let's look at an example of how


326
00:10:24,296 --> 00:10:25,786
we can do just that.


327
00:10:26,796 --> 00:10:28,416
So, here I have my custom


328
00:10:28,416 --> 00:10:29,406
composition function.


329
00:10:29,826 --> 00:10:31,626
And, the first thing I do is to


330
00:10:31,626 --> 00:10:32,866
make sure whether this feature


331
00:10:32,866 --> 00:10:34,276
is supported.


332
00:10:34,276 --> 00:10:37,006
And, once I've done that, all I


333
00:10:37,006 --> 00:10:38,246
have to do is call


334
00:10:38,246 --> 00:10:39,906
generateMatte, and I give it the


335
00:10:39,906 --> 00:10:41,646
frame and the commandBuffer, and


336
00:10:41,646 --> 00:10:42,836
it gives me back the Metal


337
00:10:42,836 --> 00:10:45,146
texture that I can then use when


338
00:10:45,146 --> 00:10:46,916
I do my own custom composition,


339
00:10:47,566 --> 00:10:49,106
and finally schedule everything


340
00:10:49,106 --> 00:10:49,786
to the GPU.


341
00:10:50,336 --> 00:10:52,406
So, the class that we're


342
00:10:52,406 --> 00:10:54,196
bringing to ARKit is called


343
00:10:54,196 --> 00:10:55,906
ARMatteGenerator, and as you saw


344
00:10:55,906 --> 00:10:57,586
in the example, it takes the


345
00:10:57,586 --> 00:10:58,986
ARFrame and the commandBuffer,


346
00:10:58,986 --> 00:11:00,806
and it gives you back a texture


347
00:11:00,806 --> 00:11:01,616
that you can use.


348
00:11:02,366 --> 00:11:04,286
However, we're not done yet.


349
00:11:05,056 --> 00:11:06,976
Just like the segmentationBuffer


350
00:11:06,976 --> 00:11:09,276
was of a lower resolution, the


351
00:11:09,276 --> 00:11:11,076
estimatedDepthData is also


352
00:11:11,076 --> 00:11:13,236
lower, and if we simply magnify


353
00:11:13,236 --> 00:11:15,046
it, and overlay it on our matted


354
00:11:15,046 --> 00:11:17,686
image, we will see that there


355
00:11:17,686 --> 00:11:18,776
might be a mismatch.


356
00:11:19,366 --> 00:11:20,836
We can have depth values where


357
00:11:20,836 --> 00:11:22,086
there's no alpha value in the


358
00:11:22,086 --> 00:11:23,706
matte, and more importantly, we


359
00:11:23,706 --> 00:11:25,346
can have alpha values in the


360
00:11:25,346 --> 00:11:26,186
matte where there's no


361
00:11:26,186 --> 00:11:27,456
corresponding depth value.


362
00:11:31,176 --> 00:11:32,626
Now, since the matte has already


363
00:11:32,626 --> 00:11:33,976
recaptured some of the missing


364
00:11:33,976 --> 00:11:35,526
detail, we can't really modify


365
00:11:35,526 --> 00:11:36,486
the alpha itself.


366
00:11:36,686 --> 00:11:38,476
Instead, we need to modify the


367
00:11:38,476 --> 00:11:39,186
depth buffer.


368
00:11:39,746 --> 00:11:42,896
So, let's go back to my previous


369
00:11:42,896 --> 00:11:44,426
example, and see how we do just


370
00:11:44,426 --> 00:11:45,346
that.


371
00:11:45,866 --> 00:11:47,766
So, here we have the line where


372
00:11:47,766 --> 00:11:49,306
I added-- we generated the matte


373
00:11:49,446 --> 00:11:50,496
in order for me to use for my


374
00:11:50,496 --> 00:11:51,526
custom composition.


375
00:11:51,906 --> 00:11:53,956
And so I add an addition


376
00:11:54,096 --> 00:11:55,356
function call to


377
00:11:55,356 --> 00:11:56,736
generateDilatedDepth.


378
00:11:57,076 --> 00:11:58,446
And, much in the same way, it


379
00:11:58,446 --> 00:11:59,816
takes the frame in the command


380
00:11:59,816 --> 00:12:01,516
buffer, and gives me back a


381
00:12:01,566 --> 00:12:01,916
texture.


382
00:12:02,536 --> 00:12:05,516
And, if you look at the API, it


383
00:12:05,516 --> 00:12:07,106
looks very similar to how we


384
00:12:07,106 --> 00:12:09,696
generated the matte, giving us


385
00:12:09,756 --> 00:12:11,966
the texture, taking the frame


386
00:12:11,966 --> 00:12:12,816
and the command buffer.


387
00:12:14,026 --> 00:12:15,926
And, what this does is ensures


388
00:12:15,926 --> 00:12:17,876
that for every alpha value we


389
00:12:17,876 --> 00:12:19,486
find in the matte, we will have


390
00:12:19,486 --> 00:12:21,076
a corresponding depth value that


391
00:12:21,076 --> 00:12:22,546
we can then use when we do our


392
00:12:22,546 --> 00:12:23,536
final composition.


393
00:12:24,956 --> 00:12:27,096
So, with the dilated depth, and


394
00:12:27,096 --> 00:12:28,976
the matte, we're now finally


395
00:12:28,976 --> 00:12:30,806
able to move on to composition.


396
00:12:32,556 --> 00:12:34,536
Composition is usually done on


397
00:12:34,536 --> 00:12:36,046
the GPU in the fragment shader.


398
00:12:36,516 --> 00:12:37,796
So, let's look at an example


399
00:12:37,796 --> 00:12:39,436
shader of how I bring everything


400
00:12:39,436 --> 00:12:39,846
together.


401
00:12:40,436 --> 00:12:43,726
I begin by doing what we would


402
00:12:43,726 --> 00:12:45,716
usually do for an AR experience.


403
00:12:46,056 --> 00:12:48,376
I sample the camera image, as


404
00:12:48,376 --> 00:12:49,626
well as the rendered texture,


405
00:12:50,386 --> 00:12:51,486
but since we're going to do


406
00:12:51,486 --> 00:12:53,096
occlusion, I'm also sampling the


407
00:12:53,096 --> 00:12:54,446
rendered depth.


408
00:12:55,056 --> 00:12:57,186
And then, I do what I would


409
00:12:57,216 --> 00:12:58,976
usually do in AR, I simply


410
00:12:58,976 --> 00:13:00,636
overlay the rendered content on


411
00:13:00,636 --> 00:13:02,856
the real image, given the


412
00:13:03,046 --> 00:13:03,766
rendered alpha.


413
00:13:05,236 --> 00:13:07,816
The new part, in order to enable


414
00:13:07,816 --> 00:13:09,506
people occlusion, is I also


415
00:13:09,506 --> 00:13:11,516
sample my matte, and my


416
00:13:11,516 --> 00:13:12,336
dilatedDepth.


417
00:13:13,556 --> 00:13:16,336
And then, I make sure to compare


418
00:13:16,336 --> 00:13:17,936
the dilatedDepth with the


419
00:13:17,936 --> 00:13:18,686
renderedDepth.


420
00:13:18,686 --> 00:13:20,726
And, if I find that there's


421
00:13:20,726 --> 00:13:21,966
something in the dilatedDepth


422
00:13:21,966 --> 00:13:23,206
that's closer to the camera,


423
00:13:23,206 --> 00:13:24,546
meaning that there might be a


424
00:13:24,546 --> 00:13:26,976
person there, I then mix the


425
00:13:26,976 --> 00:13:29,276
camera back, given the value of


426
00:13:29,276 --> 00:13:29,616
the matte.


427
00:13:30,256 --> 00:13:32,166
However, if the render content


428
00:13:32,166 --> 00:13:34,536
is closer to the camera, I


429
00:13:34,536 --> 00:13:36,106
simply do what we always do, and


430
00:13:36,106 --> 00:13:37,856
have the render content overlaid


431
00:13:37,856 --> 00:13:38,526
on top of it.


432
00:13:38,526 --> 00:13:41,106
And, with all of this, we're


433
00:13:41,196 --> 00:13:43,846
finally able to do people


434
00:13:43,846 --> 00:13:45,676
occlusion in our own custom


435
00:13:45,676 --> 00:13:45,976
renderer.


436
00:13:49,516 --> 00:13:54,636
[ Applause ]


437
00:13:55,136 --> 00:13:56,696
Since this feature is using the


438
00:13:56,696 --> 00:13:59,196
neural engine, and machine


439
00:13:59,196 --> 00:14:01,296
learning, it's supported on A12


440
00:14:01,296 --> 00:14:02,286
devices and later.


441
00:14:03,826 --> 00:14:05,796
It also works best in indoor


442
00:14:05,796 --> 00:14:06,416
environments.


443
00:14:07,226 --> 00:14:08,956
And, in all the videos you saw,


444
00:14:08,956 --> 00:14:10,316
you saw people standing in the


445
00:14:10,316 --> 00:14:12,066
scene, but this feature also


446
00:14:12,066 --> 00:14:13,426
works for your own hands and


447
00:14:13,426 --> 00:14:13,696
feet.


448
00:14:15,006 --> 00:14:16,366
It also works for multiple


449
00:14:16,366 --> 00:14:17,176
people in the scene.


450
00:14:18,386 --> 00:14:20,066
Before I hand it over to Tanmay


451
00:14:20,136 --> 00:14:21,286
to show you all about motion


452
00:14:21,286 --> 00:14:24,596
capture, let's do a quick recap.


453
00:14:24,596 --> 00:14:26,626
So, with people occlusion, we're


454
00:14:26,626 --> 00:14:28,596
enabling correct occlusion


455
00:14:28,696 --> 00:14:30,176
between rendered and real


456
00:14:30,176 --> 00:14:31,306
content for people.


457
00:14:33,136 --> 00:14:34,936
The recommended way, if you're


458
00:14:34,936 --> 00:14:36,506
building a new app, is to use


459
00:14:36,506 --> 00:14:39,006
RealityKit and ARView, since


460
00:14:39,006 --> 00:14:40,716
this does the deep integration.


461
00:14:41,706 --> 00:14:43,556
If you already have an app using


462
00:14:43,556 --> 00:14:45,156
SceneKit, we've also added


463
00:14:45,156 --> 00:14:46,796
support for people occlusion in


464
00:14:46,796 --> 00:14:47,806
the ARSCNView.


465
00:14:49,246 --> 00:14:50,166
And, if you have your own


466
00:14:50,166 --> 00:14:51,736
renderer, we're providing you


467
00:14:51,736 --> 00:14:53,266
with a new API called the


468
00:14:53,266 --> 00:14:55,436
ARMatteGenerator, so you can do


469
00:14:55,436 --> 00:14:56,756
your own compositing and


470
00:14:56,756 --> 00:14:58,136
incorporating into your own


471
00:14:58,136 --> 00:14:58,526
renderer.


472
00:14:59,346 --> 00:15:00,766
And, with that, I'd like to hand


473
00:15:00,766 --> 00:15:02,496
it over to Tanmay to give you a


474
00:15:02,496 --> 00:15:03,916
deep dive into motion capture.


475
00:15:04,516 --> 00:15:10,546
[ Applause ]


476
00:15:11,046 --> 00:15:12,156
>> Thank you, Adrian.


477
00:15:12,566 --> 00:15:13,436
Hello everyone.


478
00:15:13,596 --> 00:15:15,666
I'm Tanmay, and today I'm going


479
00:15:15,666 --> 00:15:17,326
to introduce you to this new


480
00:15:17,326 --> 00:15:18,766
piece of technology that we are


481
00:15:18,766 --> 00:15:20,636
bringing this year, motion


482
00:15:20,826 --> 00:15:21,866
capture.


483
00:15:22,516 --> 00:15:25,316
[ Applause ]


484
00:15:25,816 --> 00:15:28,046
So, what is motion capture?


485
00:15:28,746 --> 00:15:30,716
It's just a process of capturing


486
00:15:30,716 --> 00:15:31,766
movement of people.


487
00:15:33,196 --> 00:15:35,566
You see a person, you capture


488
00:15:35,566 --> 00:15:37,516
all the movements, and you use


489
00:15:37,566 --> 00:15:39,466
that motion to animate a virtual


490
00:15:39,466 --> 00:15:41,926
character so that the character


491
00:15:41,926 --> 00:15:43,876
performs the same set of actions


492
00:15:43,936 --> 00:15:45,856
as the person you're looking at.


493
00:15:45,856 --> 00:15:47,446
And, we're trying to enable this


494
00:15:47,446 --> 00:15:49,116
technology in your applications.


495
00:15:50,516 --> 00:15:51,636
Now, let's dive in.


496
00:15:52,476 --> 00:15:54,326
We would like the character to


497
00:15:54,326 --> 00:15:55,646
mimic the person that you're


498
00:15:55,646 --> 00:15:56,156
looking at.


499
00:15:56,346 --> 00:15:58,146
But, before we do that, we need


500
00:15:58,146 --> 00:16:00,456
to understand what exactly are


501
00:16:00,456 --> 00:16:01,406
we trying to animate?


502
00:16:01,816 --> 00:16:03,346
What does a character entail?


503
00:16:04,646 --> 00:16:06,066
So, this is an example of a


504
00:16:06,066 --> 00:16:06,966
virtual character.


505
00:16:06,966 --> 00:16:09,096
Let's take an x-ray of it and


506
00:16:09,096 --> 00:16:10,096
see what's inside it.


507
00:16:10,756 --> 00:16:13,806
You can see that it has two main


508
00:16:13,806 --> 00:16:14,466
components.


509
00:16:14,716 --> 00:16:16,186
The outer coating, which is


510
00:16:16,326 --> 00:16:17,256
called a mesh.


511
00:16:17,576 --> 00:16:19,296
And, the bony structure inside,


512
00:16:19,366 --> 00:16:20,776
which is called a skeleton.


513
00:16:21,026 --> 00:16:23,686
And, these two combined together


514
00:16:23,686 --> 00:16:25,426
give you the complete character.


515
00:16:26,766 --> 00:16:28,196
The skeleton is the driving


516
00:16:28,196 --> 00:16:30,186
force behind the entire


517
00:16:30,186 --> 00:16:30,686
character.


518
00:16:31,206 --> 00:16:32,906
It contains all the limbs that


519
00:16:32,906 --> 00:16:34,276
we use for motion, just like


520
00:16:34,316 --> 00:16:34,706
people.


521
00:16:35,606 --> 00:16:37,346
And so, in order to animate a


522
00:16:37,346 --> 00:16:39,196
character, we need to animate


523
00:16:39,256 --> 00:16:40,496
the skeleton with the same


524
00:16:40,496 --> 00:16:40,906
motion.


525
00:16:42,066 --> 00:16:43,706
So, what's the first step here?


526
00:16:43,986 --> 00:16:44,886
We have a person.


527
00:16:45,076 --> 00:16:46,846
And, the first step here is to


528
00:16:46,846 --> 00:16:48,296
have the skeleton mimic this


529
00:16:48,296 --> 00:16:48,726
person.


530
00:16:48,916 --> 00:16:52,186
And, this is what it looks like.


531
00:16:52,186 --> 00:16:54,146
And, once the skeleton moves,


532
00:16:55,026 --> 00:16:56,566
the character follows suit.


533
00:16:57,696 --> 00:17:00,426
And, you have the entire virtual


534
00:17:00,426 --> 00:17:02,116
character mimicking you


535
00:17:02,116 --> 00:17:02,876
automatically.


536
00:17:03,466 --> 00:17:06,296
So, how do we do it?


537
00:17:07,806 --> 00:17:09,306
How do we animate this skeleton?


538
00:17:09,665 --> 00:17:12,086
Given this image, we use machine


539
00:17:12,086 --> 00:17:14,685
learning technology to first


540
00:17:14,685 --> 00:17:16,236
estimate the pose of the person


541
00:17:16,236 --> 00:17:16,886
in the image.


542
00:17:17,316 --> 00:17:20,185
And, we use that pose to build a


543
00:17:20,185 --> 00:17:21,846
full-fledged, high-fidelity


544
00:17:21,846 --> 00:17:22,445
skeleton.


545
00:17:23,076 --> 00:17:25,756
And, finally, we use this


546
00:17:25,796 --> 00:17:27,165
skeleton, and combine it with a


547
00:17:27,165 --> 00:17:28,626
mesh to give you the final


548
00:17:28,626 --> 00:17:29,156
character.


549
00:17:29,456 --> 00:17:32,066
And, we interface all of this,


550
00:17:32,146 --> 00:17:33,266
everything that you're looking


551
00:17:33,266 --> 00:17:34,626
here, through ARKit.


552
00:17:35,336 --> 00:17:39,366
To get a complete overview, we


553
00:17:39,366 --> 00:17:40,706
are introducing the technology


554
00:17:40,706 --> 00:17:42,616
of motion capture in this year's


555
00:17:42,616 --> 00:17:42,976
ARKit.


556
00:17:43,716 --> 00:17:45,166
And, with that, you can track


557
00:17:45,166 --> 00:17:47,526
movement of people in real time,


558
00:17:47,696 --> 00:17:48,676
on your devices.


559
00:17:49,666 --> 00:17:51,246
It works seamlessly with


560
00:17:51,246 --> 00:17:53,696
RealityKit, which gives you the


561
00:17:53,696 --> 00:17:55,576
ability to drive animated


562
00:17:55,576 --> 00:17:57,336
characters, and render them on


563
00:17:57,336 --> 00:17:58,036
your screens.


564
00:17:58,946 --> 00:18:00,386
It's powered by machine learning


565
00:18:00,386 --> 00:18:02,076
and runs smoothly on Apple


566
00:18:02,076 --> 00:18:02,856
Neural Engine.


567
00:18:03,806 --> 00:18:05,616
And, we have made it available


568
00:18:05,616 --> 00:18:07,776
on A12 devices and beyond.


569
00:18:09,216 --> 00:18:10,356
So, now you have this


570
00:18:10,386 --> 00:18:12,316
technology, what can you use it


571
00:18:12,376 --> 00:18:12,626
for?


572
00:18:12,706 --> 00:18:14,846
What can you enable with it?


573
00:18:15,516 --> 00:18:17,036
Well, for starters, you can


574
00:18:17,036 --> 00:18:18,726
always have a virtual character


575
00:18:18,936 --> 00:18:20,226
follow and track a person.


576
00:18:20,546 --> 00:18:21,916
You can have your own puppet in


577
00:18:22,056 --> 00:18:22,256
AR.


578
00:18:22,256 --> 00:18:24,506
And, this is something that we


579
00:18:24,506 --> 00:18:25,856
enable out of the box.


580
00:18:26,306 --> 00:18:27,886
But, beyond that, there are a


581
00:18:27,886 --> 00:18:29,956
lot of other use cases as well,


582
00:18:30,046 --> 00:18:31,106
that you can enable with it.


583
00:18:32,006 --> 00:18:34,286
For example, you can enhance it


584
00:18:34,286 --> 00:18:35,476
further by creating your own


585
00:18:35,476 --> 00:18:37,366
models for detecting what


586
00:18:37,366 --> 00:18:38,766
actions people are doing.


587
00:18:39,316 --> 00:18:41,996
You can use it to build tools


588
00:18:41,996 --> 00:18:44,036
for analyzing motions like how


589
00:18:44,036 --> 00:18:46,526
good a golf swing is, or is your


590
00:18:46,526 --> 00:18:47,966
posture correct, or are you


591
00:18:47,966 --> 00:18:49,466
performing an exercise in a


592
00:18:49,466 --> 00:18:51,026
correct way or not?


593
00:18:52,536 --> 00:18:54,476
Also, now since a person has a


594
00:18:54,476 --> 00:18:56,766
virtual presence in the scene,


595
00:18:56,946 --> 00:18:58,506
you can enable interaction with


596
00:18:58,506 --> 00:19:00,446
any virtual object that you


597
00:19:00,446 --> 00:19:00,656
like.


598
00:19:00,656 --> 00:19:04,096
And, it's supported for all the


599
00:19:04,096 --> 00:19:06,806
virtual objects present in the


600
00:19:07,376 --> 00:19:07,496
scene.


601
00:19:07,496 --> 00:19:09,486
And, finally, you can also use


602
00:19:09,486 --> 00:19:11,406
it for image and video


603
00:19:11,406 --> 00:19:12,086
analytics.


604
00:19:12,406 --> 00:19:14,356
Because we also provide 2D


605
00:19:14,356 --> 00:19:15,716
version of the skeleton as well


606
00:19:15,906 --> 00:19:17,206
in image space, and you can


607
00:19:17,206 --> 00:19:19,136
build it to use editing tools,


608
00:19:19,196 --> 00:19:20,816
or for semantic image


609
00:19:20,816 --> 00:19:21,536
understanding.


610
00:19:21,976 --> 00:19:27,966
And, this doesn't even cover the


611
00:19:27,966 --> 00:19:30,126
entire set of possibilities that


612
00:19:30,126 --> 00:19:31,926
you can enable with it remotely.


613
00:19:32,416 --> 00:19:33,766
There are so many other things


614
00:19:33,766 --> 00:19:35,186
that you can do with it.


615
00:19:35,866 --> 00:19:37,766
Now, let me show you how you can


616
00:19:37,766 --> 00:19:39,236
leverage motion capture for your


617
00:19:39,236 --> 00:19:39,966
applications.


618
00:19:40,816 --> 00:19:42,926
Depending on the use cases, we


619
00:19:42,926 --> 00:19:44,096
have three different ways of


620
00:19:44,096 --> 00:19:44,566
using it.


621
00:19:45,706 --> 00:19:48,286
The first one is motion capture


622
00:19:48,286 --> 00:19:49,156
in RealityKit.


623
00:19:50,366 --> 00:19:51,576
If you just want to quickly


624
00:19:51,576 --> 00:19:54,086
animate a character, this


625
00:19:54,086 --> 00:19:55,696
high-level API will help you get


626
00:19:55,696 --> 00:19:55,986
there.


627
00:19:56,676 --> 00:19:58,676
If you want to enable advanced


628
00:19:58,676 --> 00:20:00,766
use cases, like activity


629
00:20:00,766 --> 00:20:02,426
recognition, analysis, or


630
00:20:02,426 --> 00:20:04,136
interaction with 3D objects in a


631
00:20:04,136 --> 00:20:06,646
scene, we've also provided


632
00:20:06,646 --> 00:20:09,026
low-level APIs to extract each


633
00:20:09,026 --> 00:20:10,276
and every element of the


634
00:20:10,276 --> 00:20:10,956
skeleton.


635
00:20:12,156 --> 00:20:15,196
And, it's for you to use it.


636
00:20:15,196 --> 00:20:17,246
And, finally, if your use case


637
00:20:17,246 --> 00:20:18,606
requires 2D versions of the


638
00:20:18,606 --> 00:20:20,966
skeleton in image space, maybe


639
00:20:20,966 --> 00:20:22,366
for doing semantic image


640
00:20:22,366 --> 00:20:24,266
analysis, or for editing tools,


641
00:20:24,306 --> 00:20:25,866
or for something else, we've


642
00:20:25,866 --> 00:20:29,556
provided access to that as well.


643
00:20:29,746 --> 00:20:31,156
So, let's get started with


644
00:20:31,156 --> 00:20:32,806
motion capture in RealityKit.


645
00:20:34,226 --> 00:20:35,926
As you all know, we introduced


646
00:20:35,926 --> 00:20:39,146
RealityKit this year, and given


647
00:20:39,146 --> 00:20:41,286
our API in RealityKit, in just


648
00:20:41,396 --> 00:20:43,106
few lines of code, you can track


649
00:20:43,106 --> 00:20:45,616
a person and add a character to


650
00:20:45,616 --> 00:20:45,996
mimic.


651
00:20:47,526 --> 00:20:49,476
We've provided a very simple and


652
00:20:49,476 --> 00:20:51,066
easy-to-use API for this


653
00:20:51,116 --> 00:20:51,506
purpose.


654
00:20:52,496 --> 00:20:54,106
You can add your own custom


655
00:20:54,146 --> 00:20:55,966
characters as well, based on the


656
00:20:55,966 --> 00:20:57,406
structure of our provided


657
00:20:57,406 --> 00:20:58,006
example.


658
00:20:58,516 --> 00:21:01,246
So, you can use any mesh, any


659
00:21:01,246 --> 00:21:02,406
character that you want,


660
00:21:02,556 --> 00:21:06,076
provided that it is based on the


661
00:21:06,076 --> 00:21:07,546
structure of our provided


662
00:21:07,546 --> 00:21:08,976
example in the sample bundle.


663
00:21:10,956 --> 00:21:13,146
Finally, the tracked person is


664
00:21:13,146 --> 00:21:15,416
very easy to access here, via an


665
00:21:15,416 --> 00:21:17,396
element called AnchorEntities,


666
00:21:17,676 --> 00:21:18,766
which are basically just


667
00:21:18,766 --> 00:21:20,126
building blocks of the scene.


668
00:21:20,666 --> 00:21:22,176
And, it automatically gathers


669
00:21:22,356 --> 00:21:24,256
all the required transforms that


670
00:21:24,256 --> 00:21:25,526
we need for motion capture.


671
00:21:26,146 --> 00:21:32,636
So, to start with, every element


672
00:21:32,636 --> 00:21:34,886
of motion capture that you get


673
00:21:34,886 --> 00:21:37,406
in ARView, which as you all


674
00:21:37,406 --> 00:21:39,326
know, is the main UI element


675
00:21:39,396 --> 00:21:41,446
combining AR and RealityKit


676
00:21:41,446 --> 00:21:43,686
together, is powered by a new


677
00:21:43,686 --> 00:21:45,976
configuration called


678
00:21:45,976 --> 00:21:47,666
ARBodyTrackingConfiguration.


679
00:21:48,346 --> 00:21:50,376
And, once you enable this, you


680
00:21:50,376 --> 00:21:53,196
start adding anchors containing


681
00:21:53,196 --> 00:21:55,716
bodies, and this entire thing is


682
00:21:55,716 --> 00:21:57,526
encapsulated in a special type


683
00:21:57,526 --> 00:21:58,976
of anchor entity called


684
00:21:58,976 --> 00:22:00,156
bodyTrackedEntity.


685
00:22:00,826 --> 00:22:01,786
So, let me give you a brief


686
00:22:01,786 --> 00:22:02,986
description of what this


687
00:22:02,986 --> 00:22:05,036
bodyTrackedEntity actually is.


688
00:22:06,656 --> 00:22:08,616
A body tracked entity represents


689
00:22:08,686 --> 00:22:09,886
one single person.


690
00:22:10,226 --> 00:22:11,996
It contains the underlying


691
00:22:11,996 --> 00:22:14,186
skeleton and its position.


692
00:22:14,776 --> 00:22:17,646
It's tracked in real time and


693
00:22:17,646 --> 00:22:19,516
gets updated every frame.


694
00:22:20,136 --> 00:22:22,136
And, finally it combines the


695
00:22:22,136 --> 00:22:23,556
skeleton to a given [inaudible]


696
00:22:23,606 --> 00:22:26,076
mesh automatically and give you


697
00:22:26,076 --> 00:22:27,296
the complete character.


698
00:22:27,296 --> 00:22:30,826
Now, let's go to the code to


699
00:22:30,916 --> 00:22:32,386
animate a character quickly, and


700
00:22:32,386 --> 00:22:33,616
you'll see how easy it is.


701
00:22:33,856 --> 00:22:35,036
You just have to follow three


702
00:22:35,036 --> 00:22:35,616
steps.


703
00:22:36,586 --> 00:22:38,136
The first step is to load a


704
00:22:38,176 --> 00:22:38,676
character.


705
00:22:39,876 --> 00:22:42,396
To automatically load a tracked


706
00:22:42,396 --> 00:22:45,006
human, asynchronously, you just


707
00:22:45,006 --> 00:22:45,616
need to call


708
00:22:45,616 --> 00:22:48,336
entity.loadBodyTrackedAsync


709
00:22:48,366 --> 00:22:48,786
function.


710
00:22:49,956 --> 00:22:52,576
And, you can use .sync to either


711
00:22:52,576 --> 00:22:54,466
catch errors in the received


712
00:22:54,466 --> 00:22:55,866
completion block, or if


713
00:22:55,866 --> 00:22:57,306
everything works fine, you can


714
00:22:57,306 --> 00:22:58,406
get your character in the


715
00:22:58,406 --> 00:22:59,466
received value block.


716
00:22:59,786 --> 00:23:01,396
And, this character is of the


717
00:23:01,396 --> 00:23:02,976
type bodyTrackedEntity.


718
00:23:07,086 --> 00:23:10,166
We have a file called robot.usdz


719
00:23:10,166 --> 00:23:11,876
in our sample code bundle, and


720
00:23:11,906 --> 00:23:13,346
if you provided the file bot to


721
00:23:13,346 --> 00:23:15,646
this file, it will automatically


722
00:23:15,646 --> 00:23:17,226
add the skeleton to the robot


723
00:23:17,226 --> 00:23:18,496
mesh, and provide you with a


724
00:23:18,536 --> 00:23:18,996
character.


725
00:23:20,516 --> 00:23:22,216
So, moving on, the second step


726
00:23:22,216 --> 00:23:24,066
is to get the location where you


727
00:23:24,066 --> 00:23:25,436
would like to put your character


728
00:23:25,436 --> 00:23:25,656
on.


729
00:23:26,846 --> 00:23:28,916
For example, let's say, if you


730
00:23:28,916 --> 00:23:30,316
would like to put the character


731
00:23:30,316 --> 00:23:31,626
right on top of the person that


732
00:23:31,626 --> 00:23:33,446
you are tracking, you can get


733
00:23:33,446 --> 00:23:34,586
the location by using


734
00:23:34,586 --> 00:23:36,296
AnchorEntity with the argument


735
00:23:36,536 --> 00:23:37,186
.body.


736
00:23:37,776 --> 00:23:39,516
And, please note that this is


737
00:23:39,516 --> 00:23:41,096
just an example, and it can be


738
00:23:41,246 --> 00:23:43,626
any other location as well, be


739
00:23:43,626 --> 00:23:45,416
it on the floor, on a tabletop,


740
00:23:45,416 --> 00:23:46,486
or anywhere else.


741
00:23:46,726 --> 00:23:47,966
You just have to provide an


742
00:23:48,056 --> 00:23:49,856
anchor containing that location.


743
00:23:50,226 --> 00:23:52,546
And, the character will still


744
00:23:52,546 --> 00:23:53,406
mimic the person.


745
00:23:53,856 --> 00:23:57,566
And, finally, just add your


746
00:23:57,566 --> 00:23:59,026
character to the location, and


747
00:23:59,496 --> 00:23:59,866
voila.


748
00:24:00,416 --> 00:24:02,546
You can drive your character.


749
00:24:02,546 --> 00:24:02,976
It's that simple.


750
00:24:08,426 --> 00:24:10,286
So, you might be wondering that,


751
00:24:10,906 --> 00:24:12,586
how can you replace this robot


752
00:24:12,766 --> 00:24:14,216
with any other custom character


753
00:24:14,216 --> 00:24:14,696
that you like?


754
00:24:15,276 --> 00:24:18,836
Like I said before, we have a


755
00:24:18,836 --> 00:24:20,876
file called robot.usdz.


756
00:24:21,166 --> 00:24:23,656
And, this usdz file has an


757
00:24:23,656 --> 00:24:24,546
entire structure.


758
00:24:24,546 --> 00:24:26,286
And, if your custom model


759
00:24:26,556 --> 00:24:28,396
follows the same structure, then


760
00:24:28,396 --> 00:24:29,096
you can use it.


761
00:24:29,756 --> 00:24:32,096
And, just to let you know that


762
00:24:32,096 --> 00:24:33,426
the underlying skeleton that we


763
00:24:33,426 --> 00:24:35,566
provide is a very high-fidelity


764
00:24:35,566 --> 00:24:37,716
skeleton consisting of 91


765
00:24:37,716 --> 00:24:38,106
joints.


766
00:24:38,586 --> 00:24:41,466
And, for your information, here


767
00:24:41,466 --> 00:24:45,316
are all of them.


768
00:24:45,526 --> 00:24:46,296
It's a lot, right?


769
00:24:46,296 --> 00:24:48,096
And, these are just regular


770
00:24:48,096 --> 00:24:50,596
joints, contained in arms, legs,


771
00:24:50,596 --> 00:24:51,296
and so on.


772
00:24:51,896 --> 00:24:53,936
And, if your character follows


773
00:24:53,936 --> 00:24:55,536
this naming scheme, you can use


774
00:24:55,536 --> 00:24:56,696
it directly in RealityKit.


775
00:25:00,456 --> 00:25:02,746
That was a quick and easy way to


776
00:25:02,746 --> 00:25:04,196
load a tracked person and drive


777
00:25:04,226 --> 00:25:04,856
the character.


778
00:25:05,496 --> 00:25:07,206
Now, let's move on to low-level


779
00:25:07,206 --> 00:25:09,016
APIs for advanced access.


780
00:25:10,406 --> 00:25:11,826
Here we will provide you with


781
00:25:12,176 --> 00:25:13,776
access to each and every element


782
00:25:13,776 --> 00:25:14,616
of the skeleton.


783
00:25:15,176 --> 00:25:17,386
And, we interface it through a


784
00:25:17,386 --> 00:25:20,806
very easy-to-use API.


785
00:25:21,046 --> 00:25:22,286
You can enable all those


786
00:25:22,286 --> 00:25:23,646
advanced use cases that we


787
00:25:23,646 --> 00:25:24,986
discussed earlier, which


788
00:25:24,986 --> 00:25:26,736
requires either using the


789
00:25:26,736 --> 00:25:29,316
skeleton data for analysis, or


790
00:25:29,316 --> 00:25:30,946
as an input to your models.


791
00:25:33,656 --> 00:25:35,426
And, finally the skeleton that


792
00:25:35,426 --> 00:25:37,856
we provide is contained in a new


793
00:25:37,856 --> 00:25:38,996
type of anchor that we're


794
00:25:38,996 --> 00:25:40,416
introducing, called


795
00:25:40,416 --> 00:25:41,366
ARBodyAnchor.


796
00:25:42,566 --> 00:25:44,106
And, ARBodyAnchor is the


797
00:25:44,106 --> 00:25:45,716
starting point of the entire


798
00:25:45,716 --> 00:25:47,466
data structure that we provide.


799
00:25:48,896 --> 00:25:50,106
And, this is what the data


800
00:25:50,106 --> 00:25:50,876
structure looks like.


801
00:25:51,406 --> 00:25:54,536
We have ARBodyAnchor on the top,


802
00:25:54,536 --> 00:25:56,966
and it contains all the skeletal


803
00:25:57,796 --> 00:25:58,066
elements.


804
00:25:59,976 --> 00:26:01,716
So, let's just walk through this


805
00:26:01,716 --> 00:26:05,996
structure, and start at the top.


806
00:26:05,996 --> 00:26:07,576
ARBodyAnchor is just a regular


807
00:26:07,806 --> 00:26:08,076
anchor.


808
00:26:08,236 --> 00:26:09,786
It contains a geometry.


809
00:26:10,146 --> 00:26:11,836
And, in this case, the geometry


810
00:26:11,836 --> 00:26:13,316
itself is the skeleton.


811
00:26:13,316 --> 00:26:14,986
And, this is what the skeleton


812
00:26:14,986 --> 00:26:15,446
looks like.


813
00:26:15,986 --> 00:26:18,146
It consists of nodes and edges,


814
00:26:18,196 --> 00:26:19,676
just like any other geometry.


815
00:26:21,076 --> 00:26:22,946
It also contains a transform.


816
00:26:23,446 --> 00:26:25,436
And, this transform is just the


817
00:26:25,436 --> 00:26:26,616
location of the anchor in


818
00:26:26,966 --> 00:26:29,146
rotation and translation matrix


819
00:26:29,146 --> 00:26:29,416
form.


820
00:26:30,256 --> 00:26:31,656
Accessing the skeleton is what


821
00:26:31,656 --> 00:26:32,546
we're interested in here,


822
00:26:32,546 --> 00:26:33,036
mainly.


823
00:26:33,236 --> 00:26:34,426
So, let's just get right into


824
00:26:35,016 --> 00:26:35,086
it.


825
00:26:35,776 --> 00:26:37,436
What is this skeleton that we


826
00:26:37,436 --> 00:26:38,026
are showing you?


827
00:26:38,446 --> 00:26:40,966
It's a geometry, composed of


828
00:26:40,966 --> 00:26:42,556
nodes, which represent the


829
00:26:42,556 --> 00:26:44,836
joints, the green points and the


830
00:26:44,836 --> 00:26:46,156
yellow points that you see here.


831
00:26:46,156 --> 00:26:48,236
And, it contains edges, which


832
00:26:48,236 --> 00:26:49,766
represent the bones, the white


833
00:26:49,766 --> 00:26:50,756
lines that you see here.


834
00:26:51,206 --> 00:26:52,776
They show you how the joints are


835
00:26:52,826 --> 00:26:53,266
connected.


836
00:26:53,266 --> 00:26:56,616
And, once all the joints are


837
00:26:56,616 --> 00:26:58,536
connected, you form this entire


838
00:26:58,536 --> 00:26:59,086
geometry.


839
00:27:00,276 --> 00:27:02,016
We call this skeleton as


840
00:27:02,016 --> 00:27:04,056
ARSkeleton, and you can simply


841
00:27:04,056 --> 00:27:05,926
access it by using skeleton


842
00:27:05,966 --> 00:27:07,596
property of ARBodyAnchor.


843
00:27:11,476 --> 00:27:13,266
The root node of this skeleton,


844
00:27:13,556 --> 00:27:15,096
the topmost point in this


845
00:27:15,176 --> 00:27:16,836
geometry in the geometry


846
00:27:16,836 --> 00:27:18,706
hierarchy, is the hip joint,


847
00:27:18,926 --> 00:27:19,816
which you can see here.


848
00:27:21,086 --> 00:27:22,506
So, let's move on and have a


849
00:27:22,506 --> 00:27:23,496
look at its structural


850
00:27:23,496 --> 00:27:24,076
definition.


851
00:27:25,116 --> 00:27:26,606
Definition, what we call here,


852
00:27:26,606 --> 00:27:27,606
is just a property of the


853
00:27:27,606 --> 00:27:29,326
skeleton, containing two


854
00:27:29,326 --> 00:27:29,926
components.


855
00:27:30,586 --> 00:27:32,286
The names of all the joints


856
00:27:32,326 --> 00:27:33,406
present in the skeleton, and the


857
00:27:33,706 --> 00:27:35,286
connections between them, which


858
00:27:35,286 --> 00:27:36,526
show you how to connect those


859
00:27:36,526 --> 00:27:37,226
joints together.


860
00:27:37,986 --> 00:27:38,986
So, let's just have a look at


861
00:27:39,066 --> 00:27:40,186
both of these properties.


862
00:27:42,016 --> 00:27:43,676
Here, we are labeling some of


863
00:27:43,676 --> 00:27:45,496
the joints in the skeleton, and


864
00:27:45,546 --> 00:27:46,786
as you can see, that these


865
00:27:46,786 --> 00:27:48,046
joints have semantically


866
00:27:48,046 --> 00:27:49,966
meaningful names, like left


867
00:27:50,126 --> 00:27:51,496
shoulder, right shoulder, left


868
00:27:51,496 --> 00:27:53,306
hand, right hand, and so on.


869
00:27:53,536 --> 00:27:54,546
Similar to people.


870
00:27:55,566 --> 00:27:57,136
Here I would like to point out


871
00:27:57,136 --> 00:27:58,486
that the green points are the


872
00:27:58,486 --> 00:28:00,446
ones that we control and are


873
00:28:00,536 --> 00:28:02,116
estimated from the person that


874
00:28:02,116 --> 00:28:03,296
you're looking at, and the


875
00:28:03,296 --> 00:28:05,056
yellow ones are not tracked.


876
00:28:05,426 --> 00:28:06,826
They just follow the motion of


877
00:28:06,826 --> 00:28:08,986
the closest green parent.


878
00:28:10,816 --> 00:28:12,876
Zooming in, focus on the right


879
00:28:12,876 --> 00:28:14,896
arm, and consider these three


880
00:28:14,896 --> 00:28:15,356
joints.


881
00:28:15,496 --> 00:28:17,106
Right hand, right elbow and


882
00:28:17,106 --> 00:28:17,736
right shoulder.


883
00:28:18,336 --> 00:28:19,966
They follow a parent-child


884
00:28:19,996 --> 00:28:20,606
relationship.


885
00:28:21,116 --> 00:28:23,136
So, your hand is a child of


886
00:28:23,136 --> 00:28:23,706
elbow.


887
00:28:23,706 --> 00:28:24,906
And elbow is a child of


888
00:28:24,936 --> 00:28:25,446
shoulder.


889
00:28:25,946 --> 00:28:27,666
And, this continues for rest of


890
00:28:27,666 --> 00:28:29,416
the skeleton as well, thus


891
00:28:29,476 --> 00:28:30,436
giving you the complete


892
00:28:30,496 --> 00:28:30,816
hierarchy.


893
00:28:36,336 --> 00:28:38,256
We know now what all the joints


894
00:28:38,256 --> 00:28:39,626
are called, and how to connect


895
00:28:39,626 --> 00:28:39,896
them.


896
00:28:40,306 --> 00:28:41,776
But, how do we get their


897
00:28:41,776 --> 00:28:42,296
locations?


898
00:28:43,036 --> 00:28:44,306
We've provided two ways to


899
00:28:44,306 --> 00:28:45,666
access the locations of all the


900
00:28:45,666 --> 00:28:45,956
joints.


901
00:28:47,256 --> 00:28:49,226
The first one is relative to its


902
00:28:49,326 --> 00:28:49,746
parent.


903
00:28:50,436 --> 00:28:51,856
If you want the location of your


904
00:28:51,856 --> 00:28:53,036
right hand relative to the


905
00:28:53,036 --> 00:28:54,696
elbow, you can have that


906
00:28:54,696 --> 00:28:55,866
transform by calling


907
00:28:55,866 --> 00:28:57,556
localTransform function, and


908
00:28:57,596 --> 00:28:59,336
provide it with the argument


909
00:28:59,336 --> 00:28:59,776
.rightHand.


910
00:29:00,426 --> 00:29:02,986
And, if you want to transform


911
00:29:02,986 --> 00:29:04,546
relative to the root, which in


912
00:29:04,546 --> 00:29:06,586
our case is the hip joint, you


913
00:29:06,586 --> 00:29:07,806
can call modelTransform


914
00:29:07,806 --> 00:29:09,986
function, and provide, again,


915
00:29:09,986 --> 00:29:12,326
same argument, .rightHand.


916
00:29:13,236 --> 00:29:15,496
Now, if you don't want to access


917
00:29:15,576 --> 00:29:17,066
the transforms of all the joints


918
00:29:17,146 --> 00:29:18,876
individually, but instead you


919
00:29:18,876 --> 00:29:20,056
want a list containing


920
00:29:20,056 --> 00:29:21,916
transforms of all the joints,


921
00:29:22,376 --> 00:29:24,576
you can also do that by simply


922
00:29:24,576 --> 00:29:26,466
using localTransforms property


923
00:29:26,466 --> 00:29:28,356
and modelTransforms property.


924
00:29:28,686 --> 00:29:31,466
This will give you a list of


925
00:29:31,466 --> 00:29:33,886
transforms of all the joints.


926
00:29:34,456 --> 00:29:36,306
And, if you want that relative


927
00:29:36,306 --> 00:29:37,916
to the parent, you can use


928
00:29:37,976 --> 00:29:39,846
localTransforms, and if you want


929
00:29:39,846 --> 00:29:41,096
relative to the root, you can


930
00:29:41,096 --> 00:29:42,126
use modelTransforms.


931
00:29:42,266 --> 00:29:45,526
So, now that we had a good look


932
00:29:45,526 --> 00:29:47,876
over the skeleton, let's go to


933
00:29:47,876 --> 00:29:49,486
the code and learn how to use


934
00:29:49,486 --> 00:29:50,696
each and every element.


935
00:29:51,176 --> 00:29:54,456
You start by iterating over all


936
00:29:54,456 --> 00:29:55,456
the anchors in the scene.


937
00:29:56,146 --> 00:29:57,226
And, just look for the


938
00:29:57,226 --> 00:29:57,896
bodyAnchor.


939
00:29:59,086 --> 00:30:00,446
Once you have the bodyAnchor,


940
00:30:00,726 --> 00:30:02,016
you might want to know where


941
00:30:02,016 --> 00:30:03,486
that bodyAnchor is located in


942
00:30:03,486 --> 00:30:03,936
the world.


943
00:30:05,266 --> 00:30:06,736
And, you can use the transform


944
00:30:06,736 --> 00:30:08,046
property of the bodyAnchor to


945
00:30:08,046 --> 00:30:10,026
get that.


946
00:30:10,026 --> 00:30:12,106
And, since in our geometry, hip


947
00:30:12,106 --> 00:30:14,586
is the root, so the


948
00:30:14,586 --> 00:30:16,156
bodyAnchor.transform will give


949
00:30:16,156 --> 00:30:17,516
you the world position of the


950
00:30:17,516 --> 00:30:18,106
hip joint.


951
00:30:19,446 --> 00:30:20,476
Once you have the transform of


952
00:30:20,476 --> 00:30:21,796
the anchor, you might need to


953
00:30:21,796 --> 00:30:22,886
access the geometry of the


954
00:30:22,946 --> 00:30:24,486
anchor, and you can do that by


955
00:30:24,486 --> 00:30:26,556
using skeleton property of the


956
00:30:26,926 --> 00:30:27,236
anchor.


957
00:30:27,956 --> 00:30:30,306
Once you have this geometry, you


958
00:30:30,306 --> 00:30:31,276
need all the joints.


959
00:30:31,386 --> 00:30:32,316
You need all the nodes.


960
00:30:32,676 --> 00:30:34,626
And, to get a list of transforms


961
00:30:34,626 --> 00:30:36,596
of all the joints, that is the


962
00:30:36,596 --> 00:30:37,906
list of locations of all the


963
00:30:37,906 --> 00:30:39,506
joints, you can simply use


964
00:30:39,586 --> 00:30:41,596
jointModelTransforms property of


965
00:30:41,636 --> 00:30:42,166
the skeleton.


966
00:30:43,346 --> 00:30:45,076
Once you have this list, you can


967
00:30:45,076 --> 00:30:46,706
iterate over this list and


968
00:30:46,746 --> 00:30:48,846
access transform of every


969
00:30:48,846 --> 00:30:49,346
element.


970
00:30:49,476 --> 00:30:51,166
Or, every joint, in this case.


971
00:30:52,186 --> 00:30:53,376
So, iterating over all the


972
00:30:53,376 --> 00:30:55,916
joints, you can access the


973
00:30:55,916 --> 00:30:58,666
parentIndex of each joint by


974
00:30:58,666 --> 00:31:00,526
using the parentIndices property


975
00:31:00,796 --> 00:31:01,706
in the definition.


976
00:31:02,316 --> 00:31:03,986
And, just check if the parent is


977
00:31:03,986 --> 00:31:04,496
not the root.


978
00:31:04,496 --> 00:31:06,076
Because the root is the topmost


979
00:31:06,076 --> 00:31:07,086
point of the hierarchy, so it


980
00:31:07,086 --> 00:31:07,956
doesn't have a parent.


981
00:31:08,726 --> 00:31:09,926
So, if the parent is not the


982
00:31:09,926 --> 00:31:13,246
root, you can access the


983
00:31:13,246 --> 00:31:15,446
transform of the parent by using


984
00:31:15,446 --> 00:31:17,176
the same jointTransforms list,


985
00:31:17,676 --> 00:31:19,386
but just index it with the


986
00:31:19,626 --> 00:31:20,436
parentIndex.


987
00:31:21,076 --> 00:31:21,726
And, that's it.


988
00:31:22,316 --> 00:31:24,726
This gives you every child pair


989
00:31:25,256 --> 00:31:27,446
in the entire hierarchy, and


990
00:31:27,576 --> 00:31:29,876
once you have every child-parent


991
00:31:29,876 --> 00:31:31,196
pair in the hierarchy, you have


992
00:31:31,196 --> 00:31:32,176
the entire hierarchy of the


993
00:31:32,176 --> 00:31:32,796
skeleton.


994
00:31:33,576 --> 00:31:35,826
And, you can now use it however


995
00:31:35,826 --> 00:31:36,256
you want.


996
00:31:36,906 --> 00:31:38,326
So, let's just run this code.


997
00:31:38,416 --> 00:31:40,486
Let's just take this, and simply


998
00:31:40,486 --> 00:31:42,486
draw the skeleton, and see what


999
00:31:42,486 --> 00:31:43,766
it looks like.


1000
00:31:44,636 --> 00:31:46,206
This is what it looks like.


1001
00:31:46,676 --> 00:31:48,366
All we did was, we took the


1002
00:31:48,366 --> 00:31:49,306
entire hierarchy.


1003
00:31:49,336 --> 00:31:51,336
We took all parent-child points,


1004
00:31:51,336 --> 00:31:52,776
and we just drew the skeleton,


1005
00:31:52,876 --> 00:31:53,316
that's it.


1006
00:31:53,956 --> 00:31:55,046
And, it starts to mimic the


1007
00:31:55,046 --> 00:31:56,106
person automatically.


1008
00:31:56,806 --> 00:31:58,096
And, this is the most basic


1009
00:31:58,096 --> 00:31:59,446
thing that you can do, because


1010
00:31:59,946 --> 00:32:00,996
once you have this entire


1011
00:32:00,996 --> 00:32:02,846
skeleton hierarchy, you can use


1012
00:32:02,846 --> 00:32:04,626
it for many other use cases that


1013
00:32:04,626 --> 00:32:05,536
we discussed earlier.


1014
00:32:06,366 --> 00:32:07,856
This technology is at your


1015
00:32:07,856 --> 00:32:09,286
disposal, wherever your


1016
00:32:09,286 --> 00:32:10,366
imagination takes you.


1017
00:32:14,356 --> 00:32:16,426
So far, we've only talked about


1018
00:32:16,796 --> 00:32:19,016
3D objects in world space.


1019
00:32:19,826 --> 00:32:21,876
But, in case you want 2D


1020
00:32:21,876 --> 00:32:23,466
versions of the skeleton in


1021
00:32:23,466 --> 00:32:25,716
image space, we have provided an


1022
00:32:25,716 --> 00:32:27,176
API for that as well.


1023
00:32:28,136 --> 00:32:29,386
Here, we provide you with


1024
00:32:29,606 --> 00:32:31,516
detailed access to each and


1025
00:32:31,516 --> 00:32:35,156
every element in 2D space.


1026
00:32:35,366 --> 00:32:37,276
We also provide all the skeleton


1027
00:32:37,276 --> 00:32:39,176
joints as normalized image


1028
00:32:39,176 --> 00:32:39,946
coordinates.


1029
00:32:40,986 --> 00:32:43,106
We have a very easy to use API


1030
00:32:43,326 --> 00:32:44,096
for this as well.


1031
00:32:44,096 --> 00:32:47,456
And, you can use it for semantic


1032
00:32:47,456 --> 00:32:49,136
image analysis, or for building


1033
00:32:49,136 --> 00:32:50,876
editing tools both for images


1034
00:32:50,876 --> 00:32:51,596
and videos.


1035
00:32:52,276 --> 00:32:55,356
And, finally this entire


1036
00:32:55,356 --> 00:32:56,976
structure is interfaced through


1037
00:32:56,976 --> 00:32:59,476
an object called ARBody2D.


1038
00:33:00,046 --> 00:33:02,896
And, this is what ARBody2D


1039
00:33:02,896 --> 00:33:04,216
object looks like when


1040
00:33:04,216 --> 00:33:04,906
visualized.


1041
00:33:05,686 --> 00:33:08,096
ARBody2D object contains the


1042
00:33:08,096 --> 00:33:09,706
entire skeletal structure.


1043
00:33:13,556 --> 00:33:15,086
And, this is what the structure


1044
00:33:15,086 --> 00:33:15,486
looks like.


1045
00:33:16,126 --> 00:33:17,976
So, you have the object itself


1046
00:33:17,976 --> 00:33:18,486
on the top.


1047
00:33:18,486 --> 00:33:20,216
And then, again, similar to its


1048
00:33:20,216 --> 00:33:22,026
3D counterpart, all the skeletal


1049
00:33:22,026 --> 00:33:23,346
elements under that object.


1050
00:33:24,226 --> 00:33:25,426
So, let's walk through this


1051
00:33:25,426 --> 00:33:26,726
structure, and just learn about


1052
00:33:26,726 --> 00:33:27,766
these elements quickly.


1053
00:33:28,246 --> 00:33:30,906
Starting with the ARBody2D


1054
00:33:30,906 --> 00:33:34,016
object on top, there are two


1055
00:33:34,016 --> 00:33:35,546
ways that you can access this


1056
00:33:35,546 --> 00:33:36,006
object.


1057
00:33:36,876 --> 00:33:38,516
If you're already working in 2D


1058
00:33:38,516 --> 00:33:40,016
space, the default way is to


1059
00:33:40,016 --> 00:33:41,626
access it via ARFrame.


1060
00:33:41,826 --> 00:33:43,586
And, you can simply do that by


1061
00:33:43,586 --> 00:33:46,296
using detectedBody property of


1062
00:33:46,336 --> 00:33:47,086
the ARFrame.


1063
00:33:48,486 --> 00:33:50,066
And, here the person is an


1064
00:33:50,066 --> 00:33:51,996
instance of ARBody2D object.


1065
00:33:53,476 --> 00:33:55,366
For your convenience, if you're


1066
00:33:55,366 --> 00:33:57,626
already working in 3D space, if


1067
00:33:57,626 --> 00:33:59,046
you're already working with a 3D


1068
00:33:59,046 --> 00:34:01,136
skeleton, and for some reason,


1069
00:34:01,136 --> 00:34:02,856
you want the corresponding 2D


1070
00:34:02,856 --> 00:34:04,846
skeleton in image space as well,


1071
00:34:05,766 --> 00:34:07,676
we have provided a direct way to


1072
00:34:07,676 --> 00:34:09,505
access it by simply using


1073
00:34:09,716 --> 00:34:11,536
referenceBody property of


1074
00:34:11,755 --> 00:34:12,576
ARBodyAnchor.


1075
00:34:13,746 --> 00:34:15,596
And, this gives you the ARBody2D


1076
00:34:15,596 --> 00:34:16,476
object as well.


1077
00:34:16,886 --> 00:34:20,446
After accessing the ARBody2D


1078
00:34:20,446 --> 00:34:22,516
object, we can extract skeleton


1079
00:34:22,516 --> 00:34:24,315
from it by simply using skeleton


1080
00:34:24,315 --> 00:34:27,206
property, and this is the


1081
00:34:27,206 --> 00:34:28,755
visualization of that skeleton.


1082
00:34:30,076 --> 00:34:31,716
So, like I said, this skeleton


1083
00:34:31,716 --> 00:34:33,525
is present in normalized image


1084
00:34:33,525 --> 00:34:34,056
space.


1085
00:34:34,606 --> 00:34:36,876
So, if this is your image grid,


1086
00:34:36,876 --> 00:34:39,005
and the top left is 0,0 and


1087
00:34:39,005 --> 00:34:40,956
bottom right is 1,1, all the


1088
00:34:40,956 --> 00:34:43,436
points on this diagram are in


1089
00:34:43,436 --> 00:34:44,906
the range of 0 and 1.


1090
00:34:44,985 --> 00:34:46,976
Both in x and y direction.


1091
00:34:49,656 --> 00:34:51,255
The green points that you see


1092
00:34:51,255 --> 00:34:52,735
here are called landmarks.


1093
00:34:53,306 --> 00:34:54,476
Note that here we don't call


1094
00:34:54,476 --> 00:34:55,985
them joints, although they do


1095
00:34:55,985 --> 00:34:56,826
represent joints.


1096
00:34:56,826 --> 00:34:58,046
We just call them landmarks,


1097
00:34:58,046 --> 00:35:00,196
because they are pixel locations


1098
00:35:00,716 --> 00:35:02,866
on image.


1099
00:35:03,106 --> 00:35:05,116
Similar to the 3D version, it


1100
00:35:05,116 --> 00:35:06,636
contains an object called


1101
00:35:06,636 --> 00:35:08,456
definition, describing what the


1102
00:35:08,456 --> 00:35:09,906
landmarks are called in the


1103
00:35:09,906 --> 00:35:11,606
skeleton, and how to connect


1104
00:35:11,646 --> 00:35:12,336
those landmarks.


1105
00:35:12,936 --> 00:35:16,096
In this skeleton there are 16


1106
00:35:16,096 --> 00:35:18,366
joints, and similar to 3D, they


1107
00:35:18,366 --> 00:35:20,316
have semantically meaningful


1108
00:35:20,316 --> 00:35:22,436
names like left shoulder, right


1109
00:35:22,436 --> 00:35:24,036
shoulder, left hand, right hand,


1110
00:35:24,036 --> 00:35:24,986
and so on.


1111
00:35:25,206 --> 00:35:27,166
The root node is still the hip


1112
00:35:27,166 --> 00:35:27,706
joint here.


1113
00:35:27,876 --> 00:35:31,796
Again, similar to the 3D.


1114
00:35:31,956 --> 00:35:34,536
So, focusing on the right arm,


1115
00:35:34,716 --> 00:35:36,516
we can see that the hand is a


1116
00:35:36,516 --> 00:35:38,116
child of right elbow, and elbow


1117
00:35:38,116 --> 00:35:39,416
is a child of right shoulder.


1118
00:35:39,416 --> 00:35:41,866
And, this is, again, similar


1119
00:35:41,866 --> 00:35:43,266
parent-child relationship that


1120
00:35:43,266 --> 00:35:45,246
you saw in the 3D version as


1121
00:35:45,246 --> 00:35:45,426
well.


1122
00:35:46,096 --> 00:35:47,976
And, the similar hierarchy is


1123
00:35:48,036 --> 00:35:48,556
formed here.


1124
00:35:49,146 --> 00:35:52,606
So, having all that information


1125
00:35:52,606 --> 00:35:54,186
with us, let's quickly walk


1126
00:35:54,186 --> 00:35:55,736
through this structure via code


1127
00:35:55,736 --> 00:35:56,056
now.


1128
00:35:57,106 --> 00:35:59,266
We start with accessing ARBody2D


1129
00:35:59,266 --> 00:35:59,706
object.


1130
00:36:01,646 --> 00:36:03,726
So, once you have your ARFRame,


1131
00:36:03,986 --> 00:36:05,626
you can simply use detectedBody


1132
00:36:05,626 --> 00:36:07,656
property to get ARBody2D object.


1133
00:36:07,986 --> 00:36:10,296
And now, once you have ARBody2D


1134
00:36:10,296 --> 00:36:11,686
object, you can access the


1135
00:36:11,686 --> 00:36:13,496
entire skeletal structure from


1136
00:36:13,496 --> 00:36:14,936
underneath it.


1137
00:36:15,096 --> 00:36:16,546
You can extract the geometry by


1138
00:36:16,546 --> 00:36:18,016
using person.skeleton.


1139
00:36:18,016 --> 00:36:20,766
In this case, person refers to


1140
00:36:20,766 --> 00:36:22,056
the ARBody2D object.


1141
00:36:22,276 --> 00:36:23,976
And, the definition of the


1142
00:36:23,976 --> 00:36:26,146
skeleton, which again, comprises


1143
00:36:26,146 --> 00:36:28,026
of the names of all the joints,


1144
00:36:28,026 --> 00:36:29,876
and the information of how to


1145
00:36:29,876 --> 00:36:31,786
connect those joints, is present


1146
00:36:32,216 --> 00:36:33,696
in the definition, which you can


1147
00:36:33,696 --> 00:36:35,426
access it by using definition


1148
00:36:35,426 --> 00:36:36,786
property of the skeleton.


1149
00:36:37,276 --> 00:36:39,896
Once you have this information,


1150
00:36:40,236 --> 00:36:42,146
you might need to know where


1151
00:36:42,146 --> 00:36:43,536
those landmarks are located.


1152
00:36:43,776 --> 00:36:45,456
And, similar to the 3D version,


1153
00:36:45,726 --> 00:36:46,666
we have a property called


1154
00:36:46,666 --> 00:36:48,706
jointLandmarks, which gives you


1155
00:36:48,706 --> 00:36:51,256
a list of locations of all those


1156
00:36:51,256 --> 00:36:52,576
green points that you see here.


1157
00:36:53,566 --> 00:36:54,716
But, please note that, in this


1158
00:36:54,716 --> 00:36:55,976
case, the green points are in


1159
00:36:55,976 --> 00:36:57,716
2D, so they're in image space.


1160
00:36:57,716 --> 00:36:59,836
They're normalized pixel


1161
00:36:59,836 --> 00:37:00,616
coordinates.


1162
00:37:01,056 --> 00:37:02,876
And, once you have that list,


1163
00:37:03,006 --> 00:37:04,546
you can iterate over all these


1164
00:37:04,546 --> 00:37:05,046
landmarks.


1165
00:37:05,726 --> 00:37:07,116
And, for each landmark, you can


1166
00:37:07,116 --> 00:37:09,356
access its parent by calling


1167
00:37:09,516 --> 00:37:11,366
parentIndices property in the


1168
00:37:11,366 --> 00:37:11,946
definition.


1169
00:37:12,426 --> 00:37:13,616
And, again, just check if the


1170
00:37:13,966 --> 00:37:15,716
parent is not the root, because


1171
00:37:15,886 --> 00:37:17,146
the root is the topmost point of


1172
00:37:17,146 --> 00:37:17,696
the hierarchy.


1173
00:37:18,236 --> 00:37:19,486
And, if the parent is not the


1174
00:37:19,486 --> 00:37:21,286
root, you can access its


1175
00:37:21,286 --> 00:37:23,106
transform by using


1176
00:37:23,106 --> 00:37:24,506
jointLandmarks list, and


1177
00:37:24,506 --> 00:37:25,496
indexing it with the


1178
00:37:25,496 --> 00:37:26,266
parentIndex.


1179
00:37:27,346 --> 00:37:29,916
And, again, in this way, you


1180
00:37:29,916 --> 00:37:32,476
have the transforms of every


1181
00:37:32,476 --> 00:37:35,016
child-parent pair in the


1182
00:37:35,016 --> 00:37:36,016
skeletal hierarchy.


1183
00:37:36,256 --> 00:37:37,596
And, you can, again, use it


1184
00:37:37,646 --> 00:37:39,206
however you want, and you can


1185
00:37:39,206 --> 00:37:40,886
continue from here and build


1186
00:37:40,886 --> 00:37:43,876
your own ideas, which brings us


1187
00:37:43,906 --> 00:37:44,686
to the conclusion.


1188
00:37:45,246 --> 00:37:49,636
We've introduced motion capture


1189
00:37:49,886 --> 00:37:51,296
in AR this year.


1190
00:37:51,826 --> 00:37:54,766
We've provided access to tracked


1191
00:37:55,036 --> 00:37:56,586
person in real time.


1192
00:37:57,316 --> 00:38:01,896
We have provided both 3D and 2D


1193
00:38:01,896 --> 00:38:04,536
skeletons for you guys to use


1194
00:38:04,536 --> 00:38:07,526
it, and that is how we interface


1195
00:38:07,526 --> 00:38:10,446
the pose of the person.


1196
00:38:10,606 --> 00:38:11,956
We've enabled character


1197
00:38:12,016 --> 00:38:13,846
animation out of the box.


1198
00:38:13,846 --> 00:38:16,216
And, it runs seamlessly in


1199
00:38:16,216 --> 00:38:16,836
RealityKit.


1200
00:38:17,356 --> 00:38:20,926
We have a RealityKit API that we


1201
00:38:20,926 --> 00:38:22,766
discussed earlier for quickly


1202
00:38:22,826 --> 00:38:25,086
animating a character, and like


1203
00:38:25,086 --> 00:38:26,376
I mentioned earlier as well, you


1204
00:38:26,376 --> 00:38:27,466
can use your own custom


1205
00:38:27,466 --> 00:38:29,516
characters as well, as long as


1206
00:38:29,556 --> 00:38:31,246
it's based on the structure of


1207
00:38:31,246 --> 00:38:32,296
our provided example.


1208
00:38:32,616 --> 00:38:36,146
And, we've provided an ARKit API


1209
00:38:36,476 --> 00:38:38,276
for all the advanced use cases


1210
00:38:38,276 --> 00:38:39,886
that you might think of, like


1211
00:38:39,916 --> 00:38:42,096
recognition tasks, or analysis


1212
00:38:42,136 --> 00:38:42,686
tasks.


1213
00:38:46,046 --> 00:38:47,436
And, that brings us to the end


1214
00:38:47,436 --> 00:38:48,066
of the session.


1215
00:38:48,626 --> 00:38:50,456
We presented two new features in


1216
00:38:50,456 --> 00:38:52,386
this session, person occlusion,


1217
00:38:52,386 --> 00:38:53,446
and motion capture.


1218
00:38:53,486 --> 00:38:55,436
And, we've provided APIs for


1219
00:38:55,706 --> 00:38:57,616
both of these features.


1220
00:38:59,076 --> 00:39:00,726
For more information from


1221
00:39:00,726 --> 00:39:02,426
today's session, please visit


1222
00:39:02,426 --> 00:39:04,296
our website, and please feel


1223
00:39:04,296 --> 00:39:05,756
free to download the sample code


1224
00:39:05,886 --> 00:39:06,436
and use it.


1225
00:39:07,306 --> 00:39:08,946
We will be in the lab tomorrow,


1226
00:39:08,946 --> 00:39:10,466
so come visit us to get all your


1227
00:39:10,466 --> 00:39:11,366
questions answered.


1228
00:39:14,016 --> 00:39:15,126
[ Applause ]


1229
00:39:15,126 --> 00:39:15,976
Thank you.


1230
00:39:16,508 --> 00:39:18,508
[ Applause ]

