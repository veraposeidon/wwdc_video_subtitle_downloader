2
00:00:00,501 --> 00:00:08,509 line:-1
♪ ♪


3
00:00:09.643 --> 00:00:13.647 line:-2 align:center
Ken Greenebaum: Hi everyone!
Welcome to WWDC 2022.


4
00:00:13,680 --> 00:00:15,082 line:-1
My name is Ken Greenebaum,


5
00:00:15.115 --> 00:00:18.352 line:-2 align:center
and I'm with the Color and Display
Technologies team at Apple.


6
00:00:18.385 --> 00:00:21.388 line:-2 align:center
We are thrilled to have three EDR talks
this year.


7
00:00:21.421 --> 00:00:25.559 line:-2 align:center
Hope you've had an opportunity
to watch "Explore EDR on iOS,"


8
00:00:25.592 --> 00:00:29.263 line:-2 align:center
where we announced
EDR API support for iOS,


9
00:00:29.296 --> 00:00:33.901 line:-2 align:center
as well as "Display EDR content
with Core Image, Metal, and SwiftUI."


10
00:00:33.934 --> 00:00:37.838 line:-2 align:center
Some of you may have also watched
my EDR talk last year,


11
00:00:37.871 --> 00:00:40.240 line:-2 align:center
where we demonstrated
how to use AVPlayer


12
00:00:40,274 --> 00:00:43,177 line:-1
to play back HDR video, using EDR.


13
00:00:44,178 --> 00:00:46,246 line:-1
In this talk we're gonna go deeper,


14
00:00:46.280 --> 00:00:49.550 line:-2 align:center
and explore how to use
Core Media interfaces to provide,


15
00:00:49,583 --> 00:00:51,785 line:-1
not only EDR playback,


16
00:00:51,818 --> 00:00:54,821 line:-2
but also how to decode
and playback HDR video,


17
00:00:54.855 --> 00:00:57.591 line:-1 align:center
into your own EDR layers or views.


18
00:00:59,326 --> 00:01:02,396 line:-2
Then we'll continue beyond
simply playing back content,


19
00:01:02.429 --> 00:01:05.732 line:-2 align:center
to show how to access the decoded
video frames in real time,


20
00:01:05,766 --> 00:01:08,101 line:-1
via Core Video's display link,


21
00:01:08.135 --> 00:01:11.271 line:-2 align:center
send those frames to CoreImage Filters,
or a Metal Shader,


22
00:01:11,305 --> 00:01:13,473 line:-1
to add color management, visual effects,


23
00:01:13.507 --> 00:01:15.909 line:-1 align:center
or apply other signal processing,


24
00:01:15.943 --> 00:01:20.247 line:-2 align:center
and finally, plumb
the resulting frames to Metal to render.


25
00:01:20.280 --> 00:01:24.751 line:-2 align:center
We're going to start by reviewing
the EDR compatible video media frameworks,


26
00:01:24,785 --> 00:01:28,522 line:-2
to help you decide which best matches
your application's requirements.


27
00:01:30.057 --> 00:01:32.226 line:-2 align:center
Next we will briefly discuss
the high level AVKit


28
00:01:32,259 --> 00:01:35,028 line:-1
and AVFoundation frameworks,


29
00:01:35,062 --> 00:01:37,764 line:-2
that can do all of the work
of playing HDR video,


30
00:01:37,798 --> 00:01:40,701 line:-2
if your application
requires straight forward playback.


31
00:01:42,302 --> 00:01:44,872 line:-2
And finally,
we'll discuss best practices


32
00:01:44.905 --> 00:01:48.175 line:-2 align:center
for using decoded video frames,
with Core Video and Metal,


33
00:01:48,208 --> 00:01:51,979 line:-2
in your EDR playback, editing,
or image processing engine.


34
00:01:54,781 --> 00:01:58,752 line:-2
Let's begin by taking a quick survey
of Apple's video frameworks;


35
00:01:58.785 --> 00:02:01.355 line:-2 align:center
Starting with
the highest level interfaces;


36
00:02:01,388 --> 00:02:03,323 line:-1
which are the easiest to use;


37
00:02:03.357 --> 00:02:06.827 line:-2 align:center
and continuing to lower level frameworks
that offer more opportunities,


38
00:02:06,860 --> 00:02:10,364 line:-2
at the expense of adding
complexity to your code.


39
00:02:10.397 --> 00:02:13.467 line:-2 align:center
It is best to use
the highest level framework possible


40
00:02:13.500 --> 00:02:17.271 line:-2 align:center
to take advantage of the optimizations
provided automatically for you.


41
00:02:17.304 --> 00:02:20.440 line:-2 align:center
This will get us ready to dive into
the body of the talk,


42
00:02:20.474 --> 00:02:23.177 line:-2 align:center
where we will be exploring
a number of scenarios,


43
00:02:23.210 --> 00:02:24.945 line:-1 align:center
from simple EDR playback


44
00:02:24.978 --> 00:02:27.981 line:-2 align:center
to more sophisticated plumbing
of decoded video frames


45
00:02:28,015 --> 00:02:31,652 line:-2
to CoreImage or Metal
for real time processing.


46
00:02:31.685 --> 00:02:34.154 line:-1 align:center
At the highest level, there is AVKit.


47
00:02:34.188 --> 00:02:38.292 line:-2 align:center
With AVKit you can create
user interfaces for media playback;


48
00:02:38.325 --> 00:02:41.562 line:-2 align:center
complete with transport controls,
chapter navigation,


49
00:02:41,595 --> 00:02:43,030 line:-1
Picture in Picture support,


50
00:02:43.063 --> 00:02:45.632 line:-2 align:center
and display of subtitles
and closed captions.


51
00:02:45.666 --> 00:02:49.136 line:-1 align:center
AVKit can playback HDR content as EDR,


52
00:02:49.169 --> 00:02:52.840 line:-2 align:center
as we will demonstrate
using AVPlayerViewController.


53
00:02:52.873 --> 00:02:57.377 line:-2 align:center
However, if your application requires
further processing of video frames,


54
00:02:57,411 --> 00:02:59,179 line:-1
you will have to use a media framework


55
00:02:59,213 --> 00:03:01,815 line:-2
that can give you more control
over your pipeline.


56
00:03:01.849 --> 00:03:04.618 line:-1 align:center
Next there is AVFoundation.


57
00:03:04.651 --> 00:03:07.688 line:-2 align:center
AVFoundation is
the full-featured framework


58
00:03:07,721 --> 00:03:12,860 line:-2
for working with time based audio visual
media on Apple Platforms.


59
00:03:12.893 --> 00:03:16.096 line:-2 align:center
Using AVFoundation,
you can easily play, create,


60
00:03:16.129 --> 00:03:19.032 line:-2 align:center
and edit QuickTime movies
and MPEG 4 files,


61
00:03:19,066 --> 00:03:20,634 line:-1
play HLS streams,


62
00:03:20.667 --> 00:03:24.271 line:-2 align:center
and build powerful media functionality
into your apps.


63
00:03:24.304 --> 00:03:26.306 line:-1 align:center
We'll be exploring use of AVPlayer


64
00:03:26,340 --> 00:03:30,577 line:-2
and the related AVPlayerLayer interface
in this talk.


65
00:03:30.611 --> 00:03:35.182 line:-2 align:center
Core Video is a framework that provides
a pipeline model for digital video.


66
00:03:35.215 --> 00:03:37.417 line:-2 align:center
It simplifies the way
you work with videos


67
00:03:37,451 --> 00:03:40,220 line:-2
by partitioning the process
into discrete steps.


68
00:03:40,254 --> 00:03:43,790 line:-2
Core Video also makes it easier
for you to access and manipulate


69
00:03:43,824 --> 00:03:46,527 line:-2
individual frames,
without having to worry about


70
00:03:46,560 --> 00:03:48,262 line:-1
translating between data types


71
00:03:48,295 --> 00:03:51,465 line:-1
or worrying about display synchronization.


72
00:03:51.498 --> 00:03:53.534 line:-2 align:center
We'll be demonstrating
use of DisplayLink,


73
00:03:53,567 --> 00:03:56,036 line:-1
and CVPixelBuffer's with Core Image.


74
00:03:56.069 --> 00:03:59.239 line:-1 align:center
And CVMetalTextureCache, with Metal.


75
00:03:59.273 --> 00:04:01.341 line:-1 align:center
Next there is Video Toolbox.


76
00:04:01,375 --> 00:04:04,011 line:-2
This is a low-level framework
that provides direct access


77
00:04:04,044 --> 00:04:06,446 line:-1
to hardware encoders and decoders.


78
00:04:06.480 --> 00:04:10.784 line:-2 align:center
Video Toolbox provides services for
video compression and decompression,


79
00:04:10.817 --> 00:04:13.253 line:-2 align:center
and for conversion between
raster image formats


80
00:04:13,287 --> 00:04:15,556 line:-1
stored in Core Video pixel buffers.


81
00:04:15,589 --> 00:04:19,159 line:-2
VTDecompressionSession
is a powerful low-level interface


82
00:04:19,193 --> 00:04:21,161 line:-1
that is outside of the scope of this talk,


83
00:04:21.195 --> 00:04:24.631 line:-2 align:center
but advanced developers
might want to investigate further.


84
00:04:24,665 --> 00:04:26,800 line:-1
And finally, there is Core Media.


85
00:04:26,834 --> 00:04:30,537 line:-2
This framework defines
the media pipeline used by AVFoundation,


86
00:04:30,571 --> 00:04:33,373 line:-1
and the other high-level media frameworks.


87
00:04:33.407 --> 00:04:37.110 line:-2 align:center
You can always use Core Media's
low-level data types and interfaces


88
00:04:37.144 --> 00:04:39.546 line:-1 align:center
to efficiently process media samples


89
00:04:39,580 --> 00:04:41,648 line:-1
and manage queues of media data.


90
00:04:41.682 --> 00:04:45.319 line:-2 align:center
In the remainder of this talk
we will demonstrate how and when


91
00:04:45.352 --> 00:04:47.688 line:-1 align:center
to use these frameworks in your app.


92
00:04:47,721 --> 00:04:51,225 line:-2
First, how to use AVKit
and AVFoundation


93
00:04:51.258 --> 00:04:55.262 line:-2 align:center
to easily playback HDR video
rendered as EDR.


94
00:04:55,295 --> 00:04:59,399 line:-2
Then a series of more sophisticated
applications of AVPlayer:


95
00:04:59.433 --> 00:05:01.235 line:-1 align:center
to render to your own layer,


96
00:05:01,268 --> 00:05:05,105 line:-2
to access individually decoded frames
via CADisplayLink


97
00:05:05.138 --> 00:05:09.243 line:-2 align:center
and send the resulting CVPixelBuffers
to Core Image for processing,


98
00:05:09,276 --> 00:05:13,247 line:-2
and finally, accessing the decoded frames
as Metal textures


99
00:05:13,280 --> 00:05:15,182 line:-1
via the CVMetalTextureCache


100
00:05:15,215 --> 00:05:17,818 line:-1
for processing and rendering in Metal.


101
00:05:17.851 --> 00:05:22.322 line:-2 align:center
Now that we have an overview of
the video media layer on Apple platforms,


102
00:05:22,356 --> 00:05:26,260 line:-2
we'll focus on AVKit
and AVFoundation frameworks.


103
00:05:26.293 --> 00:05:28.795 line:-2 align:center
Let's get things started
by first discussing playback


104
00:05:28.829 --> 00:05:30.597 line:-1 align:center
of your HDR video content


105
00:05:30,631 --> 00:05:33,433 line:-1
using AVFoundation's AVPlayer interface.


106
00:05:33.467 --> 00:05:35.969 line:-1 align:center
An AVPlayer is a controller object,


107
00:05:36,003 --> 00:05:39,773 line:-2
used to manage the playback
and timing of a media asset.


108
00:05:39,806 --> 00:05:44,244 line:-2
The AVPlayer interface can be used for
high-performance playback of HDR video,


109
00:05:44.278 --> 00:05:47.314 line:-2 align:center
automatically rendering
the result as EDR when possible.


110
00:05:48,282 --> 00:05:51,785 align:center
With AVPlayer, you can play local,
and remote file based media,


111
00:05:51,818 --> 00:05:53,820 align:center
such as QuickTime movies;


112
00:05:53,854 --> 00:05:58,025 align:center
as well as streaming media,
served using HLS.


113
00:05:58,058 --> 00:06:02,729 align:center
Essentially, AVPlayer is used to play
one media asset at a time.


114
00:06:02,763 --> 00:06:07,167 line:-2
You can reuse the player instance
to serially play additional media assets,


115
00:06:07,201 --> 00:06:12,272 line:-2
or even create multiple instances to play
more than one asset simultaneously,


116
00:06:12,306 --> 00:06:17,277 line:-2
but AVPlayer manages the playback
of only a single media asset at a time.


117
00:06:17.311 --> 00:06:21.348 line:-2 align:center
AVFoundation framework also provides
a subclass of AVPlayer


118
00:06:21.381 --> 00:06:24.218 line:-2 align:center
called AVQueuePlayer
that you can use to create


119
00:06:24,251 --> 00:06:29,556 line:-2
and manage the queuing and playing
of sequential HDR media assets.


120
00:06:29.590 --> 00:06:33.193 line:-2 align:center
If your application requires
simple playback of HDR video media


121
00:06:33,227 --> 00:06:37,164 line:-2
rendered to EDR, then AVPlayer
with AVPlayerViewController,


122
00:06:37.197 --> 00:06:39.233 line:-1 align:center
may be the best approach.


123
00:06:39.266 --> 00:06:41.668 line:-1 align:center
Use AVPlayer with AVPlayerLayer


124
00:06:41,702 --> 00:06:45,405 line:-2
to playback your own views
on iOS or macOS.


125
00:06:46.640 --> 00:06:49.810 line:-2 align:center
These are the most straightforward ways
of using AVPlayer.


126
00:06:49.843 --> 00:06:51.845 line:-1 align:center
Let's look at examples of both.


127
00:06:51,879 --> 00:06:55,916 line:-2
First we will look how you can
use AVFoundation's AVPlayer interface,


128
00:06:55,949 --> 00:06:59,853 line:-2
in conjunction with AVKit's
AVPlayer View Controller.


129
00:06:59.887 --> 00:07:03.957 line:-2 align:center
Here, we start by instantiating AVPlayer
from the media's URL.


130
00:07:06.493 --> 00:07:09.630 line:-2 align:center
Next we create
an AVPlayerViewController,


131
00:07:09.663 --> 00:07:12.332 line:-2 align:center
then set the player property
of our viewer controller


132
00:07:12.366 --> 00:07:15.235 line:-2 align:center
to the player we just created
from the media's URL.


133
00:07:18.005 --> 00:07:23.076 line:-2 align:center
And present the view controller modally
to start playback of the video.


134
00:07:23,110 --> 00:07:25,579 line:-1
AVKit manages all the details for you


135
00:07:25.612 --> 00:07:29.116 line:-2 align:center
and will automatically play back
HDR Video as EDR


136
00:07:29.149 --> 00:07:31.852 line:-1 align:center
on displays supporting EDR.


137
00:07:31,885 --> 00:07:35,656 line:-2
As I mentioned, some applications will
need to play back HDR video media


138
00:07:35,689 --> 00:07:37,491 line:-1
into their own view.


139
00:07:37.524 --> 00:07:42.563 line:-2 align:center
Let's look at how to accomplish this
using AVPlayer with AVPlayerLayer.


140
00:07:42.596 --> 00:07:46.266 line:-2 align:center
To play HDR video media as EDR
in your own view,


141
00:07:46.300 --> 00:07:51.371 line:-2 align:center
we again start by creating an AVPlayer
with the media's URL.


142
00:07:51.405 --> 00:07:54.474 line:-2 align:center
However this time
we instantiate an AVPlayerLayer


143
00:07:54.508 --> 00:07:57.444 line:-1 align:center
with the player we just created.


144
00:07:57,477 --> 00:08:00,013 line:-2
Next we need to set the bounds
on the player layer,


145
00:08:00,047 --> 00:08:02,649 line:-1
which we get from the view.


146
00:08:02.683 --> 00:08:05.385 line:-2 align:center
Now that the player layer
has the bounds from the view,


147
00:08:05.419 --> 00:08:10.023 line:-2 align:center
we can add the player layer
as a sublayer to the view.


148
00:08:10.057 --> 00:08:12.926 line:-2 align:center
Finally,
to play back the HDR video media,


149
00:08:12,960 --> 00:08:15,596 line:-1
we call AVPlayer's play method.


150
00:08:15.629 --> 00:08:19.333 line:-2 align:center
That's all that is needed to play back
HDR video media as EDR


151
00:08:19,366 --> 00:08:23,971 line:-2
in your own layer
using AVPlayer and AVPlayerLayer.


152
00:08:24,004 --> 00:08:26,139 line:-2
We just explored
the two most straightforward


153
00:08:26,173 --> 00:08:29,443 line:-2
HDR video playback workflows
using AVPlayer.


154
00:08:29,476 --> 00:08:34,014 line:-2
However, many applications require more
than simple media playback.


155
00:08:35,282 --> 00:08:38,852 line:-2
For example, an application
might require image processing,


156
00:08:38.886 --> 00:08:43.223 line:-2 align:center
such as color grading or chroma keying
to be applied to the video.


157
00:08:43.257 --> 00:08:48.095 line:-2 align:center
Let's explore a workflow that gets
decoded video frames from AVPlayer,


158
00:08:48.128 --> 00:08:51.865 line:-2 align:center
applies Core Image filters
or Metal shaders in real time,


159
00:08:51.899 --> 00:08:55.369 line:-1 align:center
and renders the results as EDR.


160
00:08:55.402 --> 00:08:59.072 line:-2 align:center
We will be demonstrating how to use
AVPlayer and the AVPlayerItem


161
00:08:59.106 --> 00:09:03.010 line:-2 align:center
to decode EDR frames
from your HDR video media,


162
00:09:03,043 --> 00:09:06,947 line:-2
access the decoded frames
from the Core Video display link,


163
00:09:06.980 --> 00:09:11.919 line:-2 align:center
send the resulting pixel buffers
to Core Image or Metal for processing,


164
00:09:11,952 --> 00:09:14,354 line:-2
then render the results
in a CAMetalLayer


165
00:09:14.388 --> 00:09:17.958 line:-1 align:center
as EDR on displays with EDR support.


166
00:09:17,991 --> 00:09:20,360 line:-2
With this in mind,
let's first demonstrate


167
00:09:20.394 --> 00:09:23.330 line:-2 align:center
setting a few key properties
on the CAMetalLayer,


168
00:09:23.363 --> 00:09:28.202 line:-2 align:center
which are required to ensure HDR media
will render correctly as EDR.


169
00:09:28,235 --> 00:09:30,337 line:-1
First we need to get the CAMetalLayer


170
00:09:30.370 --> 00:09:34.274 line:-2 align:center
that we will be rendering
the HDR video content to.


171
00:09:34.308 --> 00:09:37.177 line:-2 align:center
On that layer
we opt into EDR by setting


172
00:09:37.211 --> 00:09:40.614 line:-2 align:center
the wantsExtendedDynamicRangeContent
flag to true.


173
00:09:42.883 --> 00:09:48.222 line:-2 align:center
Please be sure to use a pixel format that
supports Extended Dynamic Range content.


174
00:09:48.255 --> 00:09:52.459 line:-2 align:center
For the AVPlayer example that follows,
we will set the CAMetalLayer to use


175
00:09:52,492 --> 00:09:54,528 line:-1
a half float pixel format,


176
00:09:54.561 --> 00:09:58.265 line:-2 align:center
however a ten bit format
used in conjunction with a PQ


177
00:09:58.298 --> 00:10:01.535 line:-1 align:center
or HLG transfer function would also work.


178
00:10:01,568 --> 00:10:03,904 line:-1
To avoid limiting the result to SDR,


179
00:10:03.937 --> 00:10:06.707 line:-2 align:center
we also need to set the layer
to an EDR compatible


180
00:10:06,740 --> 00:10:08,909 line:-1
extended range color space.


181
00:10:10,577 --> 00:10:13,580 line:-2
In our examples we will be setting
the half float metal texture


182
00:10:13.614 --> 00:10:18.151 line:-2 align:center
to the extended linear display P3
color space.


183
00:10:18,185 --> 00:10:20,487 line:0
We just scratched the surface
regarding EDR,


184
00:10:20,521 --> 00:10:23,090 line:0
color spaces, and pixel buffer formats.


185
00:10:23,123 --> 00:10:25,626 line:0
You might want to check out
my session from last year,


186
00:10:25,659 --> 00:10:27,528 line:0
"HDR rendering with EDR,"


187
00:10:27,561 --> 00:10:31,999 line:0
as well as this year's "EDR on iOS,"
for more details.


188
00:10:33,300 --> 00:10:36,303 line:-2
Now that we have set the basic properties
on the CAMetalLayer,


189
00:10:36.336 --> 00:10:39.873 line:-2 align:center
let's continue the demonstration
by adding real time image processing


190
00:10:39.907 --> 00:10:42.376 line:-1 align:center
using a Core Image, or Metal shader.


191
00:10:42.409 --> 00:10:45.412 line:-2 align:center
We'll be using a display link
in conjunction with AVPlayer


192
00:10:45,445 --> 00:10:48,415 line:-2
to access the decoded video frames
in real time.


193
00:10:49.449 --> 00:10:53.820 line:-2 align:center
For this workflow, you start by creating
an AVPlayer from an AVPlayerItem.


194
00:10:53,854 --> 00:10:57,291 line:-2
Next, you instantiate
an AVPlayerItemVideoOutput,


195
00:10:57.324 --> 00:11:02.496 line:-2 align:center
configured with appropriate pixel buffer
format and color space for EDR.


196
00:11:02,529 --> 00:11:05,399 line:-2
Then you create
and configure a Display link.


197
00:11:05,432 --> 00:11:08,669 align:center
And lastly, you run the Display link
to get the pixel buffers


198
00:11:08,702 --> 00:11:11,805 align:center
to Core Image or Metal for processing.


199
00:11:11,839 --> 00:11:16,310 line:-2
We will demonstrate a CADisplayLink
as is used on iOS.


200
00:11:16.343 --> 00:11:21.381 line:-2 align:center
Please use the equivalent CVDisplayLink
interface when developing for macOS.


201
00:11:21.415 --> 00:11:24.084 line:-2 align:center
This time we choose
to create an AVPlayerItem


202
00:11:24,117 --> 00:11:26,486 line:-1
from our media's URL,


203
00:11:26.520 --> 00:11:32.326 line:-2 align:center
and instantiate an AVPlayer with
the AVPlayerItem that we just created.


204
00:11:32.359 --> 00:11:35.562 line:-2 align:center
Now we create a pair of dictionaries
to specify the color space


205
00:11:35,596 --> 00:11:38,999 line:-2
and pixel buffer format
of the decoded frames.


206
00:11:39.032 --> 00:11:41.635 line:-2 align:center
The first dictionary,
videoColorProperties,


207
00:11:41,668 --> 00:11:45,172 line:-2
is where the color space
and transfer function are specified.


208
00:11:45,205 --> 00:11:48,475 line:-2
In this example we request
the Display P3 colorspace,


209
00:11:48,509 --> 00:11:51,945 line:-2
which corresponds to the color space
of most Apple displays,


210
00:11:51.979 --> 00:11:55.115 line:-2 align:center
and the linear transfer function
which allows AVFoundation


211
00:11:55.148 --> 00:11:58.852 line:-2 align:center
to maintain the extended range values
required for EDR.


212
00:12:00.153 --> 00:12:02.756 line:-2 align:center
The second dictionary,
outputVideoSettings,


213
00:12:02,789 --> 00:12:06,026 line:-2
specifies the characteristics of
the pixel buffer format


214
00:12:06,059 --> 00:12:08,929 line:-2
and also provides a reference
to the videoColorProperties dictionary


215
00:12:08.962 --> 00:12:11.899 line:-1 align:center
we just created.


216
00:12:11.932 --> 00:12:17.504 line:-2 align:center
In this example, we request wide color
and the half float pixel buffer format.


217
00:12:17,538 --> 00:12:20,908 line:-2
It is very helpful
that AVPlayerItemVideoOutput,


218
00:12:20.941 --> 00:12:24.378 line:-2 align:center
not only decodes video
into the pixel buffer format we specify


219
00:12:24.411 --> 00:12:26.480 line:-1 align:center
in the output settings dictionary,


220
00:12:26.513 --> 00:12:30.517 line:-2 align:center
but also automatically performs
any color conversion required


221
00:12:30.551 --> 00:12:34.021 line:-1 align:center
via a pixel transfer session.


222
00:12:34.054 --> 00:12:36.890 line:-2 align:center
Recall, a video might contain
multiple clips,


223
00:12:36,924 --> 00:12:39,393 line:-1
potentially with different colorspaces.


224
00:12:39.426 --> 00:12:42.796 line:-2 align:center
AVFoundation automatically manages
these for us,


225
00:12:42.829 --> 00:12:45.065 line:-1 align:center
and as we'll soon be demonstrating,


226
00:12:45.098 --> 00:12:48.368 line:-2 align:center
this behavior also allows
the resulting decoded video frames


227
00:12:48,402 --> 00:12:51,338 line:-2
to be sent to low level frameworks
like Metal


228
00:12:51.371 --> 00:12:54.341 line:-2 align:center
that don't themselves provide
automatic colorspace conversion


229
00:12:54.374 --> 00:12:56.643 line:-1 align:center
to the display's colorspace.


230
00:12:57,377 --> 00:13:00,447 line:-2
Now we create
the AVPlayerItemVideoOutput


231
00:13:00,480 --> 00:13:03,784 line:-1
with the outputVideoSettings dictionary.


232
00:13:03.817 --> 00:13:06.353 line:-2 align:center
As the third step,
we setup the Display link,


233
00:13:06.386 --> 00:13:10.824 line:-2 align:center
which will be used to access
the decoded frames in real time.


234
00:13:10.858 --> 00:13:15.562 line:-2 align:center
CADisplayLink takes a call back
that is run on each display update.


235
00:13:15.596 --> 00:13:19.366 line:-2 align:center
In our example we call a local function
that we will explore in a moment


236
00:13:19,399 --> 00:13:24,805 line:-2
to get the CVPixelBuffers that we will
be sending to Core Image for processing.


237
00:13:24,838 --> 00:13:27,508 line:-2
Next we create a video player item
observer


238
00:13:27,541 --> 00:13:31,512 line:-2
to allow us to handle changes
to specified player Item properties.


239
00:13:33,113 --> 00:13:35,516 line:-1
Our example will execute this code


240
00:13:35.549 --> 00:13:38.652 line:-2 align:center
every time for the player item's
status changes.


241
00:13:41,088 --> 00:13:44,191 line:-2
When the player item's status
changes to readyToPlay,


242
00:13:44,224 --> 00:13:47,060 line:-1
we add our AVPlayerItemVideoOutput


243
00:13:47,094 --> 00:13:52,165 line:-2
to the new AVPlayerItem
that was just returned,


244
00:13:52,199 --> 00:13:57,070 line:-2
register CADisplayLink
with the main run loop set to common mode,


245
00:13:57.104 --> 00:13:59.673 line:-2 align:center
and start the real time decoding
of the HDR video


246
00:13:59,706 --> 00:14:02,976 line:-1
by calling play on the video player.


247
00:14:04,478 --> 00:14:08,849 line:-2
Finally, we will take a look at an example
CADisplayLink callback implementation,


248
00:14:08,882 --> 00:14:12,653 line:-2
which we referred to earlier
as the `displayLinkCopyPixelBuffers`


249
00:14:12,686 --> 00:14:15,355 line:-1
local function.


250
00:14:15.389 --> 00:14:17.591 line:-1 align:center
Once the HDR video begins to play,


251
00:14:17,624 --> 00:14:22,629 line:-2
the CADisplayLink callback function
is called on each display refresh.


252
00:14:22,663 --> 00:14:27,401 line:-2
For instance it might be called 60 times
a second for a typical display.


253
00:14:27.434 --> 00:14:30.671 line:-2 align:center
This is our code's opportunity
to update the frame displayed


254
00:14:30,704 --> 00:14:34,141 line:-1
if there is a new CVPixelBuffer.


255
00:14:34,174 --> 00:14:37,611 line:-2
On each display callback,
we attempt to copy a CVPixelBuffer


256
00:14:37.644 --> 00:14:40.447 line:-2 align:center
containing the decoded video frame
to be displayed


257
00:14:40.480 --> 00:14:43.717 line:-1 align:center
at the current wall clock time.


258
00:14:43,750 --> 00:14:47,054 line:-2
However,
the `copyPixelBuffer` call might fail,


259
00:14:47,087 --> 00:14:50,123 line:-2
as there won't always be a new
CVPixelBuffer available


260
00:14:50.157 --> 00:14:52.359 line:-1 align:center
at every display refresh,


261
00:14:52.392 --> 00:14:56.797 line:-2 align:center
especially when the screen refresh rate
exceeds that of the video being played.


262
00:14:56.830 --> 00:14:59.032 line:-1 align:center
If there is not a new CVPixelBuffer,


263
00:14:59,066 --> 00:15:01,735 line:-2
then the call fails
and we skip the render.


264
00:15:01,768 --> 00:15:06,373 line:-2
This causes the preceding frame to remain
on-screen for another display refresh.


265
00:15:06,406 --> 00:15:10,143 line:-2
But if the copy succeeds,
then we have a fresh frame of video


266
00:15:10.177 --> 00:15:12.112 line:-1 align:center
in a CVPixelBuffer.


267
00:15:12.145 --> 00:15:16.350 line:-2 align:center
There are a number of ways that we might
process and render this new frame.


268
00:15:16.383 --> 00:15:19.219 line:-2 align:center
One opportunity is to send
the CVPixelBuffer to Core Image


269
00:15:19.253 --> 00:15:21.388 line:-1 align:center
for processing.


270
00:15:21,421 --> 00:15:24,424 line:-2
Core Image can string together
one or more CIFilters


271
00:15:24.458 --> 00:15:28.195 line:-2 align:center
to provide GPU accelerated
image processing to the video frame.


272
00:15:29.596 --> 00:15:33.066 line:-2 align:center
Please note that not all CIFilters
are compatible with EDR


273
00:15:33.100 --> 00:15:35.369 line:-1 align:center
and might have trouble with HDR content,


274
00:15:35.402 --> 00:15:38.772 line:-1 align:center
including clamping to SDR or worse.


275
00:15:38,805 --> 00:15:42,476 line:-2
Core Image provides
many EDR compatible Filters.


276
00:15:42.509 --> 00:15:46.180 line:-2 align:center
Use filter names with
CICategoryHighDynamicRange,


277
00:15:46.213 --> 00:15:49.917 line:-2 align:center
to enumerate EDR compatible
Core Image filters.


278
00:15:49.950 --> 00:15:53.787 line:-2 align:center
In our example, we will be adding
a simple sepia tone effect.


279
00:15:53.820 --> 00:15:58.025 line:-2 align:center
Now let's return to our example
and integrate Core Image.


280
00:15:58.058 --> 00:16:01.461 line:-2 align:center
On each display link callback
that yields a fresh CVPixelBuffer,


281
00:16:01.495 --> 00:16:04.398 line:-1 align:center
create a CIImage from that pixel buffer.


282
00:16:06,099 --> 00:16:09,369 line:-2
Instance the CIFilter
to implement the desired effect.


283
00:16:09,403 --> 00:16:13,574 line:-2
I am using the sepia tone filter
because of its parameter-less simplicity,


284
00:16:13.607 --> 00:16:16.810 line:-2 align:center
however there are many CIFilters
built into the system,


285
00:16:16.844 --> 00:16:20.314 line:-2 align:center
and it is straightforward
to write your own, too.


286
00:16:20.347 --> 00:16:24.685 line:-2 align:center
Set the CIFilter's inputImage
to the CIImage we just created.


287
00:16:26,153 --> 00:16:29,022 line:-2
And the processed video result
will be available


288
00:16:29.056 --> 00:16:32.326 line:-1 align:center
in the filter's outputImage.


289
00:16:32.359 --> 00:16:37.097 line:-2 align:center
Chain as many CIFilters together as are
required to achieve your desired effect.


290
00:16:37,130 --> 00:16:39,399 line:-1
Then use a CIRenderDestination


291
00:16:39,433 --> 00:16:42,970 line:-2
to render the resulting image
to your application's view code.


292
00:16:44.805 --> 00:16:50.043 line:-2 align:center
Please refer to the WWDC 2020 talk
"Optimize the Core Image pipeline for your video app"


293
00:16:50,077 --> 00:16:51,845 line:-1
to learn more about this workflow.


294
00:16:51,879 --> 00:16:55,883 line:-2
Another opportunity, is to process
and render the fresh CVPixelBuffer


295
00:16:55.916 --> 00:16:59.119 line:-1 align:center
using Metal and custom Metal shaders.


296
00:16:59,152 --> 00:17:02,523 line:-2
We will briefly describe the process
of converting the CVPixelBuffer


297
00:17:02.556 --> 00:17:04.057 line:-1 align:center
to a Metal texture.


298
00:17:04,091 --> 00:17:07,828 line:-2
However, implementing this conversion
maintaining best performance


299
00:17:07.861 --> 00:17:10.764 line:-2 align:center
is a deep topic
best left for another talk.


300
00:17:10.797 --> 00:17:13.400 line:-2 align:center
We instead recommend
deriving the Metal texture


301
00:17:13.433 --> 00:17:15.569 line:-1 align:center
from the CoreVideo Metal texture cache,


302
00:17:15,602 --> 00:17:19,640 line:-2
and will walk through that process
as the final example in this talk.


303
00:17:19.673 --> 00:17:24.711 line:-2 align:center
Generally speaking, the process is to get
the IOSurface from the CVPixelBuffer,


304
00:17:24,745 --> 00:17:27,014 line:-1
create a MetalTextureDescriptor,


305
00:17:27,047 --> 00:17:29,483 line:-2
and then create a MetalTexture
from the MetalDevice,


306
00:17:29,516 --> 00:17:32,319 line:-1
using `newTextureWithDescriptor`.


307
00:17:33,887 --> 00:17:37,324 line:-2
However, there is a danger
that the textures may be re-used,


308
00:17:37.357 --> 00:17:41.428 line:-2 align:center
and over-drawn,
if careful locking is not applied.


309
00:17:41.461 --> 00:17:45.799 line:-2 align:center
Further, not all PixelBuffer formats
are natively supported by MetalTexture,


310
00:17:45,832 --> 00:17:49,169 line:-2
which is why we use
half float in this example.


311
00:17:49.203 --> 00:17:51.972 line:-2 align:center
Because of these complications,
we instead recommend


312
00:17:52,005 --> 00:17:56,677 line:-2
directly accessing Metal textures from
Core Video, as we will now demonstrate.


313
00:17:56.710 --> 00:18:00.147 line:-2 align:center
Let's further explore Core Video
and Metal.


314
00:18:00,180 --> 00:18:03,851 line:-2
As mentioned, CVMetalTextureCache
is both a straightforward


315
00:18:03,884 --> 00:18:07,421 line:-2
and efficient way
to use CVPixelBuffers with Metal.


316
00:18:07.454 --> 00:18:10.757 line:-2 align:center
CVMetalTextureCache is handy
because you get a Metal texture


317
00:18:10,791 --> 00:18:14,862 line:-2
directly from the cache
without need for further conversion.


318
00:18:14,895 --> 00:18:18,031 line:-2
CVMetalTextureCache
automatically bridges between


319
00:18:18.065 --> 00:18:21.101 line:-1 align:center
CVPixelBuffer's, and MetalTexture's,


320
00:18:21.134 --> 00:18:26.073 line:-2 align:center
thereby both simplifying your code
and keeping you on the fast path.


321
00:18:26.106 --> 00:18:28.876 line:-1 align:center
In conjunction with CVPixelBufferPools,


322
00:18:28.909 --> 00:18:32.646 line:-2 align:center
CVMetalTextureCache
also provides performance benefits,


323
00:18:32.679 --> 00:18:36.116 line:-2 align:center
by keeping MTLTexture
to IOSurface mapping alive.


324
00:18:37,784 --> 00:18:41,054 line:-2
Finally, using CVMetalTextureCache
removes the need


325
00:18:41,088 --> 00:18:43,557 line:-1
to manually track IOSurfaces.


326
00:18:43,590 --> 00:18:46,293 line:-1
Now the final example in our talk:


327
00:18:46.326 --> 00:18:49.196 line:-2 align:center
how to extract Metal textures
directly from Core Video


328
00:18:49.229 --> 00:18:51.698 line:-1 align:center
using CVMetalTextureCache.


329
00:18:52,499 --> 00:18:55,469 line:-2
Here, we start by getting
the system default Metal device.


330
00:18:55.502 --> 00:18:58.071 line:-2 align:center
We use that to create
a Metal Texture Cache,


331
00:18:58,105 --> 00:19:01,341 line:-2
and then instantiate
a Core Video Metal Texture Cache


332
00:19:01.375 --> 00:19:04.111 line:-1 align:center
associated with the Metal Texture Cache.


333
00:19:04,144 --> 00:19:08,549 line:0
That can then be used to access
decoded video frames as Metal Textures,


334
00:19:08,582 --> 00:19:13,353 align:center
which conveniently,
can be directly used in our Metal engine.


335
00:19:13,387 --> 00:19:18,292 line:-2
In this example, we create and use
the Metal system default device.


336
00:19:18,325 --> 00:19:21,261 line:-1
Next we create the CVMetalTextureCache


337
00:19:21,295 --> 00:19:23,530 line:-1
with CVMetalTextureCacheCreate,


338
00:19:23.564 --> 00:19:26.967 line:-2 align:center
specifying the Metal device
we just created.


339
00:19:27.000 --> 00:19:29.970 line:-2 align:center
We get the height and width
of the CVPixelBuffer


340
00:19:30.003 --> 00:19:33.574 line:-2 align:center
needed to create the Core Video
Metal texture.


341
00:19:33,607 --> 00:19:37,511 line:-2
Then we call
`CVMetalTextureCacheCreateTextureFromImage`,


342
00:19:37,544 --> 00:19:40,247 line:-1
to instantiate a CVMetalTexture object


343
00:19:40,280 --> 00:19:43,750 line:-2
and associate that
with the CVPixelBuffer.


344
00:19:43,784 --> 00:19:46,653 line:-2
Finally we call
`CVMetalTextureGetTexture`,


345
00:19:46.687 --> 00:19:50.057 line:-1 align:center
to get the desired Metal texture.


346
00:19:50.090 --> 00:19:54.394 line:-2 align:center
Swift applications should use
a strong reference for CVMetalTexture,


347
00:19:54.428 --> 00:19:57.731 line:-2 align:center
however, when using Objective-C,
you must ensure that Metal is done


348
00:19:57.764 --> 00:20:01.835 line:-2 align:center
with your texture before you release
the CVMetalTextureRef.


349
00:20:01,869 --> 00:20:05,806 line:-2
This may be accomplished using
metal command buffer completion handlers.


350
00:20:07.674 --> 00:20:09.042 line:-1 align:center
And that's all, folks!


351
00:20:09,076 --> 00:20:11,678 line:-2
To review,
we explored a number of workflows


352
00:20:11.712 --> 00:20:14.715 line:-2 align:center
that will render your HDR video media
to EDR,


353
00:20:14,748 --> 00:20:17,684 line:-2
for playback, editing,
or image processing.


354
00:20:18.685 --> 00:20:23.190 line:-2 align:center
You learned how to go from AVPlayer to
AVKit's AVPlayerViewController,


355
00:20:23.223 --> 00:20:26.360 line:-1 align:center
for playback of HDR media.


356
00:20:26.393 --> 00:20:30.264 line:-2 align:center
You also learned how use AVPlayer,
along with AVPlayerLayer,


357
00:20:30,297 --> 00:20:34,234 line:-1
to display HDR media on your own view.


358
00:20:34.268 --> 00:20:38.505 line:-2 align:center
And finally, we explored how to add
real time effects during playback.


359
00:20:38,539 --> 00:20:42,009 line:-2
Connecting AVFoundation's AVPlayer
to CoreVideo


360
00:20:42.042 --> 00:20:44.178 line:-1 align:center
and then to Metal for rendering.


361
00:20:44,211 --> 00:20:46,947 line:-2
And applying real time effects
using CoreImage filters,


362
00:20:46,980 --> 00:20:49,082 line:-1
as well as Metal shaders.


363
00:20:51.852 --> 00:20:55.789 line:-2 align:center
If you want to dig deeper,
I recommend a few WWDC sessions


364
00:20:55,822 --> 00:20:58,258 line:-1
related to creating video workflows,


365
00:20:58,292 --> 00:21:02,429 line:-1
as well as integrating HDR media with EDR.


366
00:21:02.462 --> 00:21:04.565 line:-1 align:center
I especially want to call out the session


367
00:21:04,598 --> 00:21:08,368 line:-2
"Edit and play back HDR video
with AVFoundation".


368
00:21:08.402 --> 00:21:11.505 line:-2 align:center
This session explores use
of AVVideoComposition


369
00:21:11,538 --> 00:21:17,211 line:-2
with `applyingCIFiltersWithHandler`
for applying effects to your HDR media.


370
00:21:17.244 --> 00:21:20.414 line:-2 align:center
In this session you'll also learn
how to use custom compositor,


371
00:21:20,447 --> 00:21:23,183 line:-2
which can then be used
with a CVPixelBuffer,


372
00:21:23.217 --> 00:21:26.820 line:-2 align:center
when each video frame becomes
available for processing.


373
00:21:26,854 --> 00:21:29,857 align:center
As I mentioned at the beginning,
this year we're also presenting


374
00:21:29,890 --> 00:21:33,827 line:0
two other sessions on EDR:
"EDR on iOS,"


375
00:21:33,861 --> 00:21:38,665 align:center
where we announced EDR API support
has expanded to include iOS,


376
00:21:38,699 --> 00:21:44,204 align:center
and "HDR content display with
EDR using CoreImage, Metal and SwiftUI,"


377
00:21:44,238 --> 00:21:48,976 line:0
where we further explore integrating
EDR with other media frameworks.


378
00:21:49.009 --> 00:21:52.946 line:-2 align:center
Hope you incorporate HDR video
into your EDR enabled applications


379
00:21:52.980 --> 00:21:56.116 line:-1 align:center
on both macOS and now iOS.


380
00:21:56.149 --> 00:21:57.751 line:-1 align:center
Thanks for watching.

