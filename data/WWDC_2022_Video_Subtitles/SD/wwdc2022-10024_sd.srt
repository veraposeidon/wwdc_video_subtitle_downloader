2
00:00:00.334 --> 00:00:07.341 line:-1 align:center
♪ ♪


3
00:00:09,676 --> 00:00:11,645 line:-2
Hi, my name is Brett Keating,


4
00:00:11.678 --> 00:00:15.549 line:-2 align:center
and it's my pleasure to be introducing you
to what is new in the Vision framework.


5
00:00:15.582 --> 00:00:17.551 line:-1 align:center
You may be new to Vision.


6
00:00:17.584 --> 00:00:21.388 line:-2 align:center
Perhaps this is the first session
you've seen about the Vision framework.


7
00:00:21,421 --> 00:00:23,991 line:-1
If so, welcome.


8
00:00:24.024 --> 00:00:28.862 line:-2 align:center
For your benefit, let's briefly recap some
highlights about the Vision framework.


9
00:00:28.896 --> 00:00:31.665 line:-1 align:center
Some Vision framework facts for you.


10
00:00:31.698 --> 00:00:34.434 line:-1 align:center
Vision was first introduced in 2017,


11
00:00:34.468 --> 00:00:37.871 line:-2 align:center
and since then, many thousands
of great apps have been developed


12
00:00:37,905 --> 00:00:40,841 line:-1
with the technology Vision provides.


13
00:00:40,874 --> 00:00:43,510 line:-2
Vision is a collection
of computer vision algorithms


14
00:00:43.544 --> 00:00:45.646 line:-1 align:center
that continues to grow over time


15
00:00:45,679 --> 00:00:47,781 line:-2
and includes such things
as face detection,


16
00:00:47.814 --> 00:00:52.019 line:-2 align:center
image classification,
and contour detection to name a few.


17
00:00:52,052 --> 00:00:56,490 line:-2
Each of these algorithms is made available
through an easy-to-use, consistent API.


18
00:00:56.523 --> 00:00:58.926 line:-2 align:center
If you know how to run one algorithm
in the Vision framework,


19
00:00:58,959 --> 00:01:01,061 line:-1
you know how to run them all.


20
00:01:01,094 --> 00:01:04,698 line:-2
And Vision takes full advantage
of Apple Silicon


21
00:01:04.731 --> 00:01:06.934 line:-1 align:center
on all of the platforms it supports,


22
00:01:06.967 --> 00:01:10.771 line:-2 align:center
to power the machine learning at the core
of many of Vision's algorithms.


23
00:01:10,804 --> 00:01:13,473 line:-1
Vision is available on tvOS,


24
00:01:13.507 --> 00:01:15.709 line:-1 align:center
iOS, and macOS;


25
00:01:15.742 --> 00:01:19.179 line:-2 align:center
and will fully leverage
Apple Silicon on the Mac.


26
00:01:20.113 --> 00:01:23.884 line:-2 align:center
Some recent additions to the Vision
framework include Person segmentation,


27
00:01:23,917 --> 00:01:25,252 line:-1
shown here.


28
00:01:27,287 --> 00:01:31,358 line:-2
Also hand pose estimation,
shown in this demo.


29
00:01:34.761 --> 00:01:37.631 line:-2 align:center
And here is our
Action and Vision sample app,


30
00:01:37.664 --> 00:01:41.134 line:-2 align:center
which uses body pose estimation
and trajectory analysis.


31
00:01:42.069 --> 00:01:46.306 line:-2 align:center
Our agenda today begins
with an overview of some new revisions,


32
00:01:46,340 --> 00:01:49,176 line:-1
which are updates to existing requests


33
00:01:49.209 --> 00:01:54.081 line:-2 align:center
that may provide increased functionality,
improve performance, or improve accuracy.


34
00:01:57,150 --> 00:02:00,454 line:-2
First, we have a new revision
for text recognition.


35
00:02:00.487 --> 00:02:06.527 line:-2 align:center
This is the third revision,
given by VNRecognizeTextRequestRevision3.


36
00:02:06.560 --> 00:02:10.464 line:-2 align:center
This is the text recognizer
that powers the amazing Live Text feature.


37
00:02:10.497 --> 00:02:13.333 line:-2 align:center
The text recognizer supports
several languages,


38
00:02:13.367 --> 00:02:15.969 line:-2 align:center
and you may discover
which languages are supported


39
00:02:16.003 --> 00:02:19.173 line:-1 align:center
by calling supportedRecognitionLanguages.


40
00:02:19,206 --> 00:02:23,443 line:-2
We have now added a few new languages,
and I'll show you a couple examples.


41
00:02:23.477 --> 00:02:27.247 line:-2 align:center
We are now supporting
the Korean language in Vision.


42
00:02:27.281 --> 00:02:31.618 line:-2 align:center
Here is an example of Vision at work
transcribing a Korean receipt.


43
00:02:31.652 --> 00:02:35.255 line:-2 align:center
And here is a corresponding example
for Japanese,


44
00:02:35,289 --> 00:02:38,058 line:-2
also showing the results
of Vision's text recognition


45
00:02:38,091 --> 00:02:40,961 line:-1
on this now-supported language.


46
00:02:40.994 --> 00:02:46.767 line:-2 align:center
For text recognition, we have
a new automatic language identification.


47
00:02:46.800 --> 00:02:50.170 line:-2 align:center
You may still specify
the recognition languages to use


48
00:02:50,204 --> 00:02:53,740 line:-1
using the recognitionLanguages property.


49
00:02:53,774 --> 00:02:56,577 line:-2
But suppose you don't know
ahead of time which languages


50
00:02:56.610 --> 00:02:59.913 line:-2 align:center
your app user might be trying
to recognize.


51
00:02:59,947 --> 00:03:03,417 line:-2
Now, but only
for accurate recognition mode,


52
00:03:03,450 --> 00:03:07,955 line:-2
you may ask the text recognizer
to automatically detect the language


53
00:03:07,988 --> 00:03:11,091 line:-2
by setting
automaticallyDetectsLanguage to true.


54
00:03:12,759 --> 00:03:15,562 line:-2
It's best to use this
just for such a situation


55
00:03:15,596 --> 00:03:18,398 line:-2
where you don't know
which language to recognize,


56
00:03:18.432 --> 00:03:21.702 line:-2 align:center
because the language detection
can occasionally get this wrong.


57
00:03:21.735 --> 00:03:25.038 line:-2 align:center
If you have the prior knowledge
about which language to recognize,


58
00:03:25.072 --> 00:03:27.975 line:-2 align:center
it's still best to specify
these languages to Vision


59
00:03:28,008 --> 00:03:31,612 line:-2
and leave automaticallyDetectsLanguage
turned off.


60
00:03:34.681 --> 00:03:39.119 line:-2 align:center
Next, we have a new third revision
for our barcode detection,


61
00:03:39,152 --> 00:03:43,123 line:-1
called VNDetectBarcodesRequestRevision3.


62
00:03:43,156 --> 00:03:46,360 line:-2
This revision leverages
modern machine learning under the hood,


63
00:03:46,393 --> 00:03:49,162 line:-1
which is a departure from prior revisions.


64
00:03:49.196 --> 00:03:51.431 line:-1 align:center
Barcodes come in a variety of symbologies,


65
00:03:51.465 --> 00:03:55.769 line:-2 align:center
from barcodes often seen on products
in stores, to QR codes,


66
00:03:55,802 --> 00:03:59,239 line:-2
to specialty codes
used in healthcare applications.


67
00:03:59,273 --> 00:04:01,775 line:-2
In order to know
which symbologies Vision supports,


68
00:04:01.808 --> 00:04:03.977 line:-1 align:center
you may call supportedSymbologies.


69
00:04:06.113 --> 00:04:08.582 line:-1 align:center
Let's talk about performance.


70
00:04:08,615 --> 00:04:10,617 line:-1
Partly because we are using ML,


71
00:04:10.651 --> 00:04:15.222 line:-2 align:center
we are detecting multiple codes
in one shot rather than one at a time,


72
00:04:15.255 --> 00:04:20.160 line:-2 align:center
so the request will be faster
for images containing multiple codes.


73
00:04:20,194 --> 00:04:25,465 line:-2
Also, more codes are detected
in a given image containing many codes,


74
00:04:25.499 --> 00:04:27.167 line:-1 align:center
due to increased accuracy.


75
00:04:27,201 --> 00:04:31,538 line:-2
And furthermore, there are few,
if any, duplicate detections.


76
00:04:31.572 --> 00:04:34.441 line:-2 align:center
The bounding boxes are
improved for some codes,


77
00:04:34,474 --> 00:04:39,780 line:-2
particularly linear codes such as ean13,
for which a line was formerly returned.


78
00:04:39,813 --> 00:04:42,850 line:-2
Now, the bounding box surrounds
the entire visible code.


79
00:04:45.018 --> 00:04:49.022 line:-2 align:center
Finally, the ML model is more able
to ignore such things as curved surfaces,


80
00:04:49.056 --> 00:04:53.393 line:-2 align:center
reflections, and other artifacts that have
hindered detection accuracy in the past.


81
00:04:56,797 --> 00:04:59,733 line:-2
Both of these new revisions,
for text recognition


82
00:04:59,766 --> 00:05:03,504 align:center
and for barcode detection,
form the technological foundations


83
00:05:03,537 --> 00:05:08,075 align:center
for the VisionKit Data Scanner API,
which is a drop-in UI element


84
00:05:08,108 --> 00:05:11,879 align:center
that sets up the camera stream
to scan and return barcodes and text.


85
00:05:11,912 --> 00:05:14,948 line:0
It's really a fantastic addition
to our SDK,


86
00:05:14,982 --> 00:05:19,119 line:0
and I highly recommend you check out
the session about it to learn more.


87
00:05:19,152 --> 00:05:22,956 line:-2
The final new revision I'll tell you
about today is a new revision


88
00:05:22,990 --> 00:05:25,626 line:-1
for our optical flow request called


89
00:05:25.659 --> 00:05:29.363 line:-1 align:center
VNGenerateOpticalFlowRequestRevision2.


90
00:05:29,396 --> 00:05:32,900 line:-2
Like the barcode detector,
this new revision also uses


91
00:05:32,933 --> 00:05:35,102 line:-1
modern machine learning under the hood.


92
00:05:37.471 --> 00:05:41.508 line:-2 align:center
Although optical flow is one of the
longest studied computer vision problems,


93
00:05:41.542 --> 00:05:44.444 line:-2 align:center
you might not be aware of what it does,
compared to detection of things


94
00:05:44.478 --> 00:05:47.981 line:-2 align:center
which form part of all of our daily lives,
like text and barcodes.


95
00:05:48,849 --> 00:05:52,452 line:-2
Optical flow analyzes
two consecutive images,


96
00:05:52,486 --> 00:05:54,621 line:-1
typically frames from a video.


97
00:05:54.655 --> 00:05:56.590 line:-2 align:center
Depending on your use case,
you might look at motion


98
00:05:56.623 --> 00:06:00.027 line:-2 align:center
between two adjacent frames,
or skip a few frames in between,


99
00:06:00,060 --> 00:06:02,996 line:-2
but in any case, the two images
should be in chronological order.


100
00:06:04,831 --> 00:06:09,303 line:-2
The analysis provides an estimate of
the direction and magnitude of the motion,


101
00:06:09,336 --> 00:06:13,774 line:-2
or by how much parts of the first image
need to "move," so to speak,


102
00:06:13,807 --> 00:06:17,544 line:-2
to be positioned correctly
in the second image.


103
00:06:17.578 --> 00:06:20.480 line:-1 align:center
A VNPixelBufferObservation is the result,


104
00:06:20.514 --> 00:06:23.684 line:-2 align:center
which represents this motion
at all places in the image.


105
00:06:23.717 --> 00:06:25.485 line:-1 align:center
It is a two-channel image.


106
00:06:25.519 --> 00:06:28.021 line:-1 align:center
One channel contains the X magnitude,


107
00:06:28,055 --> 00:06:30,791 line:-1
and the other contains the Y magnitude.


108
00:06:30.824 --> 00:06:34.094 line:-2 align:center
Together, these form 2D vectors
at each pixel


109
00:06:34,127 --> 00:06:37,064 line:-2
arranged in this 2D image
so that their locations map


110
00:06:37,097 --> 00:06:41,134 line:-2
to corresponding locations in the images
that were provided as input.


111
00:06:41,168 --> 00:06:43,470 line:-1
Let's have a look at this visually.


112
00:06:43.504 --> 00:06:47.541 line:-2 align:center
Suppose you have an incoming video
and several frames are coming in,


113
00:06:47.574 --> 00:06:50.210 line:-2 align:center
but let's look
at these two images in particular.


114
00:06:50.244 --> 00:06:52.479 line:-1 align:center
Here we have a dog running on the beach.


115
00:06:52,513 --> 00:06:54,248 line:-1
From the left image to the right image,


116
00:06:54,281 --> 00:06:57,017 line:-2
it appears the dog has moved
a bit to the left.


117
00:06:57.050 --> 00:06:59.419 line:-2 align:center
How would you estimate
and represent this motion?


118
00:07:01,255 --> 00:07:03,323 line:0
Well, you would run optical flow


119
00:07:03,357 --> 00:07:05,926 align:center
and arrive at something
akin to the image below.


120
00:07:05,959 --> 00:07:08,562 align:center
The darker areas are
where motion has been found,


121
00:07:08,595 --> 00:07:12,566 align:center
and notice that it does indeed
look just like the shape of the dog.


122
00:07:12,599 --> 00:07:16,170 align:center
That's because only the dog
is really moving in this scene.


123
00:07:16,203 --> 00:07:19,907 align:center
We are showing the motion vectors
in this image by using "false color,"


124
00:07:19,940 --> 00:07:23,777 line:0
which maps the x,y
from the vectors into a color palette.


125
00:07:23,810 --> 00:07:27,681 line:0
In this false color representation,
"red" hues happen to indicate


126
00:07:27,714 --> 00:07:30,150 line:0
movement primarily to the left.


127
00:07:30.184 --> 00:07:33.253 line:-2 align:center
Now that you've seen
an example from one frame,


128
00:07:33.287 --> 00:07:35.989 line:-2 align:center
let's see how it looks
for a whole video clip.


129
00:07:36,023 --> 00:07:39,660 line:0
Here we compute optical flow
for a short clip of this dog


130
00:07:39,693 --> 00:07:42,196 align:center
fetching a water bottle on a beach.


131
00:07:42,229 --> 00:07:45,065 line:0
On the left is the result from revision 1.


132
00:07:45,098 --> 00:07:49,002 align:center
On the right is the result
from our new ML-based revision 2.


133
00:07:49,036 --> 00:07:52,639 align:center
Hopefully some of the improvements
in revision 2 are clear to see.


134
00:07:52,673 --> 00:07:55,142 align:center
For one thing, perhaps most obviously,


135
00:07:55,175 --> 00:07:58,712 align:center
the water bottle's motion is captured
much more accurately.


136
00:07:58,745 --> 00:08:03,083 align:center
You might also notice improvements in
some of the estimated motion of the dog.


137
00:08:03,116 --> 00:08:05,953 align:center
I notice improvements
in the tail most clearly


138
00:08:05,986 --> 00:08:10,023 align:center
but also can see the motion of his ears
flapping in the new revision.


139
00:08:10,057 --> 00:08:13,293 align:center
The first revision also contains
a bit of background noise motions,


140
00:08:13,327 --> 00:08:17,664 line:0
while the second revision more coherently
represents the backgrounds as not moving.


141
00:08:17.698 --> 00:08:21.668 line:-2 align:center
Hopefully that example gave you
a good idea what this technology does.


142
00:08:21.702 --> 00:08:25.272 line:-2 align:center
Now let's dive in a bit
on how you might use it in your app.


143
00:08:25.305 --> 00:08:29.910 line:-2 align:center
Clearly the primary use case is
to discover local motion in a video.


144
00:08:29.943 --> 00:08:32.980 line:-2 align:center
This feeds directly
into security video use cases,


145
00:08:33.013 --> 00:08:35.682 line:-2 align:center
where it's most important to identify
and localize motions


146
00:08:35,716 --> 00:08:37,784 line:-1
that deviate from the background,


147
00:08:37.818 --> 00:08:40.053 line:-2 align:center
and it should be mentioned
that optical flow does work best


148
00:08:40,087 --> 00:08:44,391 line:-2
for stationary cameras,
such as most security cameras.


149
00:08:44,424 --> 00:08:46,393 line:-2
You might want to use
Vision's object tracker


150
00:08:46,426 --> 00:08:48,795 line:-2
to track objects
that are moving in a video,


151
00:08:48.829 --> 00:08:51.265 line:-2 align:center
but need to know
where to initialize a tracker.


152
00:08:51.298 --> 00:08:54.501 line:-1 align:center
Optical flow can help you there as well.


153
00:08:54.535 --> 00:08:58.338 line:-2 align:center
If you have some computer vision
or image processing savvy of your own,


154
00:08:58,372 --> 00:09:00,407 line:-2
you might leverage
our optical flow results


155
00:09:00,440 --> 00:09:03,076 line:-1
to enable further video processing.


156
00:09:03,110 --> 00:09:06,747 line:-2
Video interpolation,
or video action analysis,


157
00:09:06.780 --> 00:09:10.851 line:-2 align:center
can greatly benefit from
the information optical flow provides.


158
00:09:10,884 --> 00:09:13,854 line:-2
Let's now dig into some
important additional differences


159
00:09:13,887 --> 00:09:16,023 line:-1
between revision 1 and revision 2.


160
00:09:16,857 --> 00:09:19,459 line:-2
Revision 1 always returns
optical flow fields


161
00:09:19.493 --> 00:09:22.029 line:-2 align:center
that have the same resolution
as the input.


162
00:09:22,062 --> 00:09:25,032 line:-1
Revision 2 will also do this by default.


163
00:09:25.065 --> 00:09:28.135 line:-2 align:center
However, there is a tiny wrinkle:
partially due to the fact


164
00:09:28.168 --> 00:09:32.039 line:-2 align:center
that revision 2 is ML-based,
the output of the underlying model


165
00:09:32.072 --> 00:09:36.710 line:-2 align:center
is relatively low resolution
compared to most input image resolutions.


166
00:09:36,743 --> 00:09:40,414 line:-2
Therefore, to match
revision 1 default behavior,


167
00:09:40,447 --> 00:09:42,382 line:-1
some upsampling must be done,


168
00:09:42.416 --> 00:09:45.853 line:-2 align:center
and we are using
bilinear upsampling to do this.


169
00:09:45,886 --> 00:09:48,856 line:-2
Here is a visual example
explaining what upsampling does.


170
00:09:48,889 --> 00:09:52,659 line:-2
On the left, we have a zoomed-in portion
of the network output,


171
00:09:52.693 --> 00:09:55.863 line:-2 align:center
which is low resolution
and therefore appears pixelated.


172
00:09:55.896 --> 00:10:00.000 line:-2 align:center
The overall flow field might have
an aspect ratio of 7:5.


173
00:10:00.033 --> 00:10:03.504 line:-2 align:center
On the right, we have a similar region
taken from the same field,


174
00:10:03.537 --> 00:10:06.507 line:-2 align:center
upsampled to the original
image resolution.


175
00:10:06.540 --> 00:10:11.645 line:-2 align:center
Perhaps that image also has
a different aspect ratio, let's say 16:9.


176
00:10:11,678 --> 00:10:14,948 line:-2
You will notice that the edges
of the flow field are smoothed out


177
00:10:14,982 --> 00:10:18,018 line:-1
by the bilinear upsampling.


178
00:10:18,051 --> 00:10:20,854 line:-3
Due to the potential
for the aspect ratios to differ,


179
00:10:20,888 --> 00:10:23,524 line:-3
keep in mind that as part
of the upsampling process,


180
00:10:23,557 --> 00:10:25,893 line:-2
the flow image will be stretched


181
00:10:25.926 --> 00:10:28.061 line:-3 align:center
in order to properly correspond
the flow field


182
00:10:28,095 --> 00:10:30,230 line:-2
to what is happening in the image.


183
00:10:30.264 --> 00:10:32.266 line:-3 align:center
When working
with the network output directly,


184
00:10:32,299 --> 00:10:36,136 line:-3
you should account for resolution
and aspect ratio in a similar fashion


185
00:10:36,170 --> 00:10:39,006 line:-3
when mapping flow results
to the original images.


186
00:10:41.575 --> 00:10:43.777 line:-1 align:center
You have the option to skip the upsampling


187
00:10:43,810 --> 00:10:47,581 line:-2
by turning on keepNetworkOutput
on the request.


188
00:10:47,614 --> 00:10:49,983 line:-1
This will give you the raw model output.


189
00:10:50.017 --> 00:10:54.154 line:-2 align:center
There are four computationAccuracy
settings you may apply to the request


190
00:10:54.188 --> 00:10:57.357 line:-2 align:center
in order to choose
an available output resolution.


191
00:10:57,391 --> 00:11:00,694 line:-2
You can see the resolutions
for each accuracy setting in this table,


192
00:11:00.727 --> 00:11:03.297 line:-2 align:center
but be sure to always check
the width and height of the pixel buffer


193
00:11:03,330 --> 00:11:05,265 line:-1
contained in the observation.


194
00:11:06,066 --> 00:11:07,634 line:-1
When should you use network output,


195
00:11:07.668 --> 00:11:10.404 line:-2 align:center
and when should you allow Vision
to upsample?


196
00:11:10.437 --> 00:11:14.408 line:-2 align:center
The default behavior is best
if you already are using optical flow


197
00:11:14.441 --> 00:11:17.678 line:-2 align:center
and want the behavior
to remain backward compatible.


198
00:11:17.711 --> 00:11:20.280 line:-2 align:center
It's also a good option
if you want upsampled output,


199
00:11:20.314 --> 00:11:24.885 line:-2 align:center
and bilinear is acceptable to you and
worth the additional memory and latency.


200
00:11:24,918 --> 00:11:28,355 line:-2
Network output is best
if you don't need full resolution


201
00:11:28,388 --> 00:11:33,227 line:-2
and can form correspondences on the fly
or just want to initialize a tracker.


202
00:11:33,260 --> 00:11:35,262 line:-2
Network output may also be
the right choice


203
00:11:35.295 --> 00:11:37.364 line:-1 align:center
if you do need a full resolution flow,


204
00:11:37.397 --> 00:11:40.567 line:-2 align:center
but would prefer to use
your own upsampling methods.


205
00:11:40.601 --> 00:11:44.705 line:-2 align:center
That covers the new algorithm revisions
for this session.


206
00:11:44.738 --> 00:11:47.140 line:-2 align:center
Let's move on to discuss
some spring cleaning we are doing


207
00:11:47,174 --> 00:11:50,511 line:-2
in the Vision framework
and how it might impact you.


208
00:11:50,544 --> 00:11:53,780 line:-2
We first introduced face detection
and face landmarks


209
00:11:53.814 --> 00:11:56.316 line:-2 align:center
when Vision was initially released
five years ago,


210
00:11:56,350 --> 00:11:59,419 line:-1
as "revision 1" for each algorithm.


211
00:11:59.453 --> 00:12:03.123 line:-2 align:center
Since that time
we've released two newer revisions,


212
00:12:03,156 --> 00:12:06,593 line:-2
which use more efficient
and more accurate technologies.


213
00:12:06.627 --> 00:12:10.330 line:-2 align:center
Therefore, we are removing
the first revisions of these algorithms


214
00:12:10,364 --> 00:12:15,035 line:-2
from Vision framework, while keeping
the second and third revisions only.


215
00:12:15,068 --> 00:12:18,138 line:-2
However, if you use revision 1,
never fear.


216
00:12:18.172 --> 00:12:21.708 line:-2 align:center
We will continue to support code
that specifies revision 1


217
00:12:21,742 --> 00:12:26,947 line:-2
or code that has been compiled against
SDKs which only contained revision 1.


218
00:12:26,980 --> 00:12:28,749 line:-1
How is that possible, you may ask?


219
00:12:28,782 --> 00:12:32,252 line:-2
Revision 1 executes an algorithm
under the hood


220
00:12:32.286 --> 00:12:36.256 line:-2 align:center
that I have called
"the revision 1 detector" in this diagram.


221
00:12:36,290 --> 00:12:40,527 line:-2
In the same way, revision 2 uses
the revision 2 detector.


222
00:12:40,561 --> 00:12:42,796 line:-2
What we have done
for this release of Vision


223
00:12:42.829 --> 00:12:45.065 line:-1 align:center
is to satisfy revision 1 requests


224
00:12:45.098 --> 00:12:48.101 line:-2 align:center
with the output
of the revision 2 detector.


225
00:12:48,135 --> 00:12:52,506 line:-2
Additionally, the revision 1 request
will be marked as deprecated.


226
00:12:52,539 --> 00:12:56,376 line:-2
This allows us to remove
the old revision 1 detector completely,


227
00:12:56.410 --> 00:12:59.613 line:-2 align:center
allowing the Vision framework
to remain streamlined.


228
00:12:59.646 --> 00:13:01.448 line:-1 align:center
This has several benefits,


229
00:13:01,481 --> 00:13:04,318 line:-2
not the least of which
is to save space on disk,


230
00:13:04,351 --> 00:13:09,256 line:-2
which makes our OS releases and SDKs
less expensive to download and install.


231
00:13:09.289 --> 00:13:13.760 line:-2 align:center
All you Vision experts out there might be
saying to yourselves, "But wait a minute,


232
00:13:13,794 --> 00:13:18,298 line:-2
"revision 2 returns upside down faces
while revision 1 does not.


233
00:13:18,332 --> 00:13:21,435 line:-2
Couldn't this behavior difference
have an impact on some apps?"


234
00:13:21.468 --> 00:13:24.605 line:-2 align:center
It certainly would,
except we will be taking precautions


235
00:13:24,638 --> 00:13:27,541 line:-1
to preserve revision 1 behavior.


236
00:13:27.574 --> 00:13:32.312 line:-2 align:center
We will not be returning upside-down faces
from the revision 2 detector.


237
00:13:32,346 --> 00:13:35,616 line:-2
Similarly,
the revision 2 landmark detector


238
00:13:35.649 --> 00:13:40.053 line:-2 align:center
will return results that match
the revision 1 landmark constellation.


239
00:13:40.087 --> 00:13:42.389 line:-1 align:center
The execution time is on par,


240
00:13:42.422 --> 00:13:45.726 line:-2 align:center
and you ought to experience
a boost in accuracy.


241
00:13:45.759 --> 00:13:48.262 line:-2 align:center
In any case,
this change will not require any apps


242
00:13:48,295 --> 00:13:52,332 line:-2
to make any modifications to their code,
and things will continue to work.


243
00:13:54.134 --> 00:13:57.237 line:-1 align:center
Still, we have a call to action for you.


244
00:13:57.271 --> 00:13:59.573 line:-2 align:center
You shouldn't be satisfied
with using revision 1


245
00:13:59,606 --> 00:14:02,509 line:-2
when we have
much better options available.


246
00:14:02,543 --> 00:14:04,645 line:-2
We always recommend
using the latest revisions,


247
00:14:04.678 --> 00:14:07.548 line:-2 align:center
and for these requests,
that would be revision 3.


248
00:14:08,749 --> 00:14:11,251 line:-2
Of course the main reason
for this recommendation


249
00:14:11.285 --> 00:14:14.588 line:-2 align:center
is to use the latest technology,
which provides the highest level


250
00:14:14,621 --> 00:14:18,725 line:-2
of accuracy and performance available,
and who doesn't want that?


251
00:14:18,759 --> 00:14:22,129 line:-2
Furthermore, we have established
and communicated several times,


252
00:14:22.162 --> 00:14:24.565 line:-1 align:center
and we reiterate again here,


253
00:14:24.598 --> 00:14:28.368 line:-2 align:center
the best practice of always
explicitly specifying your revisions,


254
00:14:28,402 --> 00:14:31,371 line:-2
rather than relying
upon default behaviors.


255
00:14:31,405 --> 00:14:34,408 line:-2
And that's what we've done
for our spring cleaning.


256
00:14:34.441 --> 00:14:36.944 line:-2 align:center
Now let's talk about how we've made it
easier to debug apps


257
00:14:36.977 --> 00:14:38.812 line:-1 align:center
that use the Vision framework.


258
00:14:38.846 --> 00:14:41.949 line:-2 align:center
We've added
Quick Look Preview support to Vision.


259
00:14:41,982 --> 00:14:44,685 line:-2
What does this mean
for Vision in particular?


260
00:14:44.718 --> 00:14:48.255 line:-2 align:center
Well, now you can mouse over
VNObservations in the debugger,


261
00:14:48,288 --> 00:14:52,860 line:-2
and with one click, you can visualize
the result on your input image.


262
00:14:52.893 --> 00:14:55.863 line:-2 align:center
We've also made this available
in Xcode Playgrounds.


263
00:14:55.896 --> 00:14:59.099 line:-2 align:center
I think the only way to really explain
how this can benefit your debugging


264
00:14:59.132 --> 00:15:00.334 line:-1 align:center
is to show you.


265
00:15:00.367 --> 00:15:02.603 line:-1 align:center
Let's move to an Xcode demo.


266
00:15:04.471 --> 00:15:08.208 line:-2 align:center
Here we have a simple routine
that will detect face landmarks


267
00:15:08.242 --> 00:15:11.345 line:-1 align:center
and return the face observations.


268
00:15:11.378 --> 00:15:15.849 line:-1 align:center
First, we set up a face landmarks request.


269
00:15:15.883 --> 00:15:20.921 line:-2 align:center
Then, if we have an image
ready to go in our class, we display it.


270
00:15:20,954 --> 00:15:24,458 line:-2
Then, we declare an array
to hold our results.


271
00:15:26.126 --> 00:15:27.995 line:-1 align:center
Inside the autoreleasepool,


272
00:15:28,028 --> 00:15:31,098 line:-2
we instantiate a request handler
with that image,


273
00:15:31.131 --> 00:15:34.034 line:-1 align:center
and then perform our request.


274
00:15:34.067 --> 00:15:38.338 line:-2 align:center
Assuming all went well, we can retrieve
the results from the request.


275
00:15:39.206 --> 00:15:44.011 line:-2 align:center
I will run it and get to a breakpoint
after we retrieve the results.


276
00:15:44,044 --> 00:15:45,812 line:-1
So now I'm in the debugger.


277
00:15:45.846 --> 00:15:47.581 line:-1 align:center
When I mouse over the results,


278
00:15:47,614 --> 00:15:50,284 line:-2
the overlay shows
I've detected three faces.


279
00:15:50,317 --> 00:15:53,754 line:-2
That's great.
I do have three faces in my input image.


280
00:15:53,787 --> 00:15:56,557 line:-2
But how do I know
which observation is which face?


281
00:15:56.590 --> 00:15:59.927 line:-2 align:center
That's where
the Quick Look Preview support comes in.


282
00:15:59.960 --> 00:16:04.898 line:-2 align:center
As I go into this request,
I can click on each "eye" icon


283
00:16:04,932 --> 00:16:07,501 line:-1
in order to visualize the result.


284
00:16:07.534 --> 00:16:12.206 line:-2 align:center
The image appears with overlays drawn
for the landmarks constellation


285
00:16:12.239 --> 00:16:14.408 line:-1 align:center
and for the face bounding box.


286
00:16:15,676 --> 00:16:18,612 line:-2
Now you know where the first
observation is in the image.


287
00:16:19.780 --> 00:16:23.250 line:-2 align:center
I can click on the next one to draw
overlays for the second observation


288
00:16:23.283 --> 00:16:25.619 line:-1 align:center
and for the third observation.


289
00:16:27,521 --> 00:16:30,924 line:-2
Continuing to the next breakpoint,
we run some code


290
00:16:30,958 --> 00:16:34,561 line:-2
that prints the face observations
to the debug console.


291
00:16:34,595 --> 00:16:37,497 line:-2
As you can imagine,
here in the debug console


292
00:16:37.531 --> 00:16:40.534 line:-2 align:center
where the face information is printed,
it's pretty hard


293
00:16:40,567 --> 00:16:43,570 line:-2
to immediately visualize in your mind
which face is which


294
00:16:43,604 --> 00:16:46,907 line:-2
or whether the results look correct
just from these printed coordinates.


295
00:16:48,976 --> 00:16:51,245 line:-2
But there is one more thing
to point out here.


296
00:16:51,278 --> 00:16:54,515 line:-2
Notice that I've somewhat artificially
forced the request handler


297
00:16:54.548 --> 00:16:57.918 line:-2 align:center
out of scope by introducing
an autoreleasepool.


298
00:16:57.951 --> 00:17:00.420 line:-2 align:center
Now that the request handler
is out of scope,


299
00:17:00.454 --> 00:17:03.924 line:-2 align:center
let's use the Quick Look Preview
support again on the results.


300
00:17:03,957 --> 00:17:06,693 line:-2
Well, what do you know,
the overlays are still drawn,


301
00:17:06,727 --> 00:17:08,395 line:-1
but the image is not available.


302
00:17:09.496 --> 00:17:12.966 line:-2 align:center
This is something to keep in mind:
the image request handler that was used


303
00:17:13,000 --> 00:17:16,737 line:-2
to generate the observations
must still be in scope somewhere


304
00:17:16.770 --> 00:17:21.108 line:-2 align:center
in order for Quick Look Preview support
to use the original image for display.


305
00:17:21,141 --> 00:17:25,212 line:-2
That is because the image request handler
is where your input image resides.


306
00:17:25.245 --> 00:17:28.348 line:-2 align:center
Things will continue to work,
but the image will not be available.


307
00:17:28.382 --> 00:17:31.952 line:-2 align:center
This Quick Look preview support
can be especially useful


308
00:17:31,985 --> 00:17:34,054 line:-1
in an Xcode Playgrounds session,


309
00:17:34,087 --> 00:17:37,691 line:-2
while doing quick experiments
to see how things work.


310
00:17:37.724 --> 00:17:40.160 line:-1 align:center
Let's have a look at that now.


311
00:17:40.194 --> 00:17:44.865 line:-2 align:center
Here we have a simple Playground set up
to analyze images for barcodes.


312
00:17:44.898 --> 00:17:47.935 line:-2 align:center
Rather than go through this code,
let's just make a couple modifications


313
00:17:47.968 --> 00:17:50.537 line:-1 align:center
and check out how it impacts the results.


314
00:17:50,571 --> 00:17:52,773 line:-1
We'll start off by using revision 2


315
00:17:52,806 --> 00:17:56,743 line:-2
on an image with two barcodes
of different symbologies.


316
00:17:56.777 --> 00:18:00.113 line:-2 align:center
All the results at once are displayed
if we ask for all the results,


317
00:18:00.147 --> 00:18:03.150 line:-2 align:center
and just the first result
is also displayed at the end.


318
00:18:04,718 --> 00:18:07,287 line:-2
Notice that revision 2 has
a couple issues.


319
00:18:07,321 --> 00:18:10,257 line:-1
First, it missed the first barcode.


320
00:18:10.290 --> 00:18:13.427 line:-2 align:center
Also, it detected
the second barcode twice.


321
00:18:13,460 --> 00:18:15,329 line:-2
And it gives you a line
through the barcode


322
00:18:15,362 --> 00:18:17,564 line:-1
rather than a complete bounding box.


323
00:18:19,566 --> 00:18:23,637 line:-2
What happens if we change
to revision 3 now, instead of revision 2?


324
00:18:26,106 --> 00:18:28,609 line:-1
First of all, we detect both barcodes.


325
00:18:28.642 --> 00:18:32.346 line:-2 align:center
And, instead of a line,
we are given complete bounding boxes.


326
00:18:33.313 --> 00:18:35.949 line:-2 align:center
What is great about this
Quick Look Preview support


327
00:18:35,983 --> 00:18:39,553 line:-2
is that we've removed the need for you
to write a variety of utility functions


328
00:18:39,586 --> 00:18:41,522 line:-1
to visualize the results.


329
00:18:41,555 --> 00:18:44,625 line:-2
They can be overlaid directly
on your images in the debugger


330
00:18:44,658 --> 00:18:46,560 line:-1
or in an Xcode Playground.


331
00:18:49,796 --> 00:18:54,101 line:-2
So that is Quick Look
Preview support in Vision.


332
00:18:54.134 --> 00:18:58.071 line:-2 align:center
Now you can more easily know
which observation is which.


333
00:18:58.105 --> 00:19:00.407 line:-2 align:center
Just be sure to keep
the image request handler in scope


334
00:19:00,440 --> 00:19:02,976 line:-1
in order to use it with your input image,


335
00:19:03,010 --> 00:19:06,113 line:-2
and hopefully the Xcode Playground support
will make live tuning


336
00:19:06.146 --> 00:19:08.482 line:-1 align:center
of your Vision framework code much easier.


337
00:19:08.515 --> 00:19:11.418 line:-2 align:center
We've covered some important updates
to Vision today.


338
00:19:11.451 --> 00:19:14.588 line:-2 align:center
To quickly review, we've added
some great new revisions


339
00:19:14.621 --> 00:19:19.493 line:-2 align:center
to text recognition,
barcode detection, and optical flow.


340
00:19:21.295 --> 00:19:25.999 line:-2 align:center
As we continue to add updated revisions,
we will also be removing older ones,


341
00:19:26.033 --> 00:19:27.768 line:-1 align:center
so keep your revisions up-to-date


342
00:19:27.801 --> 00:19:30.771 line:-2 align:center
and use the latest
and greatest technology.


343
00:19:30,804 --> 00:19:34,241 line:-2
We've also made debugging
Vision applications much easier this year


344
00:19:34,274 --> 00:19:36,643 line:-1
with Quick Look Preview support.


345
00:19:36.677 --> 00:19:41.048 line:-2 align:center
I hope you enjoyed this session,
and have a wonderful WWDC.  ♪ ♪

