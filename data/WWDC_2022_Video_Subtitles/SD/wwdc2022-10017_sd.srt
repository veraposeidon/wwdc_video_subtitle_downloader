2
00:00:00,334 --> 00:00:07,341 line:-1
♪ ♪


3
00:00:09.943 --> 00:00:12.246 line:-2 align:center
Ciao, my name is Geppy Parziale,


4
00:00:12.279 --> 00:00:15.148 line:-2 align:center
and I am a machine learning engineer
here at Apple.


5
00:00:15.182 --> 00:00:18.952 line:-2 align:center
Today, I want to walk you through
the journey of building an app


6
00:00:18.986 --> 00:00:21.355 line:-2 align:center
that uses machine learning
to solve a problem


7
00:00:21.388 --> 00:00:25.893 line:-2 align:center
that would usually require an expert
to perform some very specialized work.


8
00:00:27,394 --> 00:00:31,098 line:-2
This journey gives me the opportunity
to show you how to add


9
00:00:31,131 --> 00:00:33,600 line:-2
open source machine learning models
to your apps


10
00:00:33,634 --> 00:00:37,471 line:-1
and create fantastic new experiences.


11
00:00:37,504 --> 00:00:39,640 line:-1
During the journey, I will also highlight


12
00:00:39.673 --> 00:00:44.411 line:-2 align:center
a few of the many tools, frameworks,
and APIs available for you


13
00:00:44,444 --> 00:00:48,749 line:-2
in the Apple development ecosystem
to build apps using machine learning.


14
00:00:49.983 --> 00:00:53.654 line:-2 align:center
When building an app, you, the developer,
go through a series of decisions


15
00:00:53.687 --> 00:00:57.824 line:-2 align:center
that hopefully will bring
the best experience to your users.


16
00:00:57.858 --> 00:01:02.396 line:-2 align:center
And this is also true when adding machine
learning functionality to applications.


17
00:01:04.031 --> 00:01:07.067 line:-1 align:center
During the development, you could ask:


18
00:01:07.100 --> 00:01:10.637 line:-2 align:center
should I use machine learning
to build this feature?


19
00:01:10,671 --> 00:01:14,408 line:-1
How can I get a machine learning model?


20
00:01:14.441 --> 00:01:18.745 line:-2 align:center
How do I make that model
compatible with Apple platforms?


21
00:01:18.779 --> 00:01:22.716 line:-2 align:center
Will that model work
for my specific use case?


22
00:01:22.749 --> 00:01:26.320 line:-1 align:center
Does it run on the Apple Neural Engine?


23
00:01:26.353 --> 00:01:29.590 line:-1 align:center
So let's take this journey together.


24
00:01:29,623 --> 00:01:33,193 line:-2
I want to build an app that allows me
to add realistic colors


25
00:01:33,227 --> 00:01:38,198 line:-2
to my family black and white photos
that I found in an old box in my basement.


26
00:01:39,666 --> 00:01:44,338 line:-2
Of course, a professional photographer
could do this with some manual work,


27
00:01:44,371 --> 00:01:47,908 line:-2
spending some time
in a photo editing tool.


28
00:01:47.941 --> 00:01:50.744 line:-2 align:center
Instead, what if I want
to automate this process


29
00:01:50,777 --> 00:01:54,515 line:-2
and apply the colorization
in just a few seconds?


30
00:01:54,548 --> 00:01:57,584 line:-2
This seems to be a perfect task
for machine learning.


31
00:01:58.919 --> 00:02:02.155 line:-2 align:center
Apple offers a tremendous amount
of frameworks and tools


32
00:02:02.189 --> 00:02:06.026 line:-2 align:center
that can help you build and integrate
ML functionality in your apps.


33
00:02:06.960 --> 00:02:09.296 line:-2 align:center
They provide everything
from data processing


34
00:02:09.329 --> 00:02:11.999 line:-1 align:center
to model training and inference.


35
00:02:12,032 --> 00:02:15,669 line:-2
For this journey,
I am going to use a few of them.


36
00:02:15,702 --> 00:02:19,006 line:-1
But remember, you have many to choose from


37
00:02:19.039 --> 00:02:22.910 line:-2 align:center
depending on the specific machine learning
task that you are developing.


38
00:02:22.943 --> 00:02:25.712 line:-2 align:center
The process I use when developing
a machine learning feature


39
00:02:25.746 --> 00:02:28.582 line:-1 align:center
in my apps goes through a set of stages.


40
00:02:29.650 --> 00:02:32.753 line:-2 align:center
First, I search for the right
machine learning model


41
00:02:32.786 --> 00:02:36.723 line:-2 align:center
in either scientific publication
or specialized website.


42
00:02:38.058 --> 00:02:40.360 line:-1 align:center
I searched for photo colorization,


43
00:02:40,394 --> 00:02:45,332 line:-2
and I found a model called Colorizer
that may work for what I need.


44
00:02:46,233 --> 00:02:50,337 line:0
Here is an example of the colorization
I can get using this model.


45
00:02:53,473 --> 00:02:55,175 line:-1
Here is another one.


46
00:02:56.643 --> 00:03:00.547 line:-1 align:center
And here is another one. Really great.


47
00:03:00.581 --> 00:03:03.083 line:-1 align:center
Let me show you how it works.


48
00:03:03,116 --> 00:03:07,821 align:center
The Colorizer model expects
a black and white image as input.


49
00:03:07,855 --> 00:03:14,228 line:0
The Python source code I found converts
any RGB image to a LAB color space image.


50
00:03:15,362 --> 00:03:18,031 line:0
This color space has 3 channels:


51
00:03:18,065 --> 00:03:22,102 line:0
one represents the image lightness
or L channel,


52
00:03:22,135 --> 00:03:25,806 align:center
and the other two represent
the color components.


53
00:03:25,839 --> 00:03:27,875 line:0
The color components are discarded


54
00:03:27,908 --> 00:03:31,378 line:0
while the lightness becomes
the input of the colorizer model.


55
00:03:32,646 --> 00:03:35,516 line:0
The model then estimates
two new color components


56
00:03:35,549 --> 00:03:40,053 line:0
that, combined with the input L channel,
provide the resulting image with color.


57
00:03:41,154 --> 00:03:45,459 line:0
It's time now to make this model
compatible with my app.


58
00:03:45.492 --> 00:03:49.496 line:-2 align:center
To achieve this, I can convert
the original PyTorch model


59
00:03:49.530 --> 00:03:53.300 line:-1 align:center
to Core ML format using coremltools.


60
00:03:53,333 --> 00:03:58,071 line:-2
Here is the simple Python script I used
to convert the PyTorch model to Core ML.


61
00:03:59,439 --> 00:04:02,910 line:-2
First, I import the PyTorch model
architecture and weights.


62
00:04:04.378 --> 00:04:07.114 line:-1 align:center
Then I trace the imported model.


63
00:04:07.147 --> 00:04:11.285 line:-2 align:center
Finally, I convert the PyTorch model
to Core ML and save it.


64
00:04:12,953 --> 00:04:15,389 line:-1
Once the model is in the Core ML format,


65
00:04:15,422 --> 00:04:18,992 line:-2
I need to verify
that the conversion worked correctly.


66
00:04:19,026 --> 00:04:23,130 line:-2
I can do that directly in Python
again using coremltools.


67
00:04:23,163 --> 00:04:25,499 line:-1
And this is easy.


68
00:04:25.532 --> 00:04:28.702 line:-1 align:center
I import the image in RGB color space


69
00:04:28,735 --> 00:04:31,238 line:-1
and convert it to Lab color space.


70
00:04:32.940 --> 00:04:37.144 line:-2 align:center
Then I separate the lightness
from the color channels and discard them.


71
00:04:38.745 --> 00:04:41.515 line:-2 align:center
I run the prediction
using the Core ML model.


72
00:04:42,616 --> 00:04:44,918 align:center
And finally, compose the input lightness


73
00:04:44,952 --> 00:04:48,355 align:center
with the estimated color components
and convert to RGB.


74
00:04:49.723 --> 00:04:53.794 line:-2 align:center
This allows me to verify
that functionality of the converted model


75
00:04:53.827 --> 00:04:57.464 line:-2 align:center
matches the functionality
of the original PyTorch model.


76
00:04:57.497 --> 00:05:01.301 line:-1 align:center
I call this stage Model Verification.


77
00:05:01,335 --> 00:05:05,305 line:-2
However, there is another
important check to be done.


78
00:05:05,339 --> 00:05:10,644 line:-2
I need to understand if this model
can run fast enough on my target device.


79
00:05:10.677 --> 00:05:13.747 line:-2 align:center
So I would need to assess
the model on device


80
00:05:13,780 --> 00:05:17,851 line:-2
and make sure it would provide
the best user experience.


81
00:05:17.885 --> 00:05:22.322 line:-2 align:center
The new Core ML Performance report,
available now in Xcode 14,


82
00:05:22.356 --> 00:05:26.393 line:-2 align:center
performs a time-based analysis
of a Core ML model.


83
00:05:26,426 --> 00:05:29,630 line:-2
I just need to drag and drop
the model into Xcode


84
00:05:29,663 --> 00:05:32,699 line:-2
and create a performance report
in a few seconds.


85
00:05:33.834 --> 00:05:37.871 line:-2 align:center
Using this tool,
I can see that estimated prediction time


86
00:05:37.905 --> 00:05:43.577 line:-2 align:center
is almost 90 milliseconds
on an iPad Pro with M1 and iPadOS 16.


87
00:05:44,578 --> 00:05:48,215 align:center
And this is perfect
for my photo colorization app.


88
00:05:48,248 --> 00:05:51,718 line:0
If you want to know more about
Performance Report in Xcode,


89
00:05:51,752 --> 00:05:56,957 line:0
I suggest you to watch this year’s session
"Optimize your Core ML usage".


90
00:05:56,990 --> 00:06:01,128 line:-2
So Performance Report can help you
make an assessment of the model


91
00:06:01.161 --> 00:06:04.798 line:-2 align:center
and make sure it provides
the best on-device user experience.


92
00:06:05.966 --> 00:06:09.937 line:-2 align:center
Now that I am sure about the functionality
and performance of my model,


93
00:06:09,970 --> 00:06:11,872 line:-1
let me integrate it into my app.


94
00:06:13.507 --> 00:06:18.178 line:-2 align:center
The integration process is identical
to what I have done until now in Python,


95
00:06:18,212 --> 00:06:20,848 line:-1
but this time I can do it seamlessly in Swift


96
00:06:20,881 --> 00:06:24,151 line:-2
using Xcode and all the other tools
you are familiar with.


97
00:06:26,286 --> 00:06:29,289 line:-1
Remember the model, now in Core ML format,


98
00:06:29.323 --> 00:06:33.227 line:-2 align:center
expects a single channel image
representing its lightness.


99
00:06:34.695 --> 00:06:37.798 line:-2 align:center
So similarly to what I did
previously in Python,


100
00:06:37,831 --> 00:06:43,203 line:-2
I need to convert any RGB input image
to an image using the Lab color space.


101
00:06:45,839 --> 00:06:48,408 line:-2
I could write this transformation
in multiple ways:


102
00:06:48,442 --> 00:06:52,212 line:-2
directly in Swift with vImage,
or using Metal.


103
00:06:53.514 --> 00:06:57.784 line:-2 align:center
Exploring deeper in the documentation,
I found that the Core Image framework


104
00:06:57,818 --> 00:07:00,554 line:-2
provides something
that can help me with this.


105
00:07:02.656 --> 00:07:06.360 line:-2 align:center
So let me show you how to achieve
the RGB to LAB conversion


106
00:07:06.393 --> 00:07:08.929 line:-2 align:center
and run the prediction
using the Core ML model.


107
00:07:10,864 --> 00:07:13,667 line:-2
Here is the Swift code
to extract the lightness


108
00:07:13,700 --> 00:07:17,538 line:-2
from an RGB image
and pass it to the Core ML model.


109
00:07:17.571 --> 00:07:22.509 line:-2 align:center
First, I convert the RGB image
into LAB and extract the lightness.


110
00:07:23,944 --> 00:07:27,014 line:-1
Then, I convert lightness to a CGImage


111
00:07:27,047 --> 00:07:30,050 line:-2
and prepare the input
for the Core ML model.


112
00:07:31.585 --> 00:07:33.921 line:-1 align:center
Finally, I run the prediction.


113
00:07:33,954 --> 00:07:37,658 line:-2
To extract the L channel
from the input RGB image,


114
00:07:37.691 --> 00:07:41.094 line:-2 align:center
I first convert the RGB image
into a LAB image,


115
00:07:41,128 --> 00:07:45,365 line:-1
using the new CIFilter convertRGBtoLab.


116
00:07:45.399 --> 00:07:49.570 line:-2 align:center
The values of the lightness
are set between 0 and 100.


117
00:07:51,038 --> 00:07:54,708 line:-2
Then, I multiply the Lab image
with a color matrix


118
00:07:54.741 --> 00:07:59.613 line:-2 align:center
and discard the color channels
and return the lightness to the caller.


119
00:07:59.646 --> 00:08:02.983 line:-2 align:center
Let's now analyze what happens
at the output of the model.


120
00:08:04,751 --> 00:08:07,921 line:-2
The Core ML model returns
two MLShapedArrays


121
00:08:07,955 --> 00:08:10,624 line:-1
containing the estimated color components.


122
00:08:12.326 --> 00:08:18.031 line:-2 align:center
So after the prediction, I convert
the two MLShapedArray into two CIImages.


123
00:08:19,533 --> 00:08:23,937 line:-2
Finally, I combine them
with the model input lightness.


124
00:08:23.971 --> 00:08:28.075 line:-2 align:center
This generates a new LAB image
that I convert to RGB and return it.


125
00:08:30,344 --> 00:08:34,715 line:-2
To convert the two MLShapedArrays
into two CIImages,


126
00:08:34,748 --> 00:08:38,752 line:-2
I first extract the values
from each shaped array.


127
00:08:38,785 --> 00:08:42,356 line:-2
Then, I create two core images
representing the two color channels


128
00:08:42.389 --> 00:08:44.958 line:-1 align:center
and return them.


129
00:08:44.992 --> 00:08:47.961 line:-2 align:center
To combine the lightness
with the estimated color channels,


130
00:08:47.995 --> 00:08:51.498 line:-1 align:center
I use a custom CIKernel that takes


131
00:08:51.532 --> 00:08:55.068 line:-2 align:center
the three channels as input
and returns a CIImage.


132
00:08:56,503 --> 00:09:00,107 line:-2
Then, I use the new CIFilter
convertLabToRGB


133
00:09:00,140 --> 00:09:04,978 line:-2
to convert the Lab image to RGB
and return it to the caller.


134
00:09:05.012 --> 00:09:08.015 line:-2 align:center
This is the source code
for the custom CIKernel I use


135
00:09:08.048 --> 00:09:12.052 line:-2 align:center
to combine the lightness
with the two estimated color channels


136
00:09:12,085 --> 00:09:13,754 line:-1
in a single CIImage.


137
00:09:14,888 --> 00:09:19,359 align:center
For more information about
the new CI filters to convert RGB images


138
00:09:19,393 --> 00:09:23,664 line:0
to LAB images and vice versa,
please refer to the session


139
00:09:23,697 --> 00:09:27,734 align:center
"Display EDR content with Core Image,
Metal, and SwiftUI."


140
00:09:29,002 --> 00:09:32,472 line:-2
Now that I completed the integration
of this ML feature in my app,


141
00:09:32.506 --> 00:09:34.541 line:-1 align:center
let's see it in action.


142
00:09:34,575 --> 00:09:36,443 line:-1
But wait.


143
00:09:36,476 --> 00:09:41,014 line:-2
How will I colorize my old family photos
in real time with my application?


144
00:09:41,048 --> 00:09:45,519 line:-2
I could spend some time to digitize
each of them and import them in my app.


145
00:09:46,653 --> 00:09:48,956 line:-1
I think I have a better idea.


146
00:09:48,989 --> 00:09:51,925 line:-2
What if I use my iPad camera
to scan these photos


147
00:09:51,959 --> 00:09:54,061 line:-1
and colorize them live?


148
00:09:54,094 --> 00:09:58,465 line:-2
I think it would be really fun, and I have
everything I need to accomplish this.


149
00:09:58.498 --> 00:10:01.535 line:-1 align:center
But first, I have to solve a problem.


150
00:10:02,970 --> 00:10:06,607 line:-2
My model needs 90 milliseconds
to process an image.


151
00:10:06,640 --> 00:10:09,877 line:-2
If I want to process a video,
I would need something faster.


152
00:10:11,111 --> 00:10:13,013 line:-1
For a smooth user experience,


153
00:10:13,046 --> 00:10:16,850 line:-2
I'd like to run the device camera
at least 30 fps.


154
00:10:17.885 --> 00:10:22.256 line:-2 align:center
That means the camera will produce
a frame about every 30 milliseconds.


155
00:10:24.191 --> 00:10:28.529 line:-2 align:center
But since the model needs about
90 milliseconds to colorize a video frame,


156
00:10:28.562 --> 00:10:32.566 line:-2 align:center
I am going to lose 2 or 3 frames
during each colorization.


157
00:10:35,269 --> 00:10:39,406 line:-2
The total prediction time of a model
is a function of both its architecture


158
00:10:39.439 --> 00:10:44.111 line:-2 align:center
as well as the compute units operations
it gets mapped to.


159
00:10:44,144 --> 00:10:48,515 line:-2
Looking at the performance report again,
I notice that my model has a total


160
00:10:48.549 --> 00:10:54.188 line:-2 align:center
of 61 operations running on a combination
of the neural engine and CPU.


161
00:10:55.622 --> 00:10:59.726 line:-2 align:center
If I want a faster prediction time,
I will have to change the model.


162
00:11:00.694 --> 00:11:03.530 line:-2 align:center
I decided to experiment
with the model's architecture


163
00:11:03.564 --> 00:11:06.967 line:-2 align:center
and explore some alternatives
that may be faster.


164
00:11:07,000 --> 00:11:11,972 line:-2
However, a change in the architecture
means I need to retrain the network.


165
00:11:13.941 --> 00:11:15.609 line:-1 align:center
Apple offers different solutions


166
00:11:15.642 --> 00:11:19.112 line:-2 align:center
that allow me to train machine learning
models directly on my Mac.


167
00:11:20,514 --> 00:11:25,018 line:-2
In my case, since the original model
was developed in PyTorch,


168
00:11:25,052 --> 00:11:27,888 line:-1
I decided to use the new PyTorch on Metal,


169
00:11:27.921 --> 00:11:31.625 line:-2 align:center
so I can take advantage
of the tremendous hardware acceleration


170
00:11:31.658 --> 00:11:33.660 line:-1 align:center
provided by Apple Silicon.


171
00:11:35,696 --> 00:11:40,167 align:center
If you are interested in knowing more
about the PyTorch accelerated with Metal,


172
00:11:40,200 --> 00:11:43,937 align:center
please check the session,
"Accelerate machine learning with Metal"


173
00:11:46.173 --> 00:11:49.810 line:-2 align:center
Because of this change,
our journey needs to take a step back.


174
00:11:50.878 --> 00:11:53.914 line:-2 align:center
After retraining,
I will have to convert the result


175
00:11:53.947 --> 00:11:57.317 line:-2 align:center
to Core ML format
and run the verification again.


176
00:11:59,052 --> 00:12:01,455 line:-1
This time, model integration consists


177
00:12:01.488 --> 00:12:04.725 line:-2 align:center
of simply swapping the old model
with the new one.


178
00:12:04,758 --> 00:12:08,195 line:-2
After retraining a few candidate
alternative models,


179
00:12:08.228 --> 00:12:11.932 line:-2 align:center
I have verified one
that will meet my requirements.


180
00:12:11,965 --> 00:12:16,537 line:-2
Here it is with
the corresponding performance report.


181
00:12:16,570 --> 00:12:18,672 line:-1
It runs entirely on the neural engine


182
00:12:18.705 --> 00:12:22.843 line:-2 align:center
and the prediction time is now
around 16 milliseconds,


183
00:12:22.876 --> 00:12:24.545 line:-1 align:center
which works for video.


184
00:12:27,347 --> 00:12:32,186 line:-2
But Performance Report tells me only one
aspect of the performance of my app.


185
00:12:33.353 --> 00:12:36.723 line:-2 align:center
Indeed, after running my app,
I notice immediately


186
00:12:36,757 --> 00:12:40,294 line:-2
that colorization is not as smooth
as I expect.


187
00:12:40.327 --> 00:12:43.697 line:-1 align:center
So what happens in my app at runtime?


188
00:12:45.132 --> 00:12:50.103 line:-2 align:center
In order to understand that, I can use
the new Core ML template in Instruments.


189
00:12:52.706 --> 00:12:55.576 line:-2 align:center
Analyzing the initial portion
of the Core ML trace,


190
00:12:55.609 --> 00:13:00.514 line:-2 align:center
after loading the model, I notice
that the app accumulates predictions.


191
00:13:00.547 --> 00:13:02.549 line:-1 align:center
And this is unexpected.


192
00:13:02.583 --> 00:13:06.587 line:-2 align:center
Instead, I'd expect to have
a single prediction per frame.


193
00:13:08.121 --> 00:13:12.826 line:-2 align:center
Zooming in the trace
and checking the very first predictions,


194
00:13:12,860 --> 00:13:16,296 line:-2
I observe that the app requests
a second Core ML prediction


195
00:13:16,330 --> 00:13:18,565 line:-1
before the first one is finished.


196
00:13:19,800 --> 00:13:23,904 line:-2
Here, the Neural Engine is
still working on the first request


197
00:13:23,937 --> 00:13:26,440 line:-1
when the second is given to Core ML.


198
00:13:27.574 --> 00:13:30.310 line:-1 align:center
Similarly, the third prediction starts


199
00:13:30.344 --> 00:13:33.413 line:-1 align:center
while still processing the second one.


200
00:13:33,447 --> 00:13:35,549 line:-1
Even after four predictions,


201
00:13:35,582 --> 00:13:39,052 line:-1
the lag between the request and execution


202
00:13:39,086 --> 00:13:42,055 line:-1
is already about 20 milliseconds.


203
00:13:42.089 --> 00:13:45.759 line:-2 align:center
Instead, I need to make sure
that a new prediction starts


204
00:13:45,792 --> 00:13:50,330 line:-2
only if the previous one is finished
to avoid cascading these lags.


205
00:13:51.798 --> 00:13:55.936 line:-2 align:center
While fixing this problem,
I also found out that I accidentally set


206
00:13:55.969 --> 00:14:01.175 line:-2 align:center
the camera frame rate to 60 fps
instead of the desired 30fps.


207
00:14:03.377 --> 00:14:06.513 line:-2 align:center
After making sure that the apps process
a new frame


208
00:14:06,547 --> 00:14:08,849 line:-1
after the previous prediction completes


209
00:14:08,882 --> 00:14:12,219 line:-2
and setting the camera frame rate
to 30 fps,


210
00:14:12.252 --> 00:14:15.923 line:-2 align:center
I can see that Core ML dispatches
correctly a single prediction


211
00:14:15.956 --> 00:14:20.160 line:-2 align:center
to the Apple Neural Engine,
and now the app runs smoothly.


212
00:14:22.362 --> 00:14:24.731 line:-1 align:center
So we reached the end of our journey.


213
00:14:26.066 --> 00:14:28.869 line:-2 align:center
Let's test the app
on my old family photos.


214
00:14:34.508 --> 00:14:38.512 line:-2 align:center
Here are my black and white photos
that I found in my basement.


215
00:14:38.545 --> 00:14:42.583 line:-2 align:center
They capture some of the places
in Italy I visited a long time ago.


216
00:14:49.389 --> 00:14:52.426 line:-2 align:center
Here is a great photo
of the Colosseum in Rome.


217
00:14:53,760 --> 00:14:56,730 line:-2
The color of the walls
and the sky are so realistic.


218
00:14:59.266 --> 00:15:01.034 line:-1 align:center
Let's check this one.


219
00:15:03.370 --> 00:15:06.240 line:-2 align:center
This is Castel del Monte
in the South of Italy.


220
00:15:06,273 --> 00:15:07,741 line:-1
Really nice.


221
00:15:09.643 --> 00:15:12.513 line:-1 align:center
And this is my hometown, Grottaglie.


222
00:15:12.546 --> 00:15:15.949 line:-2 align:center
Adding colors to these images
triggered so many memories.


223
00:15:17.184 --> 00:15:20.554 line:-2 align:center
Notice that I am applying
the colorization only to the photo


224
00:15:20.587 --> 00:15:23.390 line:-2 align:center
while keeping the rest of the scene
black and white.


225
00:15:26.326 --> 00:15:29.796 line:-2 align:center
Here, I am taking advantage
of the rectangle detection algorithm


226
00:15:29.830 --> 00:15:32.432 line:-1 align:center
available in the Vision framework.


227
00:15:32.466 --> 00:15:37.204 line:-2 align:center
Using VNDetectRectangleRequest,
I can isolate the photo in the scene


228
00:15:37.237 --> 00:15:39.806 line:-2 align:center
and use it as input
to the Colorizer model.


229
00:15:41.041 --> 00:15:43.277 line:-1 align:center
And now let me recap.


230
00:15:44.645 --> 00:15:48.982 line:-2 align:center
During our journey, I explored
the enormous amount of frameworks,


231
00:15:49.016 --> 00:15:52.819 line:-2 align:center
APIs, and tools Apple provides
to prepare, integrate,


232
00:15:52.853 --> 00:15:56.423 line:-2 align:center
and evaluate machine learning
functionality for your apps.


233
00:15:56.456 --> 00:15:59.660 line:-2 align:center
I started this journey
identifying a problem


234
00:15:59.693 --> 00:16:02.863 line:-2 align:center
that required an open source
machine learning model to solve it.


235
00:16:03.964 --> 00:16:06.800 line:-2 align:center
I found an open source model
with desired functionality


236
00:16:06,834 --> 00:16:10,304 line:-2
and made it compatible
with Apple platforms.


237
00:16:10,337 --> 00:16:13,440 line:-2
I assessed the model performance
directly on the device


238
00:16:13.473 --> 00:16:16.276 line:-1 align:center
using the new Performance Report.


239
00:16:16,310 --> 00:16:19,379 line:-2
I integrated the model in my app
using the tools


240
00:16:19.413 --> 00:16:21.548 line:-1 align:center
and the frameworks you are familiar with.


241
00:16:22.983 --> 00:16:27.154 line:-2 align:center
I optimized the model using
the new Core ML Template in Instruments.


242
00:16:27,187 --> 00:16:30,524 line:-2
With Apple tools and frameworks,
I can take care of each phase


243
00:16:30,557 --> 00:16:34,828 line:-2
of the development process
directly on Apple devices and platforms


244
00:16:34.862 --> 00:16:38.866 line:-2 align:center
from data preparation, training,
integration, and optimization.


245
00:16:40.901 --> 00:16:44.938 line:-2 align:center
Today, we just scratched the surface
of what you, the developer,


246
00:16:44.972 --> 00:16:49.042 line:-2 align:center
can achieve with the frameworks
and tools Apple provides you.


247
00:16:49.076 --> 00:16:52.446 line:-2 align:center
Please, refer to previous sessions,
linked to this,


248
00:16:52,479 --> 00:16:56,950 line:-2
for additional inspiring ideas
to bring machine learning to your apps.


249
00:16:56,984 --> 00:16:59,786 line:-1
Explore and try frameworks and tools.


250
00:16:59.820 --> 00:17:03.724 line:-2 align:center
Take advantage of the great synergy
between software and hardware


251
00:17:03.757 --> 00:17:06.026 line:-2 align:center
to accelerate
your machine learning features


252
00:17:06,059 --> 00:17:09,196 line:-2
and enrich the user experience
of your apps.


253
00:17:09.229 --> 00:17:12.332 line:-1 align:center
Have a great WWDC, and arrivederci.  ♪ ♪

