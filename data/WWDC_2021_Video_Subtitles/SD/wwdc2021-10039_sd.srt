2
00:00:00,334 --> 00:00:03,337 line:-1
[upbeat music]


3
00:00:03,370 --> 00:00:09,009 line:-1
♪ ♪


4
00:00:09.042 --> 00:00:10.744 line:-1 align:center
[Nathan] Hi, and welcome


5
00:00:10,777 --> 00:00:13,847 line:-2
to "Classify Hand Poses
and Actions with Create ML."


6
00:00:13,881 --> 00:00:15,315 line:-1
I'm Nathan Wertman.


7
00:00:15,349 --> 00:00:16,717 line:-2
And today, I'll be joined
by my colleagues,


8
00:00:16.750 --> 00:00:18.919 line:-2 align:center
Brittany Weinert
and Geppy Parziale.


9
00:00:18.952 --> 00:00:20.254 line:-1 align:center
Today, we're going to be talking


10
00:00:20.287 --> 00:00:22.089 line:-2 align:center
about classifying
hand positions.


11
00:00:22,122 --> 00:00:23,323 line:-1
But before we dig into that,


12
00:00:23,357 --> 00:00:25,325 line:-2
let's talk
about the hand itself.


13
00:00:25,359 --> 00:00:28,195 line:-2
With over two dozen bones,
joints, and muscles,


14
00:00:28.228 --> 00:00:30.564 line:-2 align:center
the hand is
an engineering marvel.


15
00:00:30.597 --> 00:00:32.900 line:-1 align:center
In spite of this complexity,


16
00:00:32,933 --> 00:00:34,434 line:-2
the hand is one
of the first tools


17
00:00:34.468 --> 00:00:36.703 line:-2 align:center
infants use to interact
with the world around them.


18
00:00:36,737 --> 00:00:38,805 line:-2
Babies learn the basics
of communication


19
00:00:38,839 --> 00:00:42,242 line:-2
using simple hand movements
before they are able to speak.


20
00:00:42,276 --> 00:00:44,444 line:-2
Once we learn to speak,
our hands continue


21
00:00:44.478 --> 00:00:45.812 line:-1 align:center
to play a role in communication.


22
00:00:45.846 --> 00:00:49.550 line:-2 align:center
They shift to adding
emphasis and expression.


23
00:00:49,583 --> 00:00:51,285 line:-2
In the past year,
our hands have become


24
00:00:51.318 --> 00:00:55.255 line:-2 align:center
more important than ever
to bring people closer.


25
00:00:55,289 --> 00:00:56,757 line:-1
In 2020,


26
00:00:56,790 --> 00:00:59,026 line:-2
the Vision Framework introduced
Hand Pose Detection,


27
00:00:59.059 --> 00:01:01.628 line:-2 align:center
which allows developers
to identify hands in the frame,


28
00:01:01.662 --> 00:01:04.164 line:-2 align:center
as well as each
of the 21 identifiable joints


29
00:01:04,198 --> 00:01:05,566 line:-1
present in the hand.


30
00:01:05,599 --> 00:01:08,035 line:-2
This is a great tool
if you are trying to identify


31
00:01:08.068 --> 00:01:10.871 line:-2 align:center
if a hand exists or where
a hand is in the frame,


32
00:01:10,904 --> 00:01:13,040 line:0
but it can be a challenge
if you're trying to classify


33
00:01:13,073 --> 00:01:15,442 align:center
what the hand is doing.


34
00:01:15.475 --> 00:01:17.177 line:-2 align:center
While the expressive
capabilities of the hand


35
00:01:17.211 --> 00:01:18.946 line:-2 align:center
are limitless,
for the rest of this session,


36
00:01:18.979 --> 00:01:21.582 line:-2 align:center
I'd like to focus on short,
single-handed poses,


37
00:01:21,615 --> 00:01:24,551 line:-1
like stop, and quiet, and peace,


38
00:01:24.585 --> 00:01:26.653 line:-2 align:center
and short,
single-handed actions,


39
00:01:26,687 --> 00:01:30,257 line:-2
like back up, go away,
and come here.


40
00:01:30,290 --> 00:01:33,293 line:-2
I just referred
to hand poses and actions.


41
00:01:33.327 --> 00:01:36.196 line:-2 align:center
How about some more
concrete definitions?


42
00:01:36.230 --> 00:01:38.866 line:-2 align:center
Well, on the one hand,
we have poses,


43
00:01:38.899 --> 00:01:41.034 line:-2 align:center
which are meaningful
as a still image.


44
00:01:41,068 --> 00:01:43,203 line:-2
Even though
these two videos are paused,


45
00:01:43,237 --> 00:01:45,973 line:-2
the intent of the subject
is clearly expressed.


46
00:01:46,006 --> 00:01:48,942 line:-1
Think of a pose like an image.


47
00:01:48,976 --> 00:01:51,411 line:-2
And on the other hand,
we have an action,


48
00:01:51,445 --> 00:01:54,248 line:-2
which requires movement
to fully express meaning.


49
00:01:54.281 --> 00:01:56.483 line:-2 align:center
The meaning of these
two actions is unclear.


50
00:01:56,517 --> 00:01:59,620 line:-2
Looking at a single frame
simply isn't sufficient.


51
00:01:59,653 --> 00:02:02,222 line:-2
But with a series
of frames over time,


52
00:02:02.256 --> 00:02:04.224 line:-1 align:center
like a video or a live photo,


53
00:02:04.258 --> 00:02:06.293 line:-2 align:center
the meaning of the action
is obvious.


54
00:02:06.326 --> 00:02:08.662 line:-2 align:center
A friendly "hello"
and "come here."


55
00:02:08,695 --> 00:02:11,398 line:-2
With that cleared up,
I'm excited to introduce


56
00:02:11.431 --> 00:02:13.300 line:-2 align:center
two new Create ML
templates this year,


57
00:02:13.333 --> 00:02:15.035 line:-1 align:center
Hand Pose Classification


58
00:02:15,068 --> 00:02:17,771 line:-1
and Hand Action Classification.


59
00:02:17,804 --> 00:02:20,274 line:-2
These new templates allow you
to train Hand Pose


60
00:02:20,307 --> 00:02:23,143 line:-2
and Action models
using either the Create ML app


61
00:02:23.177 --> 00:02:24.978 line:-1 align:center
or the Create ML framework.


62
00:02:25.012 --> 00:02:27.948 line:-2 align:center
These models are compatible
with macOS Big Sur and later,


63
00:02:27.981 --> 00:02:31.818 line:-2 align:center
as well as iOS and iPadOS 14
and later.


64
00:02:33,086 --> 00:02:35,155 line:0
And new for this year,
we've added the ability


65
00:02:35,189 --> 00:02:38,592 line:0
to train models on iOS devices
using the Create ML framework.


66
00:02:38,625 --> 00:02:41,929 line:0
You can learn more about this
in the "Build Dynamic iOS Apps


67
00:02:41,962 --> 00:02:45,265 line:0
with the Create ML Framework"
session.


68
00:02:45.299 --> 00:02:48.168 line:-2 align:center
First, I'd like to talk
about Hand Pose Classification,


69
00:02:48,202 --> 00:02:51,104 line:-2
which allows you to easily train
machine learning models


70
00:02:51,138 --> 00:02:52,639 line:-1
to classify hand positions


71
00:02:52.673 --> 00:02:54.741 line:-2 align:center
detected by
the Vision Framework.


72
00:02:54,775 --> 00:02:56,877 line:-2
Since you are in charge
of training the model,


73
00:02:56,910 --> 00:02:59,646 line:-2
you define which poses
your app should classify


74
00:02:59,680 --> 00:03:02,282 line:-1
to best suit its needs.


75
00:03:02,316 --> 00:03:04,952 line:-2
Let me give you a brief demo
of a trained model in action.


76
00:03:04.985 --> 00:03:06.553 line:-2 align:center
Starting from
a simple prototype app,


77
00:03:06,587 --> 00:03:09,790 line:-2
I was able to easily integrate
a Hand Pose Classifier model.


78
00:03:09.823 --> 00:03:12.025 line:-2 align:center
My app can now classify
hand poses


79
00:03:12,059 --> 00:03:13,460 line:-1
and show the corresponding emoji


80
00:03:13,493 --> 00:03:16,163 line:-2
and confidence
of the classified pose.


81
00:03:16.196 --> 00:03:18.665 line:-1 align:center
It classifies the hand poses One


82
00:03:18.699 --> 00:03:20.434 line:-1 align:center
as well as Two.


83
00:03:20.467 --> 00:03:22.536 line:-2 align:center
But you'll notice
that all positions


84
00:03:22.569 --> 00:03:23.871 line:-1 align:center
which it does not recognize


85
00:03:23,904 --> 00:03:25,606 line:-2
are classified as part
of the background.


86
00:03:25,639 --> 00:03:27,608 line:-2
This includes
the Open Palm pose,


87
00:03:27.641 --> 00:03:29.376 line:-2 align:center
which I'd like
to add support for.


88
00:03:29,409 --> 00:03:32,112 line:-2
I'm going to hand this model
off to my colleague Brittany


89
00:03:32,145 --> 00:03:33,580 line:-2
in a moment,
to show you how to integrate


90
00:03:33,614 --> 00:03:36,216 line:-2
a Hand Pose Classifier
model into your app.


91
00:03:36.250 --> 00:03:40.187 line:-2 align:center
Before I do, I wanna add support
for the Open Palm pose.


92
00:03:40,220 --> 00:03:42,122 line:-2
It'll be really easy,
but we should talk


93
00:03:42,155 --> 00:03:44,258 line:-2
about how a model
is trained first.


94
00:03:44,925 --> 00:03:47,060 line:-2
Just like all other
Create ML projects,


95
00:03:47,094 --> 00:03:48,595 line:-1
it's very simple to integrate


96
00:03:48,629 --> 00:03:50,330 line:-2
a Hand Pose Classifier
into your app.


97
00:03:50,364 --> 00:03:52,566 line:-1
The process has three steps.


98
00:03:52.599 --> 00:03:54.701 line:-2 align:center
Collect and categorize
training data,


99
00:03:54,735 --> 00:03:56,103 line:-1
train the model,


100
00:03:56.136 --> 00:03:58.639 line:-2 align:center
integrate the model
into your application.


101
00:03:58,672 --> 00:04:02,543 line:-2
Let's start by talking
about collecting training data.


102
00:04:02.576 --> 00:04:05.212 line:-2 align:center
For Hand Pose Classifier,
you will need images.


103
00:04:05,245 --> 00:04:08,182 line:-2
Remember, poses are
fully expressive as images.


104
00:04:08,215 --> 00:04:10,450 line:-2
These images should be
categorized into folders


105
00:04:10,484 --> 00:04:13,453 line:-2
whose name matches the pose
present in the image.


106
00:04:13,487 --> 00:04:15,856 line:-2
Here we have two poses
which we would like to identify:


107
00:04:15,889 --> 00:04:19,226 line:-2
One, Two, as well
as a Background class.


108
00:04:20.761 --> 00:04:23.497 line:-2 align:center
The Background class is
a catch-all category for poses


109
00:04:23.530 --> 00:04:26.266 line:-2 align:center
which your app doesn't care
about correctly identifying.


110
00:04:26,300 --> 00:04:29,069 line:-2
In my demo, this includes
many hand positions


111
00:04:29,102 --> 00:04:30,904 line:-1
which are not One or Two.


112
00:04:30.938 --> 00:04:33.607 line:-2 align:center
A well-defined Background
class helps your app know


113
00:04:33.640 --> 00:04:36.944 line:-2 align:center
when your user is not
making an important pose.


114
00:04:36,977 --> 00:04:38,345 line:-1
There are two types of images


115
00:04:38,378 --> 00:04:41,415 line:-2
which make up
a Background class.


116
00:04:41,448 --> 00:04:44,184 align:center
First, we have a random
assortment of hand poses


117
00:04:44,218 --> 00:04:45,752 line:0
which are not
the important poses


118
00:04:45,786 --> 00:04:47,354 line:0
you'd like your app to classify.


119
00:04:47,387 --> 00:04:49,957 line:0
These poses should include
a diverse set of skin tones,


120
00:04:49,990 --> 00:04:53,493 line:0
ages, genders,
and lighting conditions.


121
00:04:53,527 --> 00:04:55,429 line:-2
Second, we have
a set of positions


122
00:04:55.462 --> 00:04:57.264 line:-2 align:center
which are very similar
to the expressions


123
00:04:57,297 --> 00:04:58,832 line:-1
you'd like your app to classify.


124
00:04:58,866 --> 00:05:01,101 line:-2
These transitional poses
frequently occur


125
00:05:01,134 --> 00:05:02,503 line:-1
as the user is moving their hand


126
00:05:02.536 --> 00:05:05.506 line:-2 align:center
towards one of the expressions
your app cares about.


127
00:05:05.539 --> 00:05:08.442 line:-2 align:center
When I raise my hand to give
an Open Palm pose,


128
00:05:08.475 --> 00:05:11.311 line:-2 align:center
notice I transition
through several positions


129
00:05:11,345 --> 00:05:13,580 line:-2
which are similar to
but not quite


130
00:05:13.614 --> 00:05:16.116 line:-2 align:center
what I want my app to consider
an Open Palm.


131
00:05:16.149 --> 00:05:19.920 line:-2 align:center
These positions occur as I
lower my arm afterward as well.


132
00:05:19.953 --> 00:05:21.822 line:-2 align:center
This isn't unique
to the Open Palm.


133
00:05:21.855 --> 00:05:23.891 line:-2 align:center
The same type
of transitional poses occur


134
00:05:23,924 --> 00:05:26,560 line:-2
as I raise my arm
to give a Two pose,


135
00:05:26,593 --> 00:05:28,896 line:-1
as well as when I lower it.


136
00:05:28,929 --> 00:05:31,798 align:center
All of these transitional poses
should be added


137
00:05:31,832 --> 00:05:34,801 align:center
into the Background class,
along with the random poses.


138
00:05:34,835 --> 00:05:36,570 align:center
This combination
allows the model


139
00:05:36,603 --> 00:05:39,306 line:0
to properly differentiate
the poses your app cares about


140
00:05:39,339 --> 00:05:41,642 line:0
and all other background poses.


141
00:05:43,010 --> 00:05:45,379 line:-2
With the training data
collected and categorized,


142
00:05:45.412 --> 00:05:48.215 line:-2 align:center
it's now time to train our model
using the Create ML app.


143
00:05:48,248 --> 00:05:50,817 line:-1
So let's get our hands dirty.


144
00:05:50.851 --> 00:05:53.320 line:-2 align:center
I'm gonna start with
the existing Create ML project


145
00:05:53.353 --> 00:05:56.423 line:-2 align:center
I used to train the model
in my previous demo.


146
00:05:56,456 --> 00:05:58,358 line:-2
The training results
looked good,


147
00:05:58.392 --> 00:06:01.728 line:-2 align:center
so I'm expecting this model
to perform fairly well.


148
00:06:01.762 --> 00:06:04.031 line:-2 align:center
Fortunately,
the Create ML app allows you


149
00:06:04.064 --> 00:06:07.734 line:-2 align:center
to preview your model before
integrating it into your app.


150
00:06:07,768 --> 00:06:10,137 line:-2
On the Preview tab,
you'll find that,


151
00:06:10,170 --> 00:06:11,505 line:-1
for Hand Pose Classifiers,


152
00:06:11.538 --> 00:06:14.474 line:-2 align:center
we've added Live Preview
capability this release.


153
00:06:14.508 --> 00:06:17.377 line:-2 align:center
Live Preview takes advantage
of the FaceTime camera


154
00:06:17,411 --> 00:06:19,680 line:-2
to show you predictions
in real-time.


155
00:06:19,713 --> 00:06:22,149 line:-2
Using Live Preview,
we can verify that this model


156
00:06:22.182 --> 00:06:25.552 line:-2 align:center
correctly classifies
the poses One and Two.


157
00:06:25,586 --> 00:06:27,621 line:-2
And I would like it
to correctly classify


158
00:06:27,654 --> 00:06:28,956 line:-1
the Open Palm as well,


159
00:06:28,989 --> 00:06:30,691 line:-2
but it currently
classifies that pose


160
00:06:30,724 --> 00:06:32,793 line:-1
as part of the Background class.


161
00:06:32.826 --> 00:06:35.729 line:-2 align:center
In the Data Source
that I used to train this model,


162
00:06:35,762 --> 00:06:38,932 line:-2
notice that it does not include
an Open Palm class,


163
00:06:38,966 --> 00:06:42,102 line:-2
only classes for One, Two,
and Background.


164
00:06:42,135 --> 00:06:43,804 line:-1
Let's train a new model


165
00:06:43,837 --> 00:06:45,806 line:-1
which supports Open Palm now.


166
00:06:45,839 --> 00:06:49,042 line:-2
First, I'm gonna create
a new model source for this.


167
00:06:54,515 --> 00:06:57,384 line:-2
I have a data set which
includes an Open Palm class


168
00:06:57.417 --> 00:06:59.786 line:-2 align:center
that I'd like to use
for this training.


169
00:06:59,820 --> 00:07:02,823 line:-1
I will select this data set.


170
00:07:05.158 --> 00:07:06.527 line:-2 align:center
Jumping into
this new data source,


171
00:07:06,560 --> 00:07:08,996 line:-1
we find that it now includes


172
00:07:09,029 --> 00:07:10,497 line:-1
an entry for the Open Palm


173
00:07:10,531 --> 00:07:13,700 line:-2
as well as the classes
from the previous data set.


174
00:07:13,734 --> 00:07:15,169 line:-1
Back on the model source,


175
00:07:15.202 --> 00:07:17.938 line:-2 align:center
I'd like to add
a few augmentations


176
00:07:17.971 --> 00:07:19.473 line:-1 align:center
to extend the training data


177
00:07:19,506 --> 00:07:21,775 line:-1
and make my model more robust.


178
00:07:23,810 --> 00:07:26,813 line:-2
That's it.
It's time to hit Train.


179
00:07:28,882 --> 00:07:30,651 line:-1
Before the training can begin,


180
00:07:30,684 --> 00:07:33,086 line:-2
Create ML needs to do some
preliminary image processing,


181
00:07:33,120 --> 00:07:34,788 line:-1
as well as feature extraction.


182
00:07:34,821 --> 00:07:37,291 line:-2
We told Create ML to train
for 80 iterations.


183
00:07:37,324 --> 00:07:38,725 line:-1
This is a good starting point,


184
00:07:38.759 --> 00:07:40.294 line:-2 align:center
but you may need
to tweak that number


185
00:07:40,327 --> 00:07:41,862 line:-1
based on your data set.


186
00:07:41,895 --> 00:07:43,697 line:-2
This process will take
some time.


187
00:07:43.730 --> 00:07:46.133 line:-2 align:center
Fortunately,
I've already trained a model.


188
00:07:46.166 --> 00:07:48.168 line:-1 align:center
Let me grab that now.


189
00:07:48,202 --> 00:07:51,371 line:-2
Live Preview shows
that our newly-trained model


190
00:07:51,405 --> 00:07:54,908 line:-2
now correctly identifies
the Open Palm pose.


191
00:07:54.942 --> 00:07:56.910 line:-2 align:center
And just to be sure,
I'm gonna verify


192
00:07:56,944 --> 00:08:00,681 line:-2
that it continues to identify
the One and the Two pose.


193
00:08:03,684 --> 00:08:05,419 line:-1
Wasn't that easy?


194
00:08:05.452 --> 00:08:07.354 line:-2 align:center
I'm gonna send this model
to my colleague, Brittany,


195
00:08:07.387 --> 00:08:10.157 line:-2 align:center
and she will talk about
integrating it into her app.


196
00:08:10,190 --> 00:08:12,259 line:-2
[Brittany]
Thanks, Nathan, for the model.


197
00:08:12.292 --> 00:08:14.194 line:-1 align:center
Hello. I'm Brittany Weinert.


198
00:08:14,228 --> 00:08:16,463 line:-2
And I'm a member
of the Vision Framework team.


199
00:08:17.164 --> 00:08:19.833 line:-2 align:center
When I first learned
about Hand Pose Classification,


200
00:08:19.867 --> 00:08:21.602 line:-1 align:center
I immediately thought,


201
00:08:21.635 --> 00:08:24.037 line:-2 align:center
I can use this to create
special effects with my hands.


202
00:08:24.972 --> 00:08:28.342 line:-2 align:center
I know that using CoreML
to classify hand poses


203
00:08:28,375 --> 00:08:30,677 line:-2
and Vision to detect
and track the hands


204
00:08:30.711 --> 00:08:33.447 line:-2 align:center
will be the perfect technologies
to use together.


205
00:08:33.480 --> 00:08:35.716 line:-2 align:center
Let's see if we can
give ourselves superpowers.


206
00:08:35,749 --> 00:08:38,552 line:-2
I've already created
a first draft of the pipeline


207
00:08:38,585 --> 00:08:40,521 line:-2
for a demo that can do
just that.


208
00:08:40.554 --> 00:08:42.322 line:-1 align:center
Let's review it.


209
00:08:43,156 --> 00:08:45,092 line:-2
First, we're going
to have a camera


210
00:08:45,125 --> 00:08:47,127 line:-1
providing a stream of frames,


211
00:08:47.160 --> 00:08:48.996 line:-2 align:center
and we're going to use
each frame


212
00:08:49.029 --> 00:08:50.631 line:-1 align:center
for a Vision request to detect


213
00:08:50,664 --> 00:08:54,168 line:-2
the location and key points
of the hands in the frame.


214
00:08:54.201 --> 00:08:56.203 line:-1 align:center
DetectHumanHandPoseRequest


215
00:08:56,236 --> 00:08:58,338 line:-1
will be the request we're using.


216
00:08:58.372 --> 00:09:01.175 line:-2 align:center
It will return
a HumanHandPoseObservation


217
00:09:01,208 --> 00:09:03,877 line:-2
for each hand it finds
in the frame.


218
00:09:03,911 --> 00:09:04,945 line:0
The data we will send


219
00:09:04,978 --> 00:09:07,514 line:0
to the CoreML Hand Pose
Classification model


220
00:09:07,548 --> 00:09:09,583 align:center
is an MLMultiArray


221
00:09:09,616 --> 00:09:12,686 align:center
and a property on the
HumanHandPoseObservation


222
00:09:12,719 --> 00:09:15,689 line:0
called the keypointsMultiArray.


223
00:09:15,722 --> 00:09:18,959 line:0
Our Hand Pose Classifier
will then return back to us


224
00:09:18,992 --> 00:09:21,195 align:center
the top estimated
hand action label


225
00:09:21,228 --> 00:09:22,963 line:0
with its confidence score,


226
00:09:22,996 --> 00:09:24,264 align:center
which we can then use


227
00:09:24,298 --> 00:09:26,333 align:center
to determine action
within the app.


228
00:09:26.366 --> 00:09:27.501 line:-1 align:center
Now that we've gone over


229
00:09:27.534 --> 00:09:29.203 line:-2 align:center
the high-level
details of the app,


230
00:09:29,236 --> 00:09:30,904 line:-1
let's look at the code.


231
00:09:30.938 --> 00:09:33.740 line:-2 align:center
Let's start out by looking
at how to use Vision


232
00:09:33.774 --> 00:09:36.543 line:-1 align:center
to detect hands in a frame.


233
00:09:36,577 --> 00:09:39,146 line:-2
For what we want to do,
we only need one instance


234
00:09:39.179 --> 00:09:42.616 line:-2 align:center
of the
VNDetectHumanHandPoseRequest,


235
00:09:42,649 --> 00:09:45,085 line:-2
and we only need
to detect one hand,


236
00:09:45.118 --> 00:09:46.854 line:-2 align:center
so we set
the maximumHandCount to one.


237
00:09:48,055 --> 00:09:50,157 line:-2
If you set the maximumHandCount,
and there's more hands


238
00:09:50,190 --> 00:09:52,993 line:-2
than specified in the frame,
the algorithm will detect


239
00:09:53.026 --> 00:09:56.330 line:-2 align:center
the most prominent and central
hands in the frame instead.


240
00:09:56,363 --> 00:09:58,866 line:-2
The default value
for maximumHandCount is two.


241
00:09:59,766 --> 00:10:01,869 line:-2
We recommend to set
the revision here,


242
00:10:01.902 --> 00:10:04.838 line:-2 align:center
so you're not surprised
by updates to the request later.


243
00:10:04,872 --> 00:10:06,840 line:-1
But if you always want to opt in


244
00:10:06,874 --> 00:10:09,476 line:-2
for the latest algorithm
supported by the SDK


245
00:10:09.510 --> 00:10:13.247 line:-2 align:center
you're linked against,
you don't have to set it.


246
00:10:13,280 --> 00:10:16,216 line:-2
Also, as a note,
we will be doing the detection


247
00:10:16.250 --> 00:10:20.687 line:-2 align:center
on every frame retrieved
by the ARSession via ARKit,


248
00:10:20.721 --> 00:10:24.391 line:-2 align:center
but this is only one way to grab
frames from a camera feed.


249
00:10:24.424 --> 00:10:27.094 line:-2 align:center
You may use
whichever method you like.


250
00:10:27.127 --> 00:10:30.097 line:-2 align:center
AVCaptureOutput would also be
a useful alternative.


251
00:10:30,731 --> 00:10:31,899 line:-1
For every frame received,


252
00:10:31,932 --> 00:10:35,235 line:-2
we need to create
a VNImageRequestHandler,


253
00:10:35.269 --> 00:10:38.272 line:-2 align:center
which handles all the requests
on a given image.


254
00:10:38,305 --> 00:10:40,407 line:-2
The results property
on the hand pose request


255
00:10:40.440 --> 00:10:43.777 line:-2 align:center
will be populated with
VNHumanHandPoseObservations,


256
00:10:43,810 --> 00:10:45,746 line:-1
up to a max hand number of one,


257
00:10:45.779 --> 00:10:47.681 line:-2 align:center
as we specified
on the request earlier.


258
00:10:49,349 --> 00:10:51,718 align:center
If the request detects
no hand poses,


259
00:10:51,752 --> 00:10:53,520 line:0
we might want
to clear out any effects


260
00:10:53,554 --> 00:10:55,055 line:0
currently being displayed.


261
00:10:55,088 --> 00:10:57,791 line:0
Otherwise, we will have
a single hand observation.


262
00:11:00.027 --> 00:11:01.595 line:-1 align:center
Next, we want to predict


263
00:11:01.628 --> 00:11:05.432 line:-2 align:center
what our hand pose is
using our CoreML model.


264
00:11:05.465 --> 00:11:08.502 line:-2 align:center
We don't want to do
a prediction every single frame,


265
00:11:08,535 --> 00:11:12,039 line:-2
as we don't want the rendering
of the effect to be jittery.


266
00:11:12,072 --> 00:11:13,507 line:-1
Doing a prediction at intervals


267
00:11:13,540 --> 00:11:16,176 line:-2
creates a smoother
user experience.


268
00:11:16,210 --> 00:11:17,911 line:-2
When we want
to make a prediction,


269
00:11:17,945 --> 00:11:20,080 line:-2
we start by passing
the MLMultiArray


270
00:11:20,113 --> 00:11:22,549 line:-1
to the Hand Pose CoreML model,


271
00:11:22,583 --> 00:11:24,384 line:-2
and we retrieve the top label
and confidence


272
00:11:24,418 --> 00:11:26,453 line:-2
from the single
prediction returned.


273
00:11:27,421 --> 00:11:30,390 line:-2
I want to trigger changes
to the effects being displayed


274
00:11:30,424 --> 00:11:34,394 line:-2
only when a label is predicted
with a high level of confidence.


275
00:11:34,428 --> 00:11:36,196 line:-1
This also is key to protecting


276
00:11:36.230 --> 00:11:38.398 line:-2 align:center
against behavior
where the effect may switch


277
00:11:38,432 --> 00:11:41,368 line:-2
on and off too quickly
and become jittery.


278
00:11:41.401 --> 00:11:44.571 line:-2 align:center
Here, Background classification
is helping us


279
00:11:44.605 --> 00:11:47.508 line:-2 align:center
by allowing us to keep
the confidence threshold very high.


280
00:11:49,209 --> 00:11:51,378 line:0
If One is predicted
with great confidence,


281
00:11:51,411 --> 00:11:53,847 line:0
we can set the effectNode
to render.


282
00:11:53,881 --> 00:11:56,550 align:center
If One isn't predicted
with great confidence,


283
00:11:56,583 --> 00:11:58,452 line:0
I want to stop the effect
on the screen


284
00:11:58,485 --> 00:12:00,487 align:center
to match what my hand is doing.


285
00:12:00,521 --> 00:12:02,289 align:center
Let's test out what we have.


286
00:12:02.322 --> 00:12:05.192 line:-2 align:center
If I make my hand
into the One pose,


287
00:12:05,225 --> 00:12:08,028 line:-2
it should trigger
a single energy beam effect.


288
00:12:08.061 --> 00:12:09.730 line:-1 align:center
Very cool!


289
00:12:09,763 --> 00:12:12,533 line:-2
The model could tell
that I made the pose One


290
00:12:12,566 --> 00:12:14,801 line:-1
and triggered the effect.


291
00:12:14.835 --> 00:12:18.505 line:-2 align:center
Although it would be even cooler
if it followed my finger.


292
00:12:18.539 --> 00:12:19.706 line:-1 align:center
Even better if it rendered


293
00:12:19.740 --> 00:12:22.075 line:-2 align:center
at a specific point
on my finger.


294
00:12:22.109 --> 00:12:23.844 line:-2 align:center
Let's go back
to the code and change it.


295
00:12:23.877 --> 00:12:27.314 line:-2 align:center
What we need to do is feed
the key point location


296
00:12:27,347 --> 00:12:29,149 line:-2
of the hand
to the graphic asset,


297
00:12:29,183 --> 00:12:31,518 line:-2
which means using
the view to translate


298
00:12:31,552 --> 00:12:35,489 line:-2
the normalized key points
into the camera view space.


299
00:12:35,522 --> 00:12:37,090 line:-1
You may also want to consider


300
00:12:37,124 --> 00:12:39,226 line:-2
pruning which key points
you save


301
00:12:39.259 --> 00:12:41.395 line:-2 align:center
by looking
at the confidence scores.


302
00:12:41,428 --> 00:12:44,698 line:-2
Here, I only care
about the index fingertip.


303
00:12:44.731 --> 00:12:46.567 line:-2 align:center
We need to translate
the key point


304
00:12:46,600 --> 00:12:48,202 line:-1
to the coordinate space,


305
00:12:48,235 --> 00:12:50,170 line:-2
as Vision uses
normalized coordinates.


306
00:12:50.204 --> 00:12:52.339 line:-1 align:center
Also, Vision's origin point


307
00:12:52.372 --> 00:12:54.341 line:-2 align:center
is in the lower left corner
of an image,


308
00:12:54,374 --> 00:12:56,777 line:-2
so keep that in mind when
you're doing the conversion.


309
00:12:57.644 --> 00:13:00.280 line:-2 align:center
Finally,
let's save the index location,


310
00:13:00,314 --> 00:13:03,016 line:-2
and if no key point was found,
we default to nil.


311
00:13:03.050 --> 00:13:04.952 line:-2 align:center
Let's look
at the code responsible


312
00:13:04,985 --> 00:13:06,420 line:-1
for rendering the effect


313
00:13:06,453 --> 00:13:08,956 line:-2
and how I can adjust it
to follow my finger.


314
00:13:08.989 --> 00:13:11.158 line:-2 align:center
We want to find the spot
where the location


315
00:13:11,191 --> 00:13:13,594 line:-2
of the graphical object
is being set.


316
00:13:13.627 --> 00:13:15.462 line:-1 align:center
setLocationForEffects


317
00:13:15.495 --> 00:13:18.565 line:-2 align:center
is being asynchronously called
every frame.


318
00:13:18.599 --> 00:13:20.868 line:-2 align:center
As a default,
we set the effect to appear


319
00:13:20.901 --> 00:13:23.270 line:-1 align:center
at the center of the view.


320
00:13:23,303 --> 00:13:26,940 line:-2
Switching it out for the
indexFingerTipLocation CGPoint


321
00:13:26.974 --> 00:13:30.177 line:-2 align:center
from earlier, we can
get the intended effect.


322
00:13:37,251 --> 00:13:38,552 line:-1
Awesome!


323
00:13:38.585 --> 00:13:40.654 line:-1 align:center
This is starting to look cool.


324
00:13:40.687 --> 00:13:43.123 line:-1 align:center
Let's take it one more step.


325
00:13:43.156 --> 00:13:45.526 line:-2 align:center
To create a more interesting
graphical story


326
00:13:45.559 --> 00:13:46.994 line:-1 align:center
surrounding superpowers,


327
00:13:47,027 --> 00:13:48,862 line:-2
it would be good
to utilize a few more


328
00:13:48,896 --> 00:13:52,132 line:-2
of the Hand Pose Classifications
in our application.


329
00:13:52,165 --> 00:13:54,001 line:-2
In this case, we'll pick
the classification


330
00:13:54.034 --> 00:13:55.969 line:-1 align:center
Two and Open Palm.


331
00:13:56,003 --> 00:13:57,871 line:-2
I've already extended
my application


332
00:13:57,905 --> 00:14:00,741 line:-2
to take action when both
of these poses are detected.


333
00:14:00,774 --> 00:14:03,277 line:-2
Here, I'm centering
the energy beam


334
00:14:03,310 --> 00:14:05,012 line:-2
to appear at the tip of my index
finger,


335
00:14:05.045 --> 00:14:08.048 line:-2 align:center
as shown before,
for the pose One.


336
00:14:08,081 --> 00:14:12,252 line:-2
Two energy beams at the tip
of my middle and index finger


337
00:14:12.286 --> 00:14:14.154 line:-1 align:center
for the pose Two.


338
00:14:14,188 --> 00:14:15,856 line:-2
And the last energy beam
is triggered


339
00:14:15,889 --> 00:14:17,958 line:-1
by the Hand Pose Open Palm


340
00:14:17,991 --> 00:14:19,660 line:-2
and is anchored
between a key point


341
00:14:19,693 --> 00:14:21,395 line:-2
at the bottom
of my middle finger


342
00:14:21.428 --> 00:14:23.096 line:-1 align:center
and the wrist key point.


343
00:14:27,534 --> 00:14:28,702 line:-1
All right.


344
00:14:28,735 --> 00:14:30,470 line:-2
Everything Nathan and I
have introduced


345
00:14:30.504 --> 00:14:32.506 line:-2 align:center
covers the steps
of fully integrating


346
00:14:32,539 --> 00:14:35,342 line:-2
your own Hand Pose
Classification model.


347
00:14:35.375 --> 00:14:37.244 line:-2 align:center
There is one more
new feature in Vision


348
00:14:37,277 --> 00:14:38,846 line:-1
that you may find helpful,


349
00:14:38.879 --> 00:14:41.448 line:-2 align:center
so let me introduce to you
an API that might help


350
00:14:41,481 --> 00:14:44,017 line:-2
with triggering and controlling
this app's functionality.


351
00:14:44,051 --> 00:14:46,753 line:-2
Vision is introducing
a new property


352
00:14:46.787 --> 00:14:48.388 line:-2 align:center
that allows users
to differentiate


353
00:14:48.422 --> 00:14:50.224 line:-2 align:center
between the left hands
and right hands


354
00:14:50,257 --> 00:14:54,061 line:-2
on the HumanHand-
PoseObservation: chirality.


355
00:14:54,094 --> 00:14:56,530 line:-2
This is an Enum
that indicates which hand


356
00:14:56.563 --> 00:14:59.333 line:-2 align:center
the HumanHandPoseObservation
most likely is


357
00:14:59,366 --> 00:15:01,568 line:-1
and can be one of three values:


358
00:15:01,602 --> 00:15:04,404 line:-2
left hand, right hand,
and unknown.


359
00:15:04,438 --> 00:15:06,206 line:-2
You can probably guess
the meaning


360
00:15:06.240 --> 00:15:07.708 line:-1 align:center
behind the first two values,


361
00:15:07,741 --> 00:15:09,943 line:-2
but the unknown value
would only appear


362
00:15:09,977 --> 00:15:12,880 line:-2
if an older version
of the HumanHandPoseObservation


363
00:15:12,913 --> 00:15:14,581 line:-1
were deserialized,


364
00:15:14.615 --> 00:15:16.950 line:-2 align:center
and the property
had never been set.


365
00:15:16.984 --> 00:15:20.120 line:-2 align:center
As Nathan mentioned earlier,
you can get more information


366
00:15:20.153 --> 00:15:22.990 line:-2 align:center
on Vision hand pose detection
by referring back


367
00:15:23,023 --> 00:15:25,859 line:-1
to the WWDC 2020 session,


368
00:15:25.893 --> 00:15:27.961 line:-2 align:center
"Detect Body and Hand Pose
with Vision."


369
00:15:27,995 --> 00:15:31,698 line:-2
As a side note, for each hand
detected in a frame,


370
00:15:31.732 --> 00:15:33.867 line:-2 align:center
the underlying algorithm
will try to predict


371
00:15:33.901 --> 00:15:36.737 line:-2 align:center
the chirality
of each hand separately.


372
00:15:36.770 --> 00:15:40.073 line:-2 align:center
This means that the prediction
of one hand does not affect


373
00:15:40,107 --> 00:15:42,676 line:-2
the prediction
of other hands in the frame.


374
00:15:42,709 --> 00:15:44,845 line:-2
Let me show you an example
of what code


375
00:15:44,878 --> 00:15:47,014 line:-1
using chirality can look like.


376
00:15:47,047 --> 00:15:49,550 line:-2
We've already covered the setup
to creating and running


377
00:15:49,583 --> 00:15:52,452 line:-1
a VNDetectHumanHandPoseRequest.


378
00:15:52.486 --> 00:15:55.055 line:-2 align:center
After performing the request,
the observations will have


379
00:15:55,088 --> 00:15:56,890 line:-1
the Enum property chirality,


380
00:15:56,924 --> 00:15:58,725 line:-2
and you can use it
to take action on


381
00:15:58,759 --> 00:16:02,529 line:-2
or sort through Vision hand
poses observations like so.


382
00:16:03.864 --> 00:16:05.966 line:-2 align:center
Everything thus far
has been about how to use


383
00:16:05.999 --> 00:16:07.935 line:-1 align:center
Hand Pose Classification.


384
00:16:07,968 --> 00:16:11,705 line:-2
But as Nathan mentioned earlier,
Hand Action Classification


385
00:16:11.738 --> 00:16:14.074 line:-2 align:center
is another new technology
this year.


386
00:16:14.107 --> 00:16:16.443 line:-2 align:center
Here's Geppy to talk
to you about it.


387
00:16:16,476 --> 00:16:18,812 line:-1
[Geppy] Thanks, Brittany.


388
00:16:18,846 --> 00:16:20,380 line:-2
Hello, my name is
Geppy Parziale,


389
00:16:20.414 --> 00:16:22.216 line:-2 align:center
and I'm a machine learning
engineer


390
00:16:22,249 --> 00:16:23,750 line:-1
from the Create ML team.


391
00:16:23.784 --> 00:16:27.087 line:-2 align:center
In addition
to Hand Pose Classification,


392
00:16:27,120 --> 00:16:29,122 line:-1
this year, Create ML introduces


393
00:16:29,156 --> 00:16:32,492 line:-2
a new template to perform
Hand Action Classification,


394
00:16:32.526 --> 00:16:35.762 line:-2 align:center
and I'm going to show you
how to use it in your apps.


395
00:16:36.597 --> 00:16:39.967 line:-2 align:center
For this reason, I will extend
Brittany's superpowers demo


396
00:16:40.000 --> 00:16:41.435 line:-1 align:center
with some hand actions


397
00:16:41,468 --> 00:16:43,370 line:-2
and highlight some
important distinctions


398
00:16:43.403 --> 00:16:45.572 line:-2 align:center
between hand poses
and hand actions.


399
00:16:47,140 --> 00:16:48,609 line:0
Please refer to the session


400
00:16:48,642 --> 00:16:51,411 align:center
"Build an Action Classifier
with Create ML"


401
00:16:51,445 --> 00:16:53,313 align:center
from the WWDC 2020


402
00:16:53,347 --> 00:16:55,616 line:0
for additional information
and comparison


403
00:16:55,649 --> 00:16:57,684 line:0
since Hand Action
and Body Action


404
00:16:57,718 --> 00:17:00,420 align:center
are two very similar tasks.


405
00:17:00,454 --> 00:17:03,757 line:-2
But now, let me explain
what Hand Action is.


406
00:17:05,859 --> 00:17:08,529 line:-2
Hand action consists
of a sequence of hand poses


407
00:17:08.562 --> 00:17:10.631 line:-2 align:center
that the ML model needs
to analyze


408
00:17:10.664 --> 00:17:13.333 line:-1 align:center
during the motion of the hand.


409
00:17:13,367 --> 00:17:15,169 line:-2
The number of poses
within a sequence


410
00:17:15,202 --> 00:17:16,970 line:-2
should be large enough
to capture


411
00:17:17,004 --> 00:17:18,805 line:-2
the entire hand action
from start to end.


412
00:17:20,641 --> 00:17:23,777 line:-2
You use video
to capture hand actions.


413
00:17:23.810 --> 00:17:25.913 line:-2 align:center
Training
a Hand Action Classifier


414
00:17:25,946 --> 00:17:28,849 line:-2
is identical to training
a Hand Pose Classifier,


415
00:17:28,882 --> 00:17:30,617 line:-1
as Nathan showed us earlier,


416
00:17:30,651 --> 00:17:32,619 line:-1
with some minor differences.


417
00:17:33,720 --> 00:17:36,523 line:-2
While a static image
represents a hand pose,


418
00:17:36,557 --> 00:17:40,394 line:-2
videos are used to capture
and represent hand actions.


419
00:17:40,427 --> 00:17:42,763 line:-2
So to train
a Hand Action Classifier,


420
00:17:42.796 --> 00:17:44.198 line:-1 align:center
you use short videos,


421
00:17:44,231 --> 00:17:48,135 line:-2
where each video
represents hand action.


422
00:17:48,168 --> 00:17:50,637 line:-2
These videos can be
organized into folders,


423
00:17:50,671 --> 00:17:53,774 line:-2
where each folder name
represents an action class.


424
00:17:54.875 --> 00:17:56.910 line:-2 align:center
And remember to include
a Background class


425
00:17:56.944 --> 00:17:59.079 line:-2 align:center
containing videos
with action dissimilar


426
00:17:59.112 --> 00:18:01.715 line:-2 align:center
than the action you want
the classifier to recognize.


427
00:18:03,784 --> 00:18:05,419 line:-1
As alternative representation,


428
00:18:05.452 --> 00:18:08.088 line:-2 align:center
you can add all your
example video files


429
00:18:08,121 --> 00:18:09,723 line:-1
in a single folder.


430
00:18:10,858 --> 00:18:12,659 line:-2
Then, you add
an annotation file,


431
00:18:12.693 --> 00:18:15.596 line:-2 align:center
using either a CSV
or JSON format.


432
00:18:17,231 --> 00:18:20,033 line:-2
Each entry in the annotation
file represents the name


433
00:18:20,067 --> 00:18:22,636 line:-2
of the video file,
the associated class,


434
00:18:22.669 --> 00:18:25.739 line:-2 align:center
the starting and ending times
of the hand action.


435
00:18:27.307 --> 00:18:30.777 line:-2 align:center
Also, in this case, remember
to include a Background class.


436
00:18:31,979 --> 00:18:34,481 line:-2
Remember,
you train the model with videos


437
00:18:34.515 --> 00:18:36.517 line:-2 align:center
of the same length,
more or less.


438
00:18:36.550 --> 00:18:40.287 line:-2 align:center
Indeed, you provide
the action duration


439
00:18:40,320 --> 00:18:44,324 line:-2
as a training parameter and then
Create ML randomly samples


440
00:18:44,358 --> 00:18:46,193 line:-1
a consecutive number of frames


441
00:18:46.226 --> 00:18:48.795 line:-2 align:center
according to the value
you provide.


442
00:18:48,829 --> 00:18:50,831 line:-2
You can also provide
video frame rate


443
00:18:50,864 --> 00:18:53,600 line:-1
and training iterations.


444
00:18:53,634 --> 00:18:57,304 line:-2
In addition to that,
the app offers different types


445
00:18:57.337 --> 00:19:00.340 line:-2 align:center
of data augmentation
that will help the model


446
00:19:00.374 --> 00:19:03.177 line:-2 align:center
to generalize better
and increase its accuracy.


447
00:19:03,210 --> 00:19:06,914 line:-2
In particular,
Time Interpolate and Frame Drop


448
00:19:06,947 --> 00:19:10,617 line:-2
are the two augmentations added
to Hand Action Classification


449
00:19:10,651 --> 00:19:14,154 line:-2
to provide video variation
closer to real use cases.


450
00:19:15,355 --> 00:19:18,125 line:-2
So I already trained
a Hand Action Classifier


451
00:19:18.158 --> 00:19:20.260 line:-2 align:center
for my demo--
let's see it in action.


452
00:19:20,294 --> 00:19:23,797 line:-2
Well, since I'm a superhero,
I need some source of energy.


453
00:19:25.732 --> 00:19:27.301 line:-1 align:center
Here is mine.


454
00:19:27,334 --> 00:19:28,836 line:-1
Here, I use hand pose


455
00:19:28,869 --> 00:19:31,638 line:-2
to visualize my source
of energy.


456
00:19:31,672 --> 00:19:35,175 line:-2
But now, let me use my
superpower to activate it.


457
00:19:37,244 --> 00:19:40,714 line:-2
In this case,
I'm using hand action.


458
00:19:40.747 --> 00:19:42.049 line:-1 align:center
This is cool.


459
00:19:44,518 --> 00:19:47,921 line:-2
And now, Hand Pose
and Hand Action Classifier


460
00:19:47,955 --> 00:19:50,624 line:-1
are executing concurrently.


461
00:19:50.657 --> 00:19:51.792 line:-1 align:center
I'm taking advantage


462
00:19:51.825 --> 00:19:53.827 line:-2 align:center
of the new chirality feature
from Vision


463
00:19:53.861 --> 00:19:55.996 line:-2 align:center
and use my left hand
for hand poses


464
00:19:56.029 --> 00:19:58.432 line:-2 align:center
and my right hand
for hand action.


465
00:20:03,904 --> 00:20:05,305 line:-1
This is super cool.


466
00:20:05,339 --> 00:20:07,541 line:-2
So this is possible because
of the optimization


467
00:20:07.574 --> 00:20:10.277 line:-2 align:center
that Create ML applies,
a training time


468
00:20:10.310 --> 00:20:12.179 line:-2 align:center
to every model
to unleash all the power


469
00:20:12.212 --> 00:20:13.814 line:-1 align:center
of the Apple Neural Engine.


470
00:20:15,516 --> 00:20:17,651 line:-2
And now, let me go back
to my real world


471
00:20:17,684 --> 00:20:19,219 line:-1
and explain you how to integrate


472
00:20:19,253 --> 00:20:22,556 line:-2
the Create ML Hand Action
Classifier into my demo.


473
00:20:25,459 --> 00:20:28,395 line:0
Let's look at the input
of the model first.


474
00:20:28,428 --> 00:20:30,831 line:0
When you integrate
a Hand Action Classifier


475
00:20:30,864 --> 00:20:32,966 line:0
into your app,
you need to make sure


476
00:20:33,000 --> 00:20:34,434 line:0
to provide the model


477
00:20:34,468 --> 00:20:37,838 line:0
with the correct number
of expected hand poses.


478
00:20:37,871 --> 00:20:39,907 line:-2
My model is expecting
a MultiArray


479
00:20:39,940 --> 00:20:42,409 line:-1
of size 45 by 3 by 21,


480
00:20:42.442 --> 00:20:44.912 line:-2 align:center
as I can inspect
in the XCode Preview.


481
00:20:44,945 --> 00:20:48,048 line:-1
Here, 45 is the number of poses


482
00:20:48,081 --> 00:20:52,119 line:-2
the classifier needs to analyze
to recognize the action.


483
00:20:52,152 --> 00:20:54,688 line:-2
21 is the number
of joints provided


484
00:20:54,721 --> 00:20:57,524 line:-2
by the Vision Framework
for each hand.


485
00:20:57,558 --> 00:21:00,260 line:-2
Finally, 3 are
the x and y coordinates


486
00:21:00.294 --> 00:21:03.030 line:-2 align:center
and the confidence value
for each joint.


487
00:21:03,063 --> 00:21:06,133 line:-1
Where does the 45 come from?


488
00:21:06,166 --> 00:21:08,569 line:-2
That's the prediction
window size


489
00:21:08.602 --> 00:21:11.305 line:-2 align:center
and depends on the video length
and the frame rate


490
00:21:11.338 --> 00:21:13.907 line:-2 align:center
of the videos used
at the training time.


491
00:21:14,942 --> 00:21:17,644 line:-2
In my case,
I decided to train my model


492
00:21:17,678 --> 00:21:20,280 line:-1
with videos recorded at 30 fps


493
00:21:20.314 --> 00:21:22.716 line:-1 align:center
and 1.5 seconds long.


494
00:21:22,749 --> 00:21:26,687 line:-2
This means the model was trained
with 45 video frames


495
00:21:26,720 --> 00:21:29,990 line:-2
per hand action,
so during inference,


496
00:21:30,023 --> 00:21:32,926 line:-2
the model is expecting
the same number of hand poses.


497
00:21:32,960 --> 00:21:36,563 line:-2
An additional consideration
needs to be taken into account


498
00:21:36,597 --> 00:21:38,198 line:-1
with respect to the frequency


499
00:21:38,232 --> 00:21:41,435 line:-2
of the arriving hand poses
at inference time.


500
00:21:41.468 --> 00:21:45.205 line:-2 align:center
It's very important
that the rate of the hand poses


501
00:21:45,239 --> 00:21:47,274 line:-2
presented to the model
during the inference


502
00:21:47.307 --> 00:21:50.677 line:-2 align:center
matches the rate of the poses
used to train the model.


503
00:21:50,711 --> 00:21:53,780 line:-1
In my demo, I used ARKit.


504
00:21:53.814 --> 00:21:55.282 line:-1 align:center
So I had to halve the number


505
00:21:55.315 --> 00:21:57.918 line:-2 align:center
of the arriving poses
per each second,


506
00:21:57.951 --> 00:22:01.555 line:-2 align:center
since ARKit provides
frames at 60 fps,


507
00:22:01.588 --> 00:22:04.992 line:-2 align:center
and my classifier was trained
with videos at 30 fps.


508
00:22:05,926 --> 00:22:07,694 line:-2
If done otherwise,
the classifier


509
00:22:07.728 --> 00:22:09.563 line:-1 align:center
could provide wrong predictions.


510
00:22:10.831 --> 00:22:12.866 line:-2 align:center
Let's jump now into
the source code


511
00:22:12,900 --> 00:22:15,736 line:-2
to show you how
to implement this.


512
00:22:15,769 --> 00:22:20,040 line:-2
First, I use a counter
to reduce the rate of the poses


513
00:22:20.073 --> 00:22:24.811 line:-2 align:center
arriving from Vision
from 60 fps to 30 fps,


514
00:22:24.845 --> 00:22:26.113 line:-1 align:center
matching so the frame rate


515
00:22:26,146 --> 00:22:28,382 line:-2
my model is expecting
to work properly.


516
00:22:30.217 --> 00:22:32.986 line:-2 align:center
Then, I get the array
containing joints


517
00:22:33.020 --> 00:22:37.391 line:-2 align:center
and chirality
for each hand in the scene.


518
00:22:37,424 --> 00:22:40,594 line:-2
Next, I discard the key points
from my left hand


519
00:22:40,627 --> 00:22:43,263 line:-2
since, in my demo,
I use my right hand


520
00:22:43.297 --> 00:22:46.733 line:-2 align:center
to activate some of the effects
with the hand action.


521
00:22:48,836 --> 00:22:52,606 line:-2
Okay, now I need to accumulate
hand poses for the classifier.


522
00:22:52,639 --> 00:22:54,842 line:-1
To do so, I use a FIFO queue


523
00:22:54,875 --> 00:22:57,211 line:-1
and accumulate 45 hand poses


524
00:22:57.244 --> 00:22:59.980 line:-2 align:center
and make sure
the queue always contains


525
00:23:00.013 --> 00:23:03.784 line:-1 align:center
the last 45 poses.


526
00:23:03,817 --> 00:23:05,786 line:-1
The queue is initially empty.


527
00:23:05.819 --> 00:23:07.688 line:-1 align:center
When a new hand pose arrives,


528
00:23:07.721 --> 00:23:09.289 line:-1 align:center
I add it to the queue,


529
00:23:09,323 --> 00:23:12,459 line:-2
and I repeat this step
until the queue is full.


530
00:23:12.492 --> 00:23:14.394 line:-1 align:center
Once the queue is full,


531
00:23:14.428 --> 00:23:16.597 line:-2 align:center
I can start to read
its entire content.


532
00:23:16,630 --> 00:23:19,233 line:-2
I could read the queue
every time I receive


533
00:23:19,266 --> 00:23:21,635 line:-1
a new hand pose from Vision.


534
00:23:21,668 --> 00:23:23,437 line:-1
But remember, now I'm processing


535
00:23:23,470 --> 00:23:25,539 line:-1
30 frames per second.


536
00:23:25.572 --> 00:23:27.107 line:-1 align:center
And depending on the use case,


537
00:23:27.140 --> 00:23:29.176 line:-2 align:center
this could be
a waste of resources.


538
00:23:31,144 --> 00:23:33,380 line:-2
So I use another counter
to read the queue


539
00:23:33.413 --> 00:23:35.782 line:-2 align:center
after I define
number of frames.


540
00:23:37.150 --> 00:23:39.620 line:-2 align:center
You should choose
the queue sampling rate


541
00:23:39.653 --> 00:23:42.089 line:-2 align:center
as a trade-off
between the responsiveness


542
00:23:42.122 --> 00:23:45.359 line:-2 align:center
of the application
and the number of predictions


543
00:23:45,392 --> 00:23:47,394 line:-1
per second you want to obtain.


544
00:23:49,630 --> 00:23:51,932 line:-2
At this point,
I read the entire sequence


545
00:23:51.965 --> 00:23:56.603 line:-2 align:center
of 45 hand poses,
organized in MLMultiArray,


546
00:23:56.637 --> 00:24:00.941 line:-2 align:center
and input it to a classifier
to predict the hand action.


547
00:24:02,342 --> 00:24:04,478 line:-2
Then, I extract
the prediction label


548
00:24:04,511 --> 00:24:07,648 line:-1
and the confidence value.


549
00:24:07,681 --> 00:24:09,650 line:-1
Finally, if the confidence value


550
00:24:09.683 --> 00:24:12.119 line:-2 align:center
is larger than
our defined threshold,


551
00:24:12.152 --> 00:24:15.956 line:-2 align:center
I add the particle effects
to the scene.


552
00:24:15.989 --> 00:24:17.991 line:-1 align:center
So remember, when you integrate


553
00:24:18.025 --> 00:24:20.961 line:-2 align:center
the Create ML Hand Action
Classifier in your app,


554
00:24:20,994 --> 00:24:23,764 line:-2
make sure to input
the sequence of hand poses


555
00:24:23,797 --> 00:24:27,201 line:-2
at the frame rate
the model is expecting.


556
00:24:27.234 --> 00:24:29.903 line:-2 align:center
Match the same frame rate
of the video used


557
00:24:29,937 --> 00:24:32,773 line:-1
to train the classifier.


558
00:24:32,806 --> 00:24:35,843 line:-1
Use a first-in-first-out queue


559
00:24:35.876 --> 00:24:39.713 line:-2 align:center
to collect hand poses
to the model prediction.


560
00:24:39,746 --> 00:24:42,649 line:-2
Read the queue
at the right frame rate.


561
00:24:42.683 --> 00:24:45.853 line:-2 align:center
I'm looking forward to seeing
all the cool applications


562
00:24:45,886 --> 00:24:48,088 line:-2
you will build
with the hand action models


563
00:24:48,121 --> 00:24:49,723 line:-1
trained with Create ML.


564
00:24:49,756 --> 00:24:52,025 line:-1
And now, back to Nathan


565
00:24:52.059 --> 00:24:54.294 line:-2 align:center
for final consideration
and recap.


566
00:24:54,328 --> 00:24:55,762 line:-1
[Nathan] Thanks, Geppy.


567
00:24:55,796 --> 00:24:57,497 line:-2
You and Brittany did
a great job on your app.


568
00:24:57,531 --> 00:24:58,999 line:-1
I'm excited to try it out.


569
00:24:59,032 --> 00:25:00,634 line:-2
But before things
get out of hand,


570
00:25:00.667 --> 00:25:02.469 line:-2 align:center
here are a few things
you should keep in mind


571
00:25:02.503 --> 00:25:05.172 line:-2 align:center
to ensure a high-quality
experience for your users.


572
00:25:05,205 --> 00:25:08,008 line:-2
Be mindful of how far away
the hand is from the camera.


573
00:25:08.041 --> 00:25:10.210 line:-2 align:center
The distance should be
kept under 11 feet,


574
00:25:10,244 --> 00:25:12,579 line:-2
or 3 1/2 meters,
for best results.


575
00:25:12.613 --> 00:25:15.115 line:-2 align:center
It's also best to avoid
extreme lighting conditions,


576
00:25:15.148 --> 00:25:17.451 line:-1 align:center
either too dark or too light.


577
00:25:17.484 --> 00:25:20.354 line:-2 align:center
Bulky, loose, or colorful gloves
can make it difficult


578
00:25:20.387 --> 00:25:22.289 line:-2 align:center
to accurately detect
the hand pose,


579
00:25:22,322 --> 00:25:25,392 line:-2
which may affect
classification quality.


580
00:25:25.425 --> 00:25:27.261 line:-1 align:center
Like all machine learning tasks,


581
00:25:27.294 --> 00:25:29.897 line:-2 align:center
the quality and quantity
of your training data is key.


582
00:25:29.930 --> 00:25:32.499 line:-2 align:center
For the Hand Pose Classifier
shown in this session,


583
00:25:32,533 --> 00:25:34,501 line:-1
we used 500 images per class.


584
00:25:34,535 --> 00:25:38,305 line:-2
The Hand Action Classifier,
we used 100 videos per class,


585
00:25:38,338 --> 00:25:41,108 line:-2
but the data requirements
for your use case may differ.


586
00:25:41,141 --> 00:25:43,110 line:-2
What is most important
is that you collect


587
00:25:43.143 --> 00:25:45.979 line:-2 align:center
enough training data to capture
the expected variation


588
00:25:46.013 --> 00:25:48.282 line:-1 align:center
your model will see in your app.


589
00:25:48.315 --> 00:25:50.684 line:-2 align:center
Now feels like
a good time for a recap.


590
00:25:50,717 --> 00:25:52,119 line:-1
So what have we learned?


591
00:25:52.152 --> 00:25:55.088 line:-2 align:center
Well, starting in 2021,
you can build apps


592
00:25:55,122 --> 00:25:57,424 line:-2
which interpret expressions
of the human hand.


593
00:25:57,457 --> 00:25:59,860 line:-2
We discussed the differences
between the two categories


594
00:25:59,893 --> 00:26:02,829 line:-2
of hand expressions,
poses and actions.


595
00:26:02.863 --> 00:26:05.032 line:-2 align:center
We talked about
how to prepare training data,


596
00:26:05,065 --> 00:26:06,466 line:-1
including a Background class,


597
00:26:06.500 --> 00:26:08.902 line:-2 align:center
for use in the Create ML app
to train a model.


598
00:26:08,936 --> 00:26:12,606 line:-2
We talked about how to integrate
a trained model into an app.


599
00:26:12,639 --> 00:26:14,541 line:-2
And finally,
we talked about incorporating


600
00:26:14,575 --> 00:26:16,410 line:-2
multiple models
into a single app


601
00:26:16.443 --> 00:26:19.446 line:-2 align:center
and using chirality
to differentiate the hands.


602
00:26:19.479 --> 00:26:22.649 line:-2 align:center
Obviously, today's demo
only scratches the surface.


603
00:26:22,683 --> 00:26:25,052 line:-2
The Vision Framework
is a powerful technology


604
00:26:25.085 --> 00:26:28.889 line:-2 align:center
for detecting hand presence,
pose, position, and chirality.


605
00:26:28,922 --> 00:26:31,391 line:-2
Create ML is a fun
and easy way to train


606
00:26:31.425 --> 00:26:33.660 line:-2 align:center
and classify hand poses
and hand actions.


607
00:26:33,694 --> 00:26:36,196 line:-2
When used together,
they provide deep insights


608
00:26:36.230 --> 00:26:39.499 line:-2 align:center
into one of humankind's most
powerful and expressive tools,


609
00:26:39,533 --> 00:26:41,802 line:-2
and we can't wait to see
what you do with them.


610
00:26:41.835 --> 00:26:43.103 line:-1 align:center
Bye.


611
00:26:43,136 --> 00:26:46,139 align:center
[upbeat music]

