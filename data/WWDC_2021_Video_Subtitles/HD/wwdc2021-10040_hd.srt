2
00:00:00.000 --> 00:00:02.135 line:-1 position:50%
[MAC STARTUP CHIME]


3
00:00:02.135 --> 00:00:05.572 line:-1 position:50%
♪ Bass music playing ♪


4
00:00:05.572 --> 00:00:07.641 line:-1 position:50%
[KEYSTROKES]


5
00:00:07,641 --> 00:00:09,243 size:2% align:right line:0
♪


6
00:00:09,243 --> 00:00:12,045 line:-1
Sergey Kamensky: Hello,
everybody and welcome to WWDC.


7
00:00:12.045 --> 00:00:14.214 line:-1 position:50%
My name is Sergey Kamensky
and I'm a software engineer


8
00:00:14.214 --> 00:00:15.716 line:-1 position:50%
in Vision framework team.


9
00:00:15,716 --> 00:00:18,518 line:-1
The topic of today's session is
to show how Vision framework


10
00:00:18,518 --> 00:00:20,921 line:-1
can help with people analysis.


11
00:00:20.921 --> 00:00:23.690 line:-1 position:50%
Our agenda today
consists of two main items.


12
00:00:23.690 --> 00:00:25.158 line:-1 position:50%
First we're going to have
an overview


13
00:00:25,158 --> 00:00:27,895 line:-1
of people analysis technology
in Vision framework.


14
00:00:27.895 --> 00:00:28.729 line:-1 position:50%
While doing that,


15
00:00:28,729 --> 00:00:31,531 line:-1
we will be specifically
focusing on the new additions.


16
00:00:31.531 --> 00:00:34.167 line:-1 position:50%
And second, we're going
to have the in-depth review


17
00:00:34.167 --> 00:00:37.905 line:-1 position:50%
of the new person segmentation
feature.


18
00:00:37,905 --> 00:00:40,741 line:-1
Let's start with people analysis
technology first.


19
00:00:40,741 --> 00:00:42,709 line:-1
The cornerstone
of people analysis in Vision


20
00:00:42,709 --> 00:00:44,811 line:-1
is person face analysis.


21
00:00:44,811 --> 00:00:46,546 line:-1
Since the inception
of Vision framework,


22
00:00:46.546 --> 00:00:47.814 line:-1 position:50%
we've been adding
and enhancing


23
00:00:47.814 --> 00:00:49.783 line:-1 position:50%
human face analysis
capabilities.


24
00:00:49.783 --> 00:00:52.152 line:-1 position:50%
We currently offer
face detection,


25
00:00:52.152 --> 00:00:56.857 line:-1 position:50%
face landmarks detection, and
face capture quality detection.


26
00:00:56.857 --> 00:00:58.659 line:-1 position:50%
Face detection functionality
in Vision framework


27
00:00:58.659 --> 00:01:02.496 line:-1 position:50%
is exposed by the means of
DetectFaceRectanglesRequest.


28
00:01:02.496 --> 00:01:05.599 line:-1 position:50%
Our face detector offers high
precision and recall metrics,


29
00:01:05,599 --> 00:01:08,001 line:-1
it can find faces
with arbitrary orientation,


30
00:01:08,001 --> 00:01:11,872 line:-1
different sizes,
and also partially occluded.


31
00:01:11.872 --> 00:01:13.507 line:-1 position:50%
So far we have
supported occlusions


32
00:01:13,507 --> 00:01:15,909 line:-1
like glasses and hats.


33
00:01:15.909 --> 00:01:17.911 line:-1 position:50%
Now we are upgrading
our face detector


34
00:01:17.911 --> 00:01:19.713 line:-1 position:50%
to revision number three which,


35
00:01:19,713 --> 00:01:22,482 line:-1
in addition to improving
all existing great qualities,


36
00:01:22,482 --> 00:01:26,286 line:-1
can now also detect faces
covered by masks.


37
00:01:26.286 --> 00:01:27.854 line:-1 position:50%
The main function
of our face detector


38
00:01:27,854 --> 00:01:30,357 line:-1
is of course to find
face bounding box,


39
00:01:30,357 --> 00:01:33,226 line:-1
but it can also detect
face pose metrics.


40
00:01:33,226 --> 00:01:36,797 line:0
Previously, we have provided
roll and yaw metrics only.


41
00:01:36,797 --> 00:01:38,565 position:50%
Most metrics
are reported in radians


42
00:01:38,565 --> 00:01:41,535 line:0
and their values are returned
in discrete bins.


43
00:01:41,535 --> 00:01:43,036 position:50%
With the new
revision introduction,


44
00:01:43,036 --> 00:01:44,438 line:0
we're also adding a pitch metric


45
00:01:44,438 --> 00:01:47,541 position:50%
and thus completing
the full picture.


46
00:01:47,541 --> 00:01:49,443 position:50%
But we didn't stop there.


47
00:01:49,443 --> 00:01:51,211 line:0
We're also making
all three metrics


48
00:01:51,211 --> 00:01:54,181 line:0
reported in continuous space.


49
00:01:54,181 --> 00:01:56,516 line:-1
All face pose metrics
are returned as property


50
00:01:56,516 --> 00:01:58,518 line:-1
of our FaceObservation
object,


51
00:01:58,518 --> 00:02:03,223 line:-1
which is the result of executing
the DetectFaceRectanglesRequest.


52
00:02:03.223 --> 00:02:05.459 line:-1 position:50%
Let's look at the demo app
that is designed to show


53
00:02:05,459 --> 00:02:08,195 line:-1
face pose detection
functionality.


54
00:02:08,195 --> 00:02:09,329 position:50%
The app processes
camera feed


55
00:02:09,329 --> 00:02:11,164 position:50%
by running Vision face detector


56
00:02:11,164 --> 00:02:13,066 line:0
and presents face pose metrics
to the user


57
00:02:13,066 --> 00:02:16,570 line:0
after converting the results
from radians to degrees.


58
00:02:16,570 --> 00:02:18,839 line:0
For better tracking
of the metric changes,


59
00:02:18,839 --> 00:02:22,009 position:50%
the app uses red color gradient
to show when face pose metrics


60
00:02:22,009 --> 00:02:23,610 position:50%
increase in positive direction


61
00:02:23,610 --> 00:02:26,046 position:50%
and blue color gradient
to show when the metrics


62
00:02:26,046 --> 00:02:28,448 line:0
increase in negative direction.


63
00:02:28,448 --> 00:02:30,751 position:50%
In both cases,
the lighter the color is,


64
00:02:30,751 --> 00:02:33,387 line:0
the closer the metric is
to the zero position.


65
00:02:33,387 --> 00:02:35,622 line:0
The zero position for each
metric is what you would call


66
00:02:35,622 --> 00:02:37,758 position:50%
a neutral position
of a human head,


67
00:02:37,758 --> 00:02:41,395 position:50%
when the person is looking
straight -- just like this.


68
00:02:41,395 --> 00:02:44,431 line:0
As we already discussed,
we have three face pose metrics:


69
00:02:44,431 --> 00:02:46,900 position:50%
roll, yaw, and pitch.


70
00:02:46,900 --> 00:02:48,368 line:0
These terms come
from flight dynamics


71
00:02:48,368 --> 00:02:50,637 line:0
and they describe
aircraft principle axes


72
00:02:50,637 --> 00:02:53,473 line:0
with respect to the aircraft's
center of gravity.


73
00:02:53,473 --> 00:02:55,375 position:50%
The same terms have been
adopted to describe


74
00:02:55,375 --> 00:02:57,344 line:0
human head pose as well.


75
00:02:57,344 --> 00:02:58,645 position:50%
When applied to head pose --


76
00:02:58,645 --> 00:03:00,747 line:0
or as we also call it,
face pose --


77
00:03:00,747 --> 00:03:03,650 position:50%
they track human head movement
as following.


78
00:03:03,650 --> 00:03:09,423 position:50%
Roll is tracking head movement
in this direction.


79
00:03:09,423 --> 00:03:13,493 line:0
When I'm going
from the most negative


80
00:03:13,493 --> 00:03:16,363 line:0
to the most positive
values of roll,


81
00:03:16,363 --> 00:03:18,365 position:50%
you can see that
the background color changes


82
00:03:18,365 --> 00:03:22,969 line:0
from dark blue to light blue,
neutral, then light red,


83
00:03:22,969 --> 00:03:25,372 line:0
and finally dark red.


84
00:03:25,372 --> 00:03:28,075 position:50%
Similar color changes are
happening with the yaw metric,


85
00:03:28,075 --> 00:03:29,109 line:0
which is tracking the angle


86
00:03:29,109 --> 00:03:33,513 line:0
when the head
is turning right or left.


87
00:03:33,513 --> 00:03:36,416 line:0
And finally, the pitch metric
is tracking my head movement


88
00:03:36,416 --> 00:03:40,620 position:50%
when my head
is nodding up or down.


89
00:03:40,620 --> 00:03:42,956 line:0
Here you can see again
similar color changes


90
00:03:42,956 --> 00:03:46,126 line:0
when I'm going
from the most negative


91
00:03:46,126 --> 00:03:50,730 position:50%
to the most positive
end of the spectrum.


92
00:03:50,730 --> 00:03:52,999 line:-1
Face landmarks detection
is another important function


93
00:03:52.999 --> 00:03:55.268 line:-1 position:50%
of our face analysis
suite.


94
00:03:55,268 --> 00:03:57,070 line:-1
Face landmarks detection
is offered by means of


95
00:03:57.070 --> 00:03:59.039 line:-1 position:50%
DetectFaceLandmarksRequest,


96
00:03:59.039 --> 00:04:02.509 line:-1 position:50%
and the latest revision
is revision number three.


97
00:04:02,509 --> 00:04:04,544 line:-1
This revision offers
76-point constellation


98
00:04:04,544 --> 00:04:06,580 line:-1
to better represent
major face regions


99
00:04:06.580 --> 00:04:12.385 line:-1 position:50%
and also provide
accurate pupil detection.


100
00:04:12.385 --> 00:04:13.854 line:-1 position:50%
Face analysis suite


101
00:04:13.854 --> 00:04:16.690 line:-1 position:50%
also includes face capture
quality detection.


102
00:04:16,690 --> 00:04:18,391 line:-1
This holistic measure
takes into account


103
00:04:18.391 --> 00:04:21.261 line:-1 position:50%
attributes like human face
expressions, lighting,


104
00:04:21,261 --> 00:04:25,198 line:-1
occlusions, blur,
focusing, et cetera.


105
00:04:25.198 --> 00:04:28.435 line:-1 position:50%
It is exposed via
DetectFaceCaptureQualityRequest API


106
00:04:28.435 --> 00:04:30.270 line:-1 position:50%
and the latest revision
of this request


107
00:04:30,270 --> 00:04:33,173 line:-1
is revision number two.


108
00:04:33.173 --> 00:04:35.475 line:-1 position:50%
It is important to remember
that face capture quality


109
00:04:35.475 --> 00:04:38.278 line:-1 position:50%
is a comparative measure
of the same subject.


110
00:04:38,278 --> 00:04:40,313 line:-1
This feature works great,
for example,


111
00:04:40.313 --> 00:04:42.916 line:-1 position:50%
to pick the best photo
out of the photo burst series


112
00:04:42.916 --> 00:04:45.352 line:-1 position:50%
or to pick the best photo
to represent a person


113
00:04:45.352 --> 00:04:46.920 line:-1 position:50%
in the photo library.


114
00:04:46.920 --> 00:04:48.889 line:-1 position:50%
This feature is not designed
to compare faces


115
00:04:48,889 --> 00:04:52,392 line:-1
of different people.


116
00:04:52.392 --> 00:04:54.494 line:-1 position:50%
Human body analysis
is another big section of


117
00:04:54,494 --> 00:04:57,531 line:-1
the people analysis technology
offered by Vision framework.


118
00:04:57.531 --> 00:04:59.766 line:-1 position:50%
Vision provides several
functions in this area,


119
00:04:59,766 --> 00:05:02,669 line:-1
which include
human body detection,


120
00:05:02.669 --> 00:05:05.105 line:-1 position:50%
human pose detection,
and last but not least,


121
00:05:05,105 --> 00:05:07,874 line:-1
human hand pose detection.


122
00:05:07.874 --> 00:05:10.944 line:-1 position:50%
First let's take a look
at the human body detection.


123
00:05:10.944 --> 00:05:14.514 line:-1 position:50%
This function is offered via
DetectHumanRectanglesRequest,


124
00:05:14.514 --> 00:05:17.450 line:-1 position:50%
and it currently detects
human upper body only.


125
00:05:17,450 --> 00:05:21,288 line:-1
We are adding new functionality
to this request,


126
00:05:21.288 --> 00:05:25.025 line:-1 position:50%
and therefore upgrading this
revision to revision number two.


127
00:05:25.025 --> 00:05:26.092 line:-1 position:50%
With the new revision,


128
00:05:26,092 --> 00:05:28,562 line:-1
in addition to
the upper-body detection,


129
00:05:28,562 --> 00:05:31,698 line:-1
we also offer
a full-body detection.


130
00:05:31.698 --> 00:05:33.166 line:-1 position:50%
The choice between
the upper-body


131
00:05:33.166 --> 00:05:34.834 line:-1 position:50%
and the full-body detection
is controlled via


132
00:05:34.834 --> 00:05:36.870 line:-1 position:50%
the new upperBodyOnly property


133
00:05:36,870 --> 00:05:39,739 line:-1
of the
DetectHumanRectanglesRequest.


134
00:05:39,739 --> 00:05:42,275 line:-1
The default value
for this property is set to true


135
00:05:42.275 --> 00:05:45.712 line:-1 position:50%
to maintain
backwards compatibility.


136
00:05:45.712 --> 00:05:47.948 line:-1 position:50%
Human body pose detection
is offered in Vision framework


137
00:05:47,948 --> 00:05:50,984 line:-1
via
DetectHumanBodyPoseRequest.


138
00:05:50,984 --> 00:05:52,319 line:-1
Processing this request provides


139
00:05:52.319 --> 00:05:55.355 line:-1 position:50%
a collection of human body
joint locations.


140
00:05:55.355 --> 00:05:57.123 line:-1 position:50%
Revision number one
is the latest


141
00:05:57.123 --> 00:06:02.262 line:-1 position:50%
and the only available
revision of this request.


142
00:06:02.262 --> 00:06:04.965 line:-1 position:50%
Vision framework also offers
human hand pose detection


143
00:06:04.965 --> 00:06:07.567 line:-1 position:50%
as DetectHumanHandPoseRequest.


144
00:06:07.567 --> 00:06:09.603 line:-1 position:50%
Similar to human body
pose detection,


145
00:06:09,603 --> 00:06:11,504 line:-1
processing of
the hand pose request


146
00:06:11,504 --> 00:06:16,276 line:-1
returns a collection
of human hand joint locations.


147
00:06:16.276 --> 00:06:17.911 line:-1 position:50%
We're upgrading functionality
of this request


148
00:06:17,911 --> 00:06:20,914 line:-1
by adding an important property
to the resulting observation,


149
00:06:20.914 --> 00:06:23.516 line:-1 position:50%
hand chirality.


150
00:06:23,516 --> 00:06:26,586 line:-1
The new chirality property
of the HumanHandPoseObservation


151
00:06:26.586 --> 00:06:27.754 line:-1 position:50%
will contain information


152
00:06:27.754 --> 00:06:30.824 line:-1 position:50%
whether the detected hand
was left or right.


153
00:06:30,824 --> 00:06:32,425 position:50%
If you want to learn
more details


154
00:06:32,425 --> 00:06:34,794 line:0
about hand pose detection,
I recommend watching


155
00:06:34,794 --> 00:06:41,968 position:50%
the "Classify hand poses and
actions with Create ML" session.


156
00:06:41,968 --> 00:06:44,070 line:-1
This concludes our overview
of the new upgrades


157
00:06:44,070 --> 00:06:46,439 line:-1
to the people analysis
technology suite.


158
00:06:46.439 --> 00:06:49.109 line:-1 position:50%
It is time now to move to the
second topic of our session,


159
00:06:49.109 --> 00:06:53.179 line:-1 position:50%
which is person segmentation.


160
00:06:53,179 --> 00:06:55,615 line:-1
What is person segmentation?


161
00:06:55.615 --> 00:06:56.750 line:-1 position:50%
In very simple terms,


162
00:06:56.750 --> 00:07:00.253 line:-1 position:50%
it's the ability to separate
people from the scene.


163
00:07:00,253 --> 00:07:01,454 line:-1
There are numerous
applications


164
00:07:01.454 --> 00:07:04.090 line:-1 position:50%
of the person segmentation
technology nowadays.


165
00:07:04.090 --> 00:07:05.258 line:-1 position:50%
You are all familiar,
for example,


166
00:07:05,258 --> 00:07:09,029 line:-1
with virtual background feature
on video conferencing apps.


167
00:07:09.029 --> 00:07:11.031 line:-1 position:50%
It's also used
in live sport analysis,


168
00:07:11,031 --> 00:07:13,833 line:-1
autonomous driving,
and many more places.


169
00:07:13,833 --> 00:07:19,105 line:-1
Person segmentation also powers
our famous Portrait mode.


170
00:07:19,105 --> 00:07:20,807 line:-1
Person segmentation
in Vision framework


171
00:07:20,807 --> 00:07:23,810 line:-1
is a feature designed
to work with a single frame.


172
00:07:23.810 --> 00:07:25.645 line:-1 position:50%
You can use it
in streaming pipeline,


173
00:07:25,645 --> 00:07:28,448 line:-1
and it's also suitable
for the offline processing.


174
00:07:28.448 --> 00:07:30.283 line:-1 position:50%
This feature is supported
on multiple platforms


175
00:07:30,283 --> 00:07:38,525 line:-1
like macOS, iOS,
iPadOS, and tvOS.


176
00:07:38,525 --> 00:07:40,860 line:-1
Vision framework implements
semantic person segmentation,


177
00:07:40.860 --> 00:07:43.396 line:-1 position:50%
which means that
it will return a single mask


178
00:07:43.396 --> 00:07:46.399 line:-1 position:50%
for all people in the frame.


179
00:07:48.668 --> 00:07:50.570 line:-1 position:50%
Vision API
for person segmentation


180
00:07:50,570 --> 00:07:52,238 line:-1
is implemented by means of


181
00:07:52,238 --> 00:07:54,607 line:-1
GeneratePersonSegmentationRequest,


182
00:07:54.607 --> 00:07:56.376 line:-1 position:50%
This is a stateful request.


183
00:07:56,376 --> 00:07:58,845 line:-1
As opposed to traditional
requests in Vision framework,


184
00:07:58.845 --> 00:08:01.147 line:-1 position:50%
the stateful request objects
are reused throughout


185
00:08:01.147 --> 00:08:03.383 line:-1 position:50%
the entire sequence of frames.


186
00:08:03.383 --> 00:08:05.485 line:-1 position:50%
In our particular case,
using request object


187
00:08:05,485 --> 00:08:08,621 line:-1
helps with smoothing temporal
changes between the frames


188
00:08:08.621 --> 00:08:10.924 line:-1 position:50%
in fast-quality level model.


189
00:08:10.924 --> 00:08:12.959 line:-1 position:50%
Let's take a look at
the Person Segmentation API


190
00:08:12,959 --> 00:08:15,095 line:-1
offered by Vision framework.


191
00:08:15.095 --> 00:08:16.763 line:-1 position:50%
This API follows
an already familiar


192
00:08:16,763 --> 00:08:18,665 line:-1
and established pattern.


193
00:08:18.665 --> 00:08:20.467 line:-1 position:50%
Create a request,


194
00:08:20,467 --> 00:08:22,369 line:-1
create a request handler,


195
00:08:22,369 --> 00:08:24,704 line:-1
process your request
with the request handler,


196
00:08:24,704 --> 00:08:27,941 line:-1
and finally,
review the results.


197
00:08:27.941 --> 00:08:29.242 line:-1 position:50%
Default initialization of


198
00:08:29,242 --> 00:08:31,044 line:-1
GeneratePersonSegmentationRequest
object


199
00:08:31.044 --> 00:08:34.414 line:-1 position:50%
is equivalent to setting
revision, qualityLevel,


200
00:08:34.414 --> 00:08:37.917 line:-1 position:50%
and outputPixelFormat properties
to their default values.


201
00:08:37,917 --> 00:08:41,187 line:-1
Let's review all properties
one by one.


202
00:08:41.187 --> 00:08:43.356 line:-1 position:50%
First is the revision property.


203
00:08:43.356 --> 00:08:46.025 line:-1 position:50%
Here we set the revision
to revision number one.


204
00:08:46,025 --> 00:08:48,395 line:-1
This is the default
and the only available revision,


205
00:08:48.395 --> 00:08:51.131 line:-1 position:50%
since we're dealing
with the new request type.


206
00:08:51,131 --> 00:08:53,500 line:-1
Even though there is technically
no choice here today,


207
00:08:53.500 --> 00:08:55.435 line:-1 position:50%
we always recommend
to set explicitly


208
00:08:55,435 --> 00:08:58,505 line:-1
to guarantee deterministic
behavior in the future.


209
00:08:58.505 --> 00:09:00.807 line:-1 position:50%
This is because if new revisions
are introduced,


210
00:09:00.807 --> 00:09:02.976 line:-1 position:50%
the default will also
change to represent


211
00:09:02.976 --> 00:09:06.112 line:-1 position:50%
the latest available revision.


212
00:09:06,112 --> 00:09:09,015 line:-1
Second is
the qualityLevel property.


213
00:09:09,015 --> 00:09:11,718 line:-1
Vision API offers
three different levels:


214
00:09:11.718 --> 00:09:13.987 line:-1 position:50%
accurate, which is also
the default one;


215
00:09:13,987 --> 00:09:16,256 line:-1
balanced; and fast.


216
00:09:16,256 --> 00:09:18,591 line:0
As far as the use cases go,
we recommend using


217
00:09:18,591 --> 00:09:22,128 line:0
accurate level
for computational photography.


218
00:09:22,128 --> 00:09:24,464 position:50%
This is the use case where
you would like to achieve


219
00:09:24,464 --> 00:09:26,266 position:50%
the highest possible quality


220
00:09:26,266 --> 00:09:29,002 line:0
and are typically
not limited in time.


221
00:09:29,002 --> 00:09:31,504 position:50%
Using similar logic,
balanced level is recommended


222
00:09:31,504 --> 00:09:33,740 line:0
for video frame-by-frame
segmentation


223
00:09:33,740 --> 00:09:37,844 line:0
and fast for streaming
processing.


224
00:09:37.844 --> 00:09:40.580 line:-1 position:50%
The third property
is the output mask format.


225
00:09:40.580 --> 00:09:43.116 line:-1 position:50%
We are going to review
the resulting mask in details,


226
00:09:43.116 --> 00:09:45.685 line:-1 position:50%
but here I would like
to mention that, as a client,


227
00:09:45.685 --> 00:09:47.454 line:-1 position:50%
you can specify which format


228
00:09:47,454 --> 00:09:50,256 line:-1
the resulting mask
will be returned in.


229
00:09:50,256 --> 00:09:52,926 line:0
There are three choices here:
unsigned 8-bit integer mask


230
00:09:52,926 --> 00:09:55,995 position:50%
with a typical 0 to 255
quantization range,


231
00:09:55,995 --> 00:09:57,864 line:0
and two floating point
mask formats --


232
00:09:57,864 --> 00:10:00,099 position:50%
one with 32-bit full precision


233
00:10:00,099 --> 00:10:02,268 position:50%
and another
with 16-bit half precision.


234
00:10:02,268 --> 00:10:05,104 position:50%
The 16-bit half precision
is intended to provide you with


235
00:10:05,104 --> 00:10:07,106 position:50%
a reduced memory
floating point format


236
00:10:07,106 --> 00:10:08,508 position:50%
that can be inserted directly


237
00:10:08,508 --> 00:10:14,414 position:50%
into further GPU-based
processing with Metal.


238
00:10:14,414 --> 00:10:16,683 line:-1
So far we have learned
how to create, configure,


239
00:10:16,683 --> 00:10:19,052 line:-1
and execute our
person segmentation request.


240
00:10:19.052 --> 00:10:22.655 line:-1 position:50%
It is time now
to look at the results.


241
00:10:22.655 --> 00:10:25.225 line:-1 position:50%
The result of processing
person segmentation request


242
00:10:25.225 --> 00:10:28.394 line:-1 position:50%
come in form of
PixelBufferObservation object.


243
00:10:28.394 --> 00:10:30.964 line:-1 position:50%
PixelBufferObservation
derives from observation


244
00:10:30,964 --> 00:10:35,301 line:-1
and it adds an important
pixelBuffer property.


245
00:10:35,301 --> 00:10:37,904 position:50%
The actual CVPixelBuffer object
stored in this property


246
00:10:37,904 --> 00:10:41,307 line:0
has the same pixel format as our
person segmentation request


247
00:10:41,307 --> 00:10:45,645 position:50%
was configured with.


248
00:10:45,645 --> 00:10:47,313 line:-1
Processing of
person segmentation request


249
00:10:47,313 --> 00:10:49,916 line:-1
will produce
a segmentation mask.


250
00:10:49.916 --> 00:10:52.452 line:-1 position:50%
Let's look at the original image


251
00:10:52,452 --> 00:10:54,487 line:-1
and three different
quality level masks


252
00:10:54.487 --> 00:10:57.724 line:-1 position:50%
generated by executing
person segmentation request.


253
00:10:57.724 --> 00:11:02.562 line:-1 position:50%
Fast, balanced, and accurate.


254
00:11:02.562 --> 00:11:05.498 line:-1 position:50%
Let's zoom in to look
at the details of each mask.


255
00:11:05,498 --> 00:11:08,268 line:-1
As expected, when we go
from fast to balanced


256
00:11:08,268 --> 00:11:09,736 line:-1
and eventually to accurate,


257
00:11:09,736 --> 00:11:11,404 line:-1
the quality of the mask
increases


258
00:11:11.404 --> 00:11:15.208 line:-1 position:50%
and we'll start seeing
more and more details.


259
00:11:15,208 --> 00:11:17,577 position:50%
Now let's examine
the different mask levels


260
00:11:17,577 --> 00:11:21,314 position:50%
as a function of quality
versus performance.


261
00:11:21,314 --> 00:11:23,716 line:0
When we move
from fast to balanced,


262
00:11:23,716 --> 00:11:27,353 position:50%
and eventually to accurate,
the quality of the mask goes up


263
00:11:27,353 --> 00:11:30,390 line:0
but so does the resource usage.


264
00:11:30.390 --> 00:11:32.425 line:-1 position:50%
The dynamic range,
mask resolution,


265
00:11:32.425 --> 00:11:35.228 line:-1 position:50%
memory consumption,
processing time all go up


266
00:11:35,228 --> 00:11:36,996 line:-1
when the mask quality
increases.


267
00:11:36.996 --> 00:11:38.431 line:-1 position:50%
This represents a trade-off


268
00:11:38.431 --> 00:11:40.366 line:-1 position:50%
between the quality
of the segmentation mask


269
00:11:40,366 --> 00:11:46,272 line:-1
and the resource consumption
needed to compute the mask.


270
00:11:46,272 --> 00:11:48,775 line:-1
So you already know everything
about mask generation


271
00:11:48.775 --> 00:11:50.410 line:-1 position:50%
and their properties.


272
00:11:50.410 --> 00:11:54.314 line:-1 position:50%
What can you actually do
with the masks?


273
00:11:54.314 --> 00:11:56.549 line:-1 position:50%
Let's start with three images.


274
00:11:56,549 --> 00:11:58,885 line:-1
The input image,
the segmentation mask


275
00:11:58.885 --> 00:12:01.588 line:-1 position:50%
that was obtained by processing
the input image,


276
00:12:01,588 --> 00:12:03,590 line:-1
and the background image.


277
00:12:03,590 --> 00:12:06,259 line:-1
What we would like to do
is to replace the background


278
00:12:06,259 --> 00:12:09,228 line:-1
in the original image in the
area outside of the mask region


279
00:12:09,228 --> 00:12:12,865 line:-1
with the background
from a different image.


280
00:12:12.865 --> 00:12:15.034 line:-1 position:50%
When you execute
such blending operation,


281
00:12:15,034 --> 00:12:17,804 line:-1
we end up with the young man
in the original image


282
00:12:17,804 --> 00:12:22,709 line:-1
being transported from the beach
promenade to the forest.


283
00:12:22.709 --> 00:12:26.045 line:-1 position:50%
How does this blending sequence
look like in the code?


284
00:12:26.045 --> 00:12:28.781 line:-1 position:50%
First let's assume we have done
all relevant processing


285
00:12:28,781 --> 00:12:31,150 line:-1
and already have
our three images:


286
00:12:31.150 --> 00:12:35.088 line:-1 position:50%
the input image, the mask,
and the background.


287
00:12:35.088 --> 00:12:37.490 line:-1 position:50%
Now we need to scale both
the mask and the background


288
00:12:37,490 --> 00:12:40,426 line:-1
to the size
of the original image.


289
00:12:40,426 --> 00:12:42,895 position:50%
Then we will create
and initialize


290
00:12:42,895 --> 00:12:45,331 position:50%
the Core Image blending filter.


291
00:12:45,331 --> 00:12:47,333 line:0
You probably noticed that
I created my blending filter


292
00:12:47,333 --> 00:12:48,801 position:50%
with the red mask.


293
00:12:48,801 --> 00:12:51,204 line:0
This is because
when CIImage is initialized


294
00:12:51,204 --> 00:12:52,972 line:0
with one component
PixelBuffer --


295
00:12:52,972 --> 00:12:54,807 line:0
as all our masks are --


296
00:12:54,807 --> 00:12:58,378 line:0
it creates an object with
the red channel by default.


297
00:12:58,378 --> 00:13:04,350 position:50%
Finally, we perform the blending
operation to get our results.


298
00:13:04.350 --> 00:13:06.886 line:-1 position:50%
Let's take a look how we can use
person segmentation feature


299
00:13:06.886 --> 00:13:09.589 line:-1 position:50%
in Vision framework.


300
00:13:09.589 --> 00:13:10.823 line:-1 position:50%
Our second demo app,


301
00:13:10,823 --> 00:13:12,258 line:-1
which is available
for downloading,


302
00:13:12.258 --> 00:13:14.193 line:-1 position:50%
combines face pose
metric detection


303
00:13:14,193 --> 00:13:16,729 line:-1
with the new person
segmentation capability.


304
00:13:16.729 --> 00:13:18.831 line:-1 position:50%
The app processes camera feed
by running face detection


305
00:13:18,831 --> 00:13:20,433 line:-1
and person segmentation.


306
00:13:20,433 --> 00:13:22,502 line:-1
Then that takes up
the end segmentation mask


307
00:13:22.502 --> 00:13:25.171 line:-1 position:50%
and uses it to replace
the background in the area


308
00:13:25,171 --> 00:13:28,307 line:-1
outside of the mask pixels
with a different color.


309
00:13:28.307 --> 00:13:30.076 line:-1 position:50%
The decision of what
background color to use


310
00:13:30.076 --> 00:13:31.544 line:-1 position:50%
comes from the combination
of values


311
00:13:31.544 --> 00:13:36.049 line:-1 position:50%
for roll, yaw, and pitch
at any given point in time.


312
00:13:36,049 --> 00:13:38,885 line:-1
I'm currently located in a room
with a table and chairs,


313
00:13:38.885 --> 00:13:41.487 line:-1 position:50%
and the demo app shows
my segmented silhouette


314
00:13:41.487 --> 00:13:43.956 line:-1 position:50%
over the new background,
which is the color mix


315
00:13:43,956 --> 00:13:46,192 line:-1
corresponding to
my head position.


316
00:13:46.192 --> 00:13:49.228 line:-1 position:50%
Let's see if it tracks
roll, yaw, and pitch changes.


317
00:13:49.228 --> 00:13:56.102 line:-1 position:50%
When I turn my head like this,
roll is a major contributor


318
00:13:56.102 --> 00:13:58.404 line:-1 position:50%
to the background
color mix decision.


319
00:13:58,404 --> 00:14:02,408 line:-1
When I turn my head
left and right,


320
00:14:02.408 --> 00:14:04.343 line:-1 position:50%
yaw becomes
the major contributor.


321
00:14:04.343 --> 00:14:08.047 line:-1 position:50%
And finally,
nodding my head up and down


322
00:14:08.047 --> 00:14:12.151 line:-1 position:50%
makes pitch
the major contributor.


323
00:14:12.151 --> 00:14:13.753 line:-1 position:50%
Vision framework
is not the only place


324
00:14:13.753 --> 00:14:15.955 line:-1 position:50%
that offers
person segmentation API.


325
00:14:15.955 --> 00:14:17.490 line:-1 position:50%
There are several other
frameworks that offer


326
00:14:17.490 --> 00:14:20.793 line:-1 position:50%
similar functionality powered
by the same technology.


327
00:14:20,793 --> 00:14:23,362 line:-1
Let's take a brief look
at each one of them.


328
00:14:23,362 --> 00:14:26,299 line:-1
First is the AVFoundation.


329
00:14:26.299 --> 00:14:29.068 line:-1 position:50%
AVFoundation can return
a person segmentation mask


330
00:14:29.068 --> 00:14:30.970 line:-1 position:50%
in some of the newer-generation
devices


331
00:14:30.970 --> 00:14:33.406 line:-1 position:50%
during photo capture session.


332
00:14:33,406 --> 00:14:34,807 line:0
The segmentation mask
is returned


333
00:14:34,807 --> 00:14:39,479 line:0
via PortraitEffectsMatte
property of AVCapturePhoto.


334
00:14:39,479 --> 00:14:41,347 position:50%
In order to get it,
you will first need to check


335
00:14:41,347 --> 00:14:42,415 line:0
if it's supported;


336
00:14:42,415 --> 00:14:45,818 position:50%
and if it is,
enable the delivery of it.


337
00:14:45.818 --> 00:14:47.954 line:-1 position:50%
The second framework that
offers person segmentation API


338
00:14:47.954 --> 00:14:50.156 line:-1 position:50%
is ARKit.


339
00:14:50.156 --> 00:14:53.493 line:-1 position:50%
This functionality is supported
on A12 Bionic and later devices,


340
00:14:53,493 --> 00:14:56,429 line:-1
and is generated when processing
camera feed.


341
00:14:56,429 --> 00:14:57,730 line:0
The segmentation mask
is returned


342
00:14:57,730 --> 00:15:01,634 position:50%
via segmentationBuffer property
of ARFrame.


343
00:15:01,634 --> 00:15:03,102 line:0
Before attempting
to retrieve it,


344
00:15:03,102 --> 00:15:05,371 position:50%
you need to check if it's
supported by examining


345
00:15:05,371 --> 00:15:07,173 line:0
supportsFrameSemantics
property


346
00:15:07,173 --> 00:15:12,478 line:0
of ARWorldTrackingConfiguration
class.


347
00:15:12,478 --> 00:15:14,380 line:-1
The third framework
is Core Image.


348
00:15:14.380 --> 00:15:15.915 line:-1 position:50%
Core Image offers a thin wrapper


349
00:15:15.915 --> 00:15:18.317 line:-1 position:50%
on top of Vision person
segmentation API,


350
00:15:18.317 --> 00:15:19.952 line:-1 position:50%
so you can perform
the entire use case


351
00:15:19.952 --> 00:15:22.922 line:-1 position:50%
within the Core Image domain.


352
00:15:22.922 --> 00:15:25.258 line:-1 position:50%
Let's take a look now
at how person segmentation


353
00:15:25.258 --> 00:15:28.661 line:-1 position:50%
can be implemented
using Core Image API.


354
00:15:28.661 --> 00:15:30.129 line:-1 position:50%
We will start
with logging an image


355
00:15:30,129 --> 00:15:32,465 line:-1
to perform segmentation on.


356
00:15:32,465 --> 00:15:35,635 line:0
Then we will create
a person segmentation CIFilter,


357
00:15:35,635 --> 00:15:37,804 line:0
assign an inputImage to it,


358
00:15:37,804 --> 00:15:43,176 line:0
and execute the filter
to get our segmentation mask.


359
00:15:43.176 --> 00:15:44.677 line:-1 position:50%
We have just reviewed
multiple versions


360
00:15:44.677 --> 00:15:47.580 line:-1 position:50%
of person segmentation APIs
and Apple SDKs.


361
00:15:47.580 --> 00:15:51.384 line:-1 position:50%
Let's summarize to see
where each one could be used.


362
00:15:51,384 --> 00:15:54,120 line:-1
AVFoundation is available
on some of iOS devices


363
00:15:54.120 --> 00:15:56.556 line:-1 position:50%
with AVCaptureSession.


364
00:15:56.556 --> 00:15:58.157 line:-1 position:50%
If you have a capture session
running,


365
00:15:58,157 --> 00:16:00,126 line:-1
this will be your choice.


366
00:16:00,126 --> 00:16:01,961 line:-1
If you're developing
an ARKit app,


367
00:16:01.961 --> 00:16:03.462 line:-1 position:50%
you should already
have an AR session


368
00:16:03,462 --> 00:16:06,132 line:-1
where you can get
your segmentation mask from.


369
00:16:06.132 --> 00:16:10.937 line:-1 position:50%
In this case, ARKit API is
the recommended one to use.


370
00:16:10,937 --> 00:16:12,972 position:50%
Vision API is available
on multiple platforms


371
00:16:12,972 --> 00:16:16,576 position:50%
for online and offline
single-frame processing.


372
00:16:16,576 --> 00:16:18,945 position:50%
And finally, Core Image
offers a thin wrapper


373
00:16:18,945 --> 00:16:21,247 position:50%
around Vision API,
which is a convenient option


374
00:16:21,247 --> 00:16:24,317 line:0
if you want to stay within
the Core Image domain.


375
00:16:24,317 --> 00:16:26,285 line:-1
As any algorithm,
person segmentation


376
00:16:26.285 --> 00:16:28.721 line:-1 position:50%
has its best practices --
or in other words,


377
00:16:28,721 --> 00:16:31,557 line:-1
the set of conditions
where it works best.


378
00:16:31,557 --> 00:16:33,826 line:-1
If you're planning to use
person segmentation feature,


379
00:16:33,826 --> 00:16:35,228 line:-1
your app will perform better


380
00:16:35,228 --> 00:16:37,730 line:-1
if you try to follow
these rules.


381
00:16:37,730 --> 00:16:41,634 line:-1
First, you should try to segment
up to four people in the scene


382
00:16:41.634 --> 00:16:43.469 line:-1 position:50%
where all people
are mostly visible,


383
00:16:43.469 --> 00:16:45.972 line:-1 position:50%
while maybe
with natural occlusions.


384
00:16:45,972 --> 00:16:47,974 line:-1
Second, the height
of each person


385
00:16:47.974 --> 00:16:50.243 line:-1 position:50%
should be at least
half of the image height,


386
00:16:50.243 --> 00:16:53.312 line:-1 position:50%
ideally with good contrast
compared to the background.


387
00:16:53,312 --> 00:16:56,415 line:-1
And third, we also recommend
you to avoid ambiguities


388
00:16:56.415 --> 00:16:58.885 line:-1 position:50%
like statues,
pictures of people,


389
00:16:58.885 --> 00:17:02.188 line:-1 position:50%
people at far distance.


390
00:17:02.188 --> 00:17:03.823 line:-1 position:50%
This concludes our session.


391
00:17:03,823 --> 00:17:07,226 line:-1
Let's take a brief look
at what we have learned today.


392
00:17:07,226 --> 00:17:08,728 line:-1
First, we had an overview


393
00:17:08.728 --> 00:17:11.297 line:-1 position:50%
of the person analysis
technology in Vision framework


394
00:17:11,297 --> 00:17:14,500 line:-1
while focusing on the upgrades
like masked face detection,


395
00:17:14.500 --> 00:17:15.968 line:-1 position:50%
adding face pitch metric,


396
00:17:15.968 --> 00:17:19.872 line:-1 position:50%
and making all face pose metrics
reported in continuous space.


397
00:17:19,872 --> 00:17:22,241 line:-1
We also introduced
new hand chirality metric


398
00:17:22,241 --> 00:17:25,244 line:-1
to the human hand pose
detection.


399
00:17:25.244 --> 00:17:27.546 line:-1 position:50%
In the second part,
we took a deep dive into


400
00:17:27,546 --> 00:17:30,950 line:-1
the new person segmentation API
added to Vision framework.


401
00:17:30,950 --> 00:17:33,786 line:-1
We also looked into other APIs
offering similar functionality


402
00:17:33,786 --> 00:17:37,256 line:-1
and provided the guidance
where each one could be used.


403
00:17:37,256 --> 00:17:39,058 line:-1
I really hope that by watching
this session,


404
00:17:39,058 --> 00:17:41,527 line:-1
you have learned new tools
for developing your apps


405
00:17:41.527 --> 00:17:44.063 line:-1 position:50%
and are really eager
to try them right away.


406
00:17:44.063 --> 00:17:45.231 line:-1 position:50%
Before we finish today,


407
00:17:45,231 --> 00:17:47,099 line:-1
I would like to thank you
for watching,


408
00:17:47,099 --> 00:17:48,067 line:-1
wish you good luck,


409
00:17:48.067 --> 00:17:50.903 line:-1 position:50%
and have a great
rest of the WWDC.


410
00:17:50,903 --> 00:17:55,074 size:2% align:right position:90%
♪

