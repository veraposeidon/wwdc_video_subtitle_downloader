2
00:00:00,000 --> 00:00:01,935 line:-1
[MAC STARTUP CHIME]


3
00:00:01.935 --> 00:00:05.739 line:-1 position:50%
♪ Bass music playing ♪


4
00:00:05,739 --> 00:00:07,207 line:-1
[KEYSTROKES]


5
00:00:07,207 --> 00:00:09,943 size:2% position:89% align:center
♪


6
00:00:09.943 --> 00:00:11.478 line:-1 position:50%
Rokhini Prabhu:
Hello, and welcome


7
00:00:11,478 --> 00:00:14,181 line:-1
to "Swift Concurrency
Behind the Scenes."


8
00:00:14,181 --> 00:00:18,018 line:-1
My name is Rokhini, and I work
on the Darwin Runtime team.


9
00:00:18,018 --> 00:00:22,456 line:-1
Today, my colleague Varun and I
are very excited to talk to you


10
00:00:22.456 --> 00:00:24.858 line:-1 position:50%
about some of
the underlying nuances


11
00:00:24.858 --> 00:00:26.994 line:-1 position:50%
around Swift concurrency.


12
00:00:28,996 --> 00:00:31,932 line:0
This is an advanced talk
which builds upon


13
00:00:31,932 --> 00:00:35,269 position:50%
some of the earlier talks
on Swift concurrency.


14
00:00:35,269 --> 00:00:39,139 line:0
If you are unfamiliar with
the concepts of async/await,


15
00:00:39,139 --> 00:00:41,675 line:0
structured concurrency,
and actors


16
00:00:41,675 --> 00:00:46,613 position:50%
I encourage you to watch
these others talks first.


17
00:00:46,613 --> 00:00:49,783 line:-1
In the previous talks on Swift
concurrency you've learned about


18
00:00:49,783 --> 00:00:53,020 line:-1
the various language features
available this year


19
00:00:53,020 --> 00:00:56,423 line:-1
native to Swift
and about how to use them.


20
00:00:56,423 --> 00:00:59,059 line:-1
In this talk, we will be diving
deeper to understand


21
00:00:59,059 --> 00:01:02,095 line:-1
why these primitives
are designed the way they are,


22
00:01:02.095 --> 00:01:04.097 line:-1 position:50%
not only for language safety


23
00:01:04.097 --> 00:01:07.501 line:-1 position:50%
but also for performance
and efficiency.


24
00:01:07,501 --> 00:01:10,537 line:-1
As you experiment
and adopt Swift concurrency


25
00:01:10,537 --> 00:01:13,674 line:-1
in your own apps,
we hope this talk will give you


26
00:01:13,674 --> 00:01:18,312 line:-1
a better mental model of how to
reason about Swift concurrency


27
00:01:18,312 --> 00:01:22,449 line:-1
as well how it interfaces with
existing threading libraries


28
00:01:22,449 --> 00:01:25,919 line:-1
like Grand Central Dispatch.


29
00:01:25.919 --> 00:01:28.855 line:-1 position:50%
We're going to discuss
a few things today.


30
00:01:28,855 --> 00:01:31,224 line:-1
First, we'll talk about
the threading model


31
00:01:31,224 --> 00:01:33,060 line:-1
behind Swift concurrency


32
00:01:33,060 --> 00:01:36,530 line:-1
and contrast it with
Grand Central Dispatch.


33
00:01:36,530 --> 00:01:38,932 line:-1
We'll talk about how
we've taken advantage


34
00:01:38,932 --> 00:01:41,101 line:-1
of concurrency
language features


35
00:01:41,101 --> 00:01:43,737 line:-1
to build a new
thread pool for Swift,


36
00:01:43.737 --> 00:01:47.641 line:-1 position:50%
thereby achieving better
performance and efficiency.


37
00:01:47,641 --> 00:01:49,443 line:-1
Lastly, in this section,


38
00:01:49,443 --> 00:01:52,546 line:-1
we'll touch on considerations
you need to keep in mind


39
00:01:52.546 --> 00:01:56.850 line:-1 position:50%
when porting your code
to using Swift concurrency.


40
00:01:56,850 --> 00:01:59,653 line:-1
Varun will then talk
about synchronization


41
00:01:59,653 --> 00:02:02,622 line:-1
in Swift concurrency via actors.


42
00:02:02,622 --> 00:02:05,592 line:-1
We'll talk how actors work
under the hood,


43
00:02:05.592 --> 00:02:08.729 line:-1 position:50%
how they compare to existing
synchronization primitives


44
00:02:08,729 --> 00:02:10,964 line:-1
you already may be
familiar with --


45
00:02:10.964 --> 00:02:12.966 line:-1 position:50%
like serial dispatch queues --


46
00:02:12.966 --> 00:02:15.335 line:-1 position:50%
and finally,
some things to keep in mind


47
00:02:15,335 --> 00:02:17,604 line:-1
when writing code with actors.


48
00:02:17,604 --> 00:02:22,142 line:-1
We have a lot of ground to cover
today so let's dive right in.


49
00:02:22,142 --> 00:02:24,678 line:-1
In our discussion today
about threading models,


50
00:02:24.678 --> 00:02:28.215 line:-1 position:50%
we'll start by taking a look
at an example app


51
00:02:28,215 --> 00:02:30,984 line:-1
written with the technologies
available today


52
00:02:30,984 --> 00:02:33,120 line:-1
like Grand Central Dispatch.


53
00:02:33,120 --> 00:02:36,256 line:-1
We will then see how
the same application fares


54
00:02:36,256 --> 00:02:39,793 line:-1
when rewritten
with Swift concurrency.


55
00:02:39,793 --> 00:02:43,797 line:-1
Suppose I wanted to write
my own news feed reader app.


56
00:02:43.797 --> 00:02:46.166 line:-1 position:50%
Let's talk about what
the high-level components


57
00:02:46.166 --> 00:02:48.835 line:-1 position:50%
of my application might be.


58
00:02:48,835 --> 00:02:50,771 line:-1
My app will have a main thread


59
00:02:50.771 --> 00:02:53.373 line:-1 position:50%
that will be driving
the user interface.


60
00:02:53.373 --> 00:02:56.777 line:-1 position:50%
I will have a database keeping
track of the news feeds


61
00:02:56,777 --> 00:02:59,079 line:-1
that the user is subscribed to,


62
00:02:59.079 --> 00:03:02.883 line:-1 position:50%
and finally, a subsystem
to handle the networking logic


63
00:03:02.883 --> 00:03:07.054 line:-1 position:50%
to fetch the latest contents
from the feeds.


64
00:03:07.054 --> 00:03:09.890 line:-1 position:50%
Let's consider how
one might structure this app


65
00:03:09.890 --> 00:03:12.726 line:-1 position:50%
with Grand Central Dispatch
queues.


66
00:03:12.726 --> 00:03:16.630 line:-1 position:50%
Let's suppose that the user has
asked to see the latest news.


67
00:03:16,630 --> 00:03:21,001 line:-1
On the main thread, we will
handle the user event gesture.


68
00:03:21,001 --> 00:03:24,004 line:-1
From here, we will
dispatch async the request


69
00:03:24,004 --> 00:03:28,442 line:-1
onto a serial queue that handles
the database operations.


70
00:03:28,442 --> 00:03:30,944 line:-1
The reason for this is twofold.


71
00:03:30.944 --> 00:03:34.681 line:-1 position:50%
Firstly, by dispatching the work
onto a different queue,


72
00:03:34.681 --> 00:03:36.149 line:-1 position:50%
we ensure that the main thread


73
00:03:36,149 --> 00:03:40,287 line:-1
can remain responsive
to user input even while waiting


74
00:03:40,287 --> 00:03:43,990 line:-1
for a potentially large amount
of work to happen.


75
00:03:43,990 --> 00:03:47,394 line:-1
Secondly, access to
the database is protected,


76
00:03:47.394 --> 00:03:51.832 line:-1 position:50%
since a serial queue
guarantees mutual exclusion.


77
00:03:51.832 --> 00:03:53.834 line:-1 position:50%
While on the database queue,


78
00:03:53,834 --> 00:03:55,735 line:-1
we'll iterate through
the news feeds


79
00:03:55,735 --> 00:03:59,339 line:-1
the user has subscribed to
and for each of them,


80
00:03:59.339 --> 00:04:02.709 line:-1 position:50%
schedule a networking request
to our URLSession


81
00:04:02,709 --> 00:04:06,179 line:-1
to download the contents
of that feed.


82
00:04:06,179 --> 00:04:09,516 line:-1
As the results of the networking
requests come in,


83
00:04:09,516 --> 00:04:13,553 line:-1
the URLSession callback will be
called on our delegate queue


84
00:04:13,553 --> 00:04:16,022 line:-1
which is a concurrent queue.


85
00:04:16,022 --> 00:04:19,259 line:-1
In the completion handler
for each of the results,


86
00:04:19,259 --> 00:04:21,728 line:-1
we will synchronously
update the database


87
00:04:21,728 --> 00:04:25,031 line:-1
with the latest requests
from each of these feeds,


88
00:04:25,031 --> 00:04:27,634 line:-1
so as to cache it
for future use.


89
00:04:27,634 --> 00:04:30,570 line:-1
And finally, we'll wake up
the main thread


90
00:04:30,570 --> 00:04:33,773 line:-1
to refresh the UI.


91
00:04:33.773 --> 00:04:35.942 line:-1 position:50%
This seems like
a perfectly reasonable way


92
00:04:35,942 --> 00:04:38,145 line:-1
to structure such
an application.


93
00:04:38.145 --> 00:04:40.480 line:-1 position:50%
We've made sure
not to block the main thread


94
00:04:40.480 --> 00:04:42.182 line:-1 position:50%
while working on requests.


95
00:04:42,182 --> 00:04:45,252 line:-1
And by handling the network
requests concurrently,


96
00:04:45,252 --> 00:04:47,888 line:-1
we've taken advantage
of the inherent parallelism


97
00:04:47,888 --> 00:04:49,823 line:-1
in our program.


98
00:04:49,823 --> 00:04:52,526 line:-1
Let's take a closer look
at a code snippet


99
00:04:52,526 --> 00:04:54,427 line:-1
that shows how
we process the results


100
00:04:54.427 --> 00:04:58.331 line:-1 position:50%
of our networking requests.


101
00:04:58,331 --> 00:05:01,168 line:-1
First, we have created
a URLSession


102
00:05:01.168 --> 00:05:04.337 line:-1 position:50%
for performing downloads
from our news feeds.


103
00:05:04.337 --> 00:05:06.973 line:-1 position:50%
As you can see here,
we've set the delegate queue


104
00:05:06.973 --> 00:05:10.944 line:-1 position:50%
of this URLSession
to be a concurrent queue.


105
00:05:10.944 --> 00:05:13.380 line:-1 position:50%
We then iterate over
all the news feeds


106
00:05:13,380 --> 00:05:16,683 line:-1
that need to be updated
and for each of them,


107
00:05:16,683 --> 00:05:20,086 line:-1
schedule a data task
in the URLSession.


108
00:05:20.086 --> 00:05:22.822 line:-1 position:50%
In the completion handler
of the data task --


109
00:05:22,822 --> 00:05:25,659 line:-1
which will be invoked
on the delegate queue --


110
00:05:25.659 --> 00:05:28.562 line:-1 position:50%
we deserialize the results
of our download


111
00:05:28.562 --> 00:05:31.264 line:-1 position:50%
and format them into articles.


112
00:05:31,264 --> 00:05:34,701 line:-1
We then dispatch sync
against our database queue


113
00:05:34.701 --> 00:05:38.104 line:-1 position:50%
before updating the results
for the feed.


114
00:05:38,104 --> 00:05:40,674 line:-1
So here you can see
that we've written


115
00:05:40,674 --> 00:05:44,244 line:-1
some straight-line code to do
something fairly straightforward


116
00:05:44,244 --> 00:05:49,282 line:-1
but this code has some
hidden performance pitfalls.


117
00:05:49,282 --> 00:05:52,252 line:-1
To understand more about
these performance problems,


118
00:05:52.252 --> 00:05:55.355 line:-1 position:50%
we need to first dig into
how threads are brought up


119
00:05:55,355 --> 00:05:58,425 line:-1
to handle work on GCD queues.


120
00:05:58,425 --> 00:06:00,126 line:-1
In Grand Central Dispatch,


121
00:06:00.126 --> 00:06:02.696 line:-1 position:50%
when work is enqueued
onto a queue,


122
00:06:02,696 --> 00:06:04,364 line:-1
the system will
bring up a thread


123
00:06:04,364 --> 00:06:08,768 line:-1
to service that work item.


124
00:06:08.768 --> 00:06:10.303 line:-1 position:50%
Since a concurrent queue


125
00:06:10,303 --> 00:06:12,906 line:-1
can handle multiple
work items at once,


126
00:06:12,906 --> 00:06:15,308 line:-1
the system will bring up
several threads


127
00:06:15,308 --> 00:06:19,279 line:-1
until we have saturated
all the CPU cores.


128
00:06:19,279 --> 00:06:21,548 line:-1
However, if a thread blocks --


129
00:06:21.548 --> 00:06:24.317 line:-1 position:50%
as seen on
the first CPU core here --


130
00:06:24.317 --> 00:06:27.621 line:-1 position:50%
and there is more work to be
done on the concurrent queue,


131
00:06:27,621 --> 00:06:29,889 line:-1
GCD will bring up more threads


132
00:06:29,889 --> 00:06:33,293 line:-1
to drain the remaining
work items.


133
00:06:33,293 --> 00:06:36,196 line:-1
The reason for this is twofold.


134
00:06:36.196 --> 00:06:39.366 line:-1 position:50%
Firstly, by giving your process
another thread,


135
00:06:39,366 --> 00:06:41,801 line:-1
we are able to ensure
that each core


136
00:06:41.801 --> 00:06:44.771 line:-1 position:50%
continues to have a thread
that executes work


137
00:06:44.771 --> 00:06:47.407 line:-1 position:50%
at any given time.


138
00:06:47,407 --> 00:06:49,309 line:-1
This gives your applications


139
00:06:49.309 --> 00:06:53.346 line:-1 position:50%
a good, continuing
level of concurrency.


140
00:06:53.346 --> 00:06:56.783 line:-1 position:50%
Secondly the blocked thread
may be waiting on a resource


141
00:06:56,783 --> 00:07:00,954 line:-1
like a semaphore, before it
can make further progress.


142
00:07:00,954 --> 00:07:02,822 line:-1
The new thread
that is brought up


143
00:07:02,822 --> 00:07:04,591 line:-1
to continue working
on the queue


144
00:07:04,591 --> 00:07:07,560 line:-1
may be able to help
unblock the resource


145
00:07:07.560 --> 00:07:10.997 line:-1 position:50%
that is being waited on
by the first thread.


146
00:07:10,997 --> 00:07:13,099 line:-1
So now that we understand
a bit more


147
00:07:13,099 --> 00:07:16,803 line:-1
about thread bringups in GCD,
let's go back


148
00:07:16,803 --> 00:07:19,539 line:-1
and look at the CPU
execution of the code


149
00:07:19,539 --> 00:07:22,309 line:-1
from our news app.


150
00:07:22.309 --> 00:07:25.312 line:-1 position:50%
On a two-core device
like the Apple Watch,


151
00:07:25.312 --> 00:07:27.714 line:-1 position:50%
GCD will first bring up
two threads


152
00:07:27.714 --> 00:07:30.550 line:-1 position:50%
to process
the feed update results.


153
00:07:30,550 --> 00:07:34,421 line:-1
As the threads block on
accessing the database queue,


154
00:07:34.421 --> 00:07:37.324 line:-1 position:50%
more threads are created
to continue working


155
00:07:37,324 --> 00:07:39,626 line:-1
on the networking queue.


156
00:07:39.626 --> 00:07:43.797 line:-1 position:50%
The CPU then has to context
switch between different threads


157
00:07:43.797 --> 00:07:45.932 line:-1 position:50%
processing the
networking results


158
00:07:45.932 --> 00:07:48.535 line:-1 position:50%
as indicated by
the white vertical lines


159
00:07:48.535 --> 00:07:51.938 line:-1 position:50%
between the various threads.


160
00:07:51.938 --> 00:07:54.741 line:-1 position:50%
This means that
in our news application,


161
00:07:54.741 --> 00:07:58.645 line:-1 position:50%
we could easily end up with
a very large number of threads.


162
00:07:58,645 --> 00:08:02,582 line:-1
If the user has a hundred feeds
that need to be updated,


163
00:08:02.582 --> 00:08:05.085 line:-1 position:50%
then each of those
URL data tasks


164
00:08:05,085 --> 00:08:07,954 line:-1
will have a completion block
on the concurrent queue


165
00:08:07.954 --> 00:08:10.990 line:-1 position:50%
when the network
requests complete.


166
00:08:10,990 --> 00:08:14,361 line:-1
GCD will bring up more threads
when each of the callbacks


167
00:08:14.361 --> 00:08:16.596 line:-1 position:50%
block on the database queue,


168
00:08:16,596 --> 00:08:20,600 line:-1
resulting in the application
having lots of threads.


169
00:08:20,600 --> 00:08:21,835 line:-1
Now you might ask,


170
00:08:21,835 --> 00:08:26,473 line:-1
what's so bad about having a lot
of threads in our application?


171
00:08:26.473 --> 00:08:28.708 line:-1 position:50%
Having a lot of threads
in our applications


172
00:08:28,708 --> 00:08:31,344 line:-1
means that the system
is overcommitted


173
00:08:31,344 --> 00:08:34,581 line:-1
with more threads
than we have CPU cores.


174
00:08:34.581 --> 00:08:37.784 line:-1 position:50%
Consider an iPhone
with six CPU cores.


175
00:08:37,784 --> 00:08:40,720 line:-1
If our news application
has a hundred feed updates


176
00:08:40.720 --> 00:08:42.422 line:-1 position:50%
that need to be processed,


177
00:08:42,422 --> 00:08:45,191 line:-1
this means that we have
overcommitted the iPhone


178
00:08:45.191 --> 00:08:48.495 line:-1 position:50%
with 16 times more threads
than cores.


179
00:08:48,495 --> 00:08:52,198 line:-1
This is the phenomenon
we call thread explosion.


180
00:08:52,198 --> 00:08:56,736 position:50%
Some of our previous WWDC talks
have gone into further detail


181
00:08:56,736 --> 00:08:59,239 line:0
on the risks
associated with this,


182
00:08:59,239 --> 00:09:03,510 line:0
including the possibility of
deadlocks in your application.


183
00:09:03,510 --> 00:09:06,112 line:0
Thread explosion
also comes with memory


184
00:09:06,112 --> 00:09:09,916 line:0
and scheduling overhead that
may not be immediately obvious,


185
00:09:09,916 --> 00:09:13,686 position:50%
so let's look into this further.


186
00:09:13,686 --> 00:09:16,322 line:0
Looking back at
our news application,


187
00:09:16,322 --> 00:09:20,059 line:0
each of the blocked threads
is holding onto valuable memory


188
00:09:20,059 --> 00:09:23,530 position:50%
and resources
while waiting to run again.


189
00:09:23,530 --> 00:09:25,932 line:0
Each blocked thread has a stack


190
00:09:25,932 --> 00:09:29,903 line:0
and associated kernel data
structures to track the thread.


191
00:09:29,903 --> 00:09:32,505 position:50%
Some of these threads
may be holding onto locks


192
00:09:32,505 --> 00:09:35,942 line:0
which other threads
that are running might need.


193
00:09:35,942 --> 00:09:39,012 line:0
This is a large number
of resources and memory


194
00:09:39,012 --> 00:09:43,850 line:0
to be holding onto for threads
which are not making progress.


195
00:09:43,850 --> 00:09:46,419 line:0
There is also greater
scheduling overhead


196
00:09:46,419 --> 00:09:48,888 line:0
as a result of thread explosion.


197
00:09:48,888 --> 00:09:50,957 position:50%
As new threads are brought up,


198
00:09:50,957 --> 00:09:54,561 line:0
the CPU need to perform
a full thread context switch


199
00:09:54,561 --> 00:09:56,930 position:50%
in order to switch away
from the old thread


200
00:09:56,930 --> 00:10:00,400 line:0
to start executing
the new thread.


201
00:10:00.400 --> 00:10:03.369 line:-1 position:50%
As the blocked threads
become runnable again,


202
00:10:03.369 --> 00:10:06.940 line:-1 position:50%
the scheduler will have to
timeshare the threads on the CPU


203
00:10:06,940 --> 00:10:10,910 line:-1
so that they are all able
to make forward progress.


204
00:10:10.910 --> 00:10:12.345 line:-1 position:50%
Now, timesharing of threads


205
00:10:12.345 --> 00:10:15.181 line:-1 position:50%
is fine if that happens
a few times --


206
00:10:15,181 --> 00:10:17,917 line:-1
that is the power
of concurrency.


207
00:10:17,917 --> 00:10:20,420 line:-1
But when there
is thread explosion,


208
00:10:20.420 --> 00:10:23.590 line:-1 position:50%
having to timeshare hundreds
of threads on a device


209
00:10:23,590 --> 00:10:28,228 line:-1
with limited cores can lead
to excessive context switching.


210
00:10:28.228 --> 00:10:30.430 line:-1 position:50%
The scheduling latencies
of these threads


211
00:10:30,430 --> 00:10:33,800 line:-1
outweigh the amount of useful
work they would do,


212
00:10:33,800 --> 00:10:35,835 line:-1
therefore, resulting in the CPU


213
00:10:35.835 --> 00:10:38.972 line:-1 position:50%
running less efficiently
as well.


214
00:10:38,972 --> 00:10:41,674 line:-1
As we've seen so far,
it is easy to miss


215
00:10:41.674 --> 00:10:44.878 line:-1 position:50%
some of these nuances
about the threading hygiene


216
00:10:44.878 --> 00:10:47.881 line:-1 position:50%
when writing applications
with GCD queues


217
00:10:47.881 --> 00:10:50.116 line:-1 position:50%
thereby resulting
in poor performance


218
00:10:50,116 --> 00:10:52,151 line:-1
and greater overhead.


219
00:10:52,151 --> 00:10:53,853 line:-1
Building on this experience,


220
00:10:53,853 --> 00:10:55,922 line:-1
Swift has taken
a different approach


221
00:10:55.922 --> 00:10:59.292 line:-1 position:50%
when designing concurrency
into the language.


222
00:10:59.292 --> 00:11:02.061 line:-1 position:50%
We've built Swift concurrency
with performance


223
00:11:02.061 --> 00:11:05.598 line:-1 position:50%
and efficiency in mind as well
so that your apps


224
00:11:05,598 --> 00:11:07,800 line:-1
can enjoy controlled,
structured,


225
00:11:07,800 --> 00:11:11,304 line:-1
and safe concurrency.


226
00:11:11,304 --> 00:11:15,108 line:0
With Swift, we want to change
the execution model of apps


227
00:11:15,108 --> 00:11:16,609 line:0
from the following model,


228
00:11:16,609 --> 00:11:24,017 position:50%
which has lots of threads
and context switches, to this.


229
00:11:24,017 --> 00:11:26,486 line:0
Here you see that we have
just two threads


230
00:11:26,486 --> 00:11:28,788 line:0
executing on our two-core system


231
00:11:28,788 --> 00:11:31,524 position:50%
and there are no thread
context switches.


232
00:11:31,524 --> 00:11:34,794 line:0
All of our blocked threads
go away and instead


233
00:11:34,794 --> 00:11:38,064 line:0
we have a lightweight object
known as a continuation


234
00:11:38,064 --> 00:11:41,067 line:0
to track resumption of work.


235
00:11:41,067 --> 00:11:44,170 position:50%
When threads execute work
under Swift concurrency


236
00:11:44,170 --> 00:11:46,539 position:50%
they switch between
continuations


237
00:11:46,539 --> 00:11:49,976 line:0
instead of performing
a full thread context switch.


238
00:11:49,976 --> 00:11:52,178 line:0
This means that we now only pay


239
00:11:52,178 --> 00:11:56,182 line:0
the cost of
a function call instead.


240
00:11:56.182 --> 00:11:59.619 line:-1 position:50%
So the runtime behavior that
we want for Swift concurrency


241
00:11:59,619 --> 00:12:03,656 line:-1
is to create only as many
threads as there are CPU cores,


242
00:12:03,656 --> 00:12:07,393 line:-1
and for threads to be able
to cheaply and efficiently


243
00:12:07,393 --> 00:12:10,830 line:-1
switch between work items
when they are blocked.


244
00:12:10.830 --> 00:12:13.866 line:-1 position:50%
We want you to be able
to write straight-line code


245
00:12:13.866 --> 00:12:15.835 line:-1 position:50%
that is easy to reason about


246
00:12:15.835 --> 00:12:20.340 line:-1 position:50%
and also gives you
safe, controlled concurrency.


247
00:12:20.340 --> 00:12:23.543 line:-1 position:50%
In order to achieve
this behavior that we are after,


248
00:12:23.543 --> 00:12:26.613 line:-1 position:50%
the operating system
needs a runtime contract


249
00:12:26,613 --> 00:12:30,483 line:-1
that threads will not block,
and that is only possible


250
00:12:30,483 --> 00:12:33,753 line:-1
if the language is able
to provide us with that.


251
00:12:33,753 --> 00:12:37,323 line:-1
Swift's concurrency model
and the semantics around it


252
00:12:37,323 --> 00:12:42,128 line:-1
have therefore been designed
with this goal in mind.


253
00:12:42,128 --> 00:12:44,564 line:-1
To that end,
I'd like to dive into two


254
00:12:44,564 --> 00:12:47,567 line:-1
of Swift's language-level
features that enable us


255
00:12:47.567 --> 00:12:50.670 line:-1 position:50%
to maintain a contract
with the runtime.


256
00:12:50,670 --> 00:12:53,840 line:-1
The first comes from
the semantics of await


257
00:12:53.840 --> 00:12:56.943 line:-1 position:50%
and the second, from the
tracking of task dependencies


258
00:12:56.943 --> 00:12:59.145 line:-1 position:50%
in the Swift runtime.


259
00:12:59,145 --> 00:13:01,214 line:-1
Let's consider
these language features


260
00:13:01,214 --> 00:13:05,752 line:-1
in the context of our example
news application.


261
00:13:05,752 --> 00:13:08,254 line:-1
This is the code snippet
we walked through earlier


262
00:13:08,254 --> 00:13:11,591 line:-1
that handles the results
of our news feed updates.


263
00:13:11.591 --> 00:13:13.793 line:-1 position:50%
Let's see what
this logic looks like


264
00:13:13,793 --> 00:13:18,564 line:-1
when written with Swift
concurrency primitives instead.


265
00:13:18.564 --> 00:13:22.001 line:-1 position:50%
We'd first start with creating
an async implementation


266
00:13:22,001 --> 00:13:23,836 line:-1
of our helper function.


267
00:13:23,836 --> 00:13:26,205 line:-1
Then, instead of handling
the results


268
00:13:26,205 --> 00:13:29,575 line:-1
of our networking requests
on a concurrent dispatch queue,


269
00:13:29.575 --> 00:13:32.812 line:-1 position:50%
here we are using
a task group instead


270
00:13:32,812 --> 00:13:35,248 line:-1
to manage our concurrency.


271
00:13:35.248 --> 00:13:38.284 line:-1 position:50%
In the task group,
we will create child tasks


272
00:13:38,284 --> 00:13:41,454 line:-1
for each feed
that needs to be updated.


273
00:13:41,454 --> 00:13:45,491 line:-1
Each child task will perform
a download from the feed's URL


274
00:13:45.491 --> 00:13:47.994 line:-1 position:50%
using the shared URLSession.


275
00:13:47.994 --> 00:13:51.097 line:-1 position:50%
It will then deserialize
the results of the download,


276
00:13:51,097 --> 00:13:53,833 line:-1
format them into articles
and finally,


277
00:13:53.833 --> 00:13:58.104 line:-1 position:50%
we will call an async function
to update our database.


278
00:13:58,104 --> 00:14:01,007 line:-1
Here, when calling
any async functions,


279
00:14:01,007 --> 00:14:04,410 line:-1
we annotate it
with an await keyword.


280
00:14:04,410 --> 00:14:07,780 line:-1
From the "Meet async/await
in Swift" talk,


281
00:14:07,780 --> 00:14:11,284 line:-1
we learned that an await
is an asynchronous wait.


282
00:14:11.284 --> 00:14:14.120 line:-1 position:50%
That is, it does not block
the current thread


283
00:14:14.120 --> 00:14:17.990 line:-1 position:50%
while waiting for results
from the async function.


284
00:14:17,990 --> 00:14:20,593 line:-1
Instead, the function
may be suspended


285
00:14:20,593 --> 00:14:25,131 line:-1
and the thread will be freed up
to execute other tasks.


286
00:14:25.131 --> 00:14:26.632 line:-1 position:50%
How does this happen?


287
00:14:26,632 --> 00:14:28,534 line:-1
How does one give up a thread?


288
00:14:28,534 --> 00:14:30,903 line:-1
My colleague Varun
will now shed light


289
00:14:30.903 --> 00:14:33.873 line:-1 position:50%
on what is done under the hood
in the Swift runtime


290
00:14:33,873 --> 00:14:37,443 line:-1
to make this possible.


291
00:14:37,443 --> 00:14:38,878 line:-1
Varun Gandhi: Thanks, Rokhini.


292
00:14:38.878 --> 00:14:41.881 line:-1 position:50%
Before jumping into how async
functions are implemented,


293
00:14:41.881 --> 00:14:46.285 line:-1 position:50%
let's do a quick refresher on
how nonasync functions work.


294
00:14:46,285 --> 00:14:49,288 position:50%
Every thread in a running
program has one stack,


295
00:14:49,288 --> 00:14:52,158 line:0
which it uses to store state
for function calls.


296
00:14:52,158 --> 00:14:55,628 line:0
Let's focus on one thread
for now.


297
00:14:55,628 --> 00:14:57,697 line:0
When the thread executes
a function call,


298
00:14:57,697 --> 00:15:00,666 line:0
a new frame is pushed
onto its stack.


299
00:15:00,666 --> 00:15:03,770 line:0
This newly created stack frame
can be used by the function


300
00:15:03,770 --> 00:15:06,739 position:50%
to store local variables,
the return address,


301
00:15:06,739 --> 00:15:09,609 line:0
and any other information
that is needed.


302
00:15:09,609 --> 00:15:12,612 line:0
Once the function finishes
executing and returns,


303
00:15:12,612 --> 00:15:16,215 line:0
its stack frame is popped.


304
00:15:16.215 --> 00:15:19.018 line:-1 position:50%
Now let's consider
async functions.


305
00:15:19.018 --> 00:15:22.488 line:-1 position:50%
Suppose that a thread called
an add(_:) method on the Feed type


306
00:15:22.488 --> 00:15:24.991 line:-1 position:50%
from the updateDatabase
function.


307
00:15:24,991 --> 00:15:29,061 line:-1
At this stage, the most recent
stack frame will be for add(_:).


308
00:15:29.061 --> 00:15:31.197 line:-1 position:50%
The stack frame stores
local variables


309
00:15:31.197 --> 00:15:35.034 line:-1 position:50%
that do not need to be available
across a suspension point.


310
00:15:35,034 --> 00:15:37,904 line:-1
The body of add(_:)
has one suspension point,


311
00:15:37.904 --> 00:15:39.705 line:-1 position:50%
marked by await.


312
00:15:39.705 --> 00:15:42.975 line:-1 position:50%
The local variables, id
and article, are immediately


313
00:15:42,975 --> 00:15:46,512 line:-1
used in the body of the for loop
after being defined,


314
00:15:46,512 --> 00:15:49,215 line:-1
without any suspension
points in-between.


315
00:15:49,215 --> 00:15:53,653 line:-1
So they will be stored
in the stack frame.


316
00:15:53,653 --> 00:15:57,223 line:-1
Additionally, there will be two
async frames on the heap,


317
00:15:57.223 --> 00:16:00.760 line:-1 position:50%
one for updateDatabase
and one for add.


318
00:16:00.760 --> 00:16:03.930 line:-1 position:50%
Async frames store information
that does need to be available


319
00:16:03.930 --> 00:16:06.632 line:-1 position:50%
across suspension points.


320
00:16:06,632 --> 00:16:09,502 line:-1
Notice that the newArticles
argument is defined


321
00:16:09.502 --> 00:16:13.806 line:-1 position:50%
before await but needs
to be available after the await.


322
00:16:13.806 --> 00:16:16.042 line:-1 position:50%
This means that
the async frame for add


323
00:16:16,042 --> 00:16:19,078 line:-1
will keep track of newArticles.


324
00:16:19,078 --> 00:16:22,782 line:-1
Suppose the thread
continues executing.


325
00:16:22,782 --> 00:16:25,017 line:-1
When the save function
starts executing,


326
00:16:25.017 --> 00:16:27.486 line:-1 position:50%
the stack frame for add
is replaced


327
00:16:27,486 --> 00:16:30,256 line:-1
by a stack frame for save.


328
00:16:30.256 --> 00:16:32.458 line:-1 position:50%
Instead of adding
new stack frames,


329
00:16:32,458 --> 00:16:34,594 line:-1
the top most stack frame
is replaced


330
00:16:34.594 --> 00:16:37.763 line:-1 position:50%
since any variables that will
be needed in the future


331
00:16:37.763 --> 00:16:42.268 line:-1 position:50%
will already have been stored
in the list of async frames.


332
00:16:42.268 --> 00:16:48.007 line:-1 position:50%
The save function also gains
an async frame for its use.


333
00:16:48.007 --> 00:16:50.476 line:-1 position:50%
While the articles are being
saved to the database,


334
00:16:50,476 --> 00:16:53,346 line:-1
it would be better if the thread
could do some useful work


335
00:16:53,346 --> 00:16:55,381 line:-1
instead of being blocked.


336
00:16:55,381 --> 00:17:00,052 line:-1
Suppose the execution of the
save function is suspended.


337
00:17:00.052 --> 00:17:03.623 line:-1 position:50%
And the thread is reused to do
some other useful work


338
00:17:03,623 --> 00:17:08,594 line:-1
instead of being blocked.


339
00:17:08,594 --> 00:17:10,763 line:-1
Since all information
that is maintained


340
00:17:10,763 --> 00:17:14,166 line:-1
across a suspension point
is stored on the heap,


341
00:17:14,166 --> 00:17:18,571 line:-1
it can be used to continue
execution at a later stage.


342
00:17:18.571 --> 00:17:22.174 line:-1 position:50%
This list of async frames
is the runtime representation


343
00:17:22,174 --> 00:17:25,845 line:-1
of a continuation.


344
00:17:25.845 --> 00:17:27.046 line:-1 position:50%
Say after a little while,


345
00:17:27,046 --> 00:17:29,148 line:-1
the database request
is complete,


346
00:17:29,148 --> 00:17:32,919 line:-1
and suppose some thread
is freed up.


347
00:17:32.919 --> 00:17:34.787 line:-1 position:50%
This could be
the same thread as before,


348
00:17:34,787 --> 00:17:38,658 line:-1
or it could be
a different thread.


349
00:17:38.658 --> 00:17:40.927 line:-1 position:50%
Suppose the save function
resumes executing


350
00:17:40.927 --> 00:17:42.428 line:-1 position:50%
on this thread.


351
00:17:42,428 --> 00:17:45,197 line:-1
Once it finishes executing


352
00:17:45.197 --> 00:17:48.334 line:-1 position:50%
and returns some IDs,
then the stack frame for save


353
00:17:48,334 --> 00:17:52,505 line:-1
will again be replaced
by a stack frame for add.


354
00:17:52,505 --> 00:17:56,909 line:-1
After that, the thread can
start executing zip.


355
00:17:56,909 --> 00:17:59,578 line:-1
Zipping two arrays
is a nonasync operation,


356
00:17:59,578 --> 00:18:03,049 line:-1
so it will create
a new stack frame.


357
00:18:03.049 --> 00:18:05.985 line:-1 position:50%
Since Swift continues to use
the operating system stack,


358
00:18:05.985 --> 00:18:08.587 line:-1 position:50%
both async
and nonasync Swift code


359
00:18:08,587 --> 00:18:12,091 line:-1
can efficiently call
into C and Objective-C.


360
00:18:12,091 --> 00:18:15,061 line:0
Moreover, C and Objective-C code
can continue


361
00:18:15,061 --> 00:18:18,497 line:0
efficiently calling
nonasync Swift code.


362
00:18:18,497 --> 00:18:22,134 position:50%
Once the zip function finishes,
its stack frame will be popped


363
00:18:22,134 --> 00:18:27,306 line:0
and execution will continue.


364
00:18:27.306 --> 00:18:30.743 line:-1 position:50%
So far, I've described how await
is designed to ensure


365
00:18:30.743 --> 00:18:33.012 line:-1 position:50%
efficient suspension
and resumption,


366
00:18:33,012 --> 00:18:37,249 line:-1
while freeing up a thread's
resources to do other work.


367
00:18:37.249 --> 00:18:40.386 line:-1 position:50%
Next, Rokhini will describe
the second language feature,


368
00:18:40,386 --> 00:18:44,657 line:-1
which is the runtime's tracking
of dependencies between tasks.


369
00:18:44.657 --> 00:18:46.058 line:-1 position:50%
Rokhini: Thanks, Varun.


370
00:18:46.058 --> 00:18:49.161 line:-1 position:50%
As described earlier,
a function can be broken up


371
00:18:49,161 --> 00:18:51,630 line:-1
into continuations
at an await,


372
00:18:51,630 --> 00:18:55,534 line:-1
also known as a potential
suspension point.


373
00:18:55.534 --> 00:18:56.669 line:-1 position:50%
In this case,


374
00:18:56.669 --> 00:19:00.106 line:-1 position:50%
the URLSession data task
is the async function


375
00:19:00.106 --> 00:19:03.976 line:-1 position:50%
and the remaining work after it
is the continuation.


376
00:19:03,976 --> 00:19:06,812 line:-1
The continuation
can only be executed


377
00:19:06,812 --> 00:19:09,615 line:-1
after the async function
is completed.


378
00:19:09.615 --> 00:19:14.820 line:-1 position:50%
This is a dependency tracked by
the Swift concurrency runtime.


379
00:19:14,820 --> 00:19:17,356 line:-1
Similarly,
within the task group,


380
00:19:17.356 --> 00:19:20.559 line:-1 position:50%
a parent task may create
several child tasks


381
00:19:20.559 --> 00:19:23.829 line:-1 position:50%
and each of those child tasks
needs to complete


382
00:19:23,829 --> 00:19:26,766 line:-1
before a parent task
can proceed.


383
00:19:26.766 --> 00:19:30.436 line:-1 position:50%
This is a dependency that
is expressed in your code


384
00:19:30,436 --> 00:19:34,740 line:-1
by the scope of the task group
and therefore explicitly known


385
00:19:34.740 --> 00:19:38.310 line:-1 position:50%
to the Swift compiler
and runtime.


386
00:19:38.310 --> 00:19:42.181 line:-1 position:50%
In Swift, tasks can only await
other tasks that are known


387
00:19:42,181 --> 00:19:43,749 line:-1
to the Swift runtime --


388
00:19:43,749 --> 00:19:46,852 line:-1
be it continuations
or child tasks.


389
00:19:46,852 --> 00:19:48,854 line:-1
Therefore, code when structured


390
00:19:48.854 --> 00:19:50.890 line:-1 position:50%
with Swift's
concurrency primitives


391
00:19:50,890 --> 00:19:53,859 line:-1
provide the runtime
with a clear understanding


392
00:19:53,859 --> 00:19:58,030 line:-1
of the dependency
chain between the tasks.


393
00:19:58.030 --> 00:20:01.500 line:-1 position:50%
So far, you've learned
how Swift's language features


394
00:20:01.500 --> 00:20:05.304 line:-1 position:50%
allow a task to be suspended
during an await.


395
00:20:05.304 --> 00:20:08.507 line:-1 position:50%
Instead, the executing thread
is able to reason


396
00:20:08.507 --> 00:20:10.242 line:-1 position:50%
about task dependencies


397
00:20:10,242 --> 00:20:13,145 line:-1
and pick up
a different task instead.


398
00:20:13,145 --> 00:20:16,449 line:-1
This means that code written
with Swift concurrency


399
00:20:16.449 --> 00:20:19.018 line:-1 position:50%
can maintain a runtime contract


400
00:20:19,018 --> 00:20:23,389 line:-1
that threads are always able
to make forward progress.


401
00:20:23.389 --> 00:20:26.292 line:-1 position:50%
We have taken advantage
of this runtime contract


402
00:20:26,292 --> 00:20:31,497 line:-1
to build integrated OS support
for Swift concurrency.


403
00:20:31,497 --> 00:20:35,301 line:-1
This is in the form of a new
cooperative thread pool


404
00:20:35.301 --> 00:20:39.305 line:-1 position:50%
to back Swift concurrency
as the default executor.


405
00:20:39.305 --> 00:20:42.608 line:-1 position:50%
The new thread pool will only
spawn as many threads


406
00:20:42,608 --> 00:20:44,510 line:-1
as there are CPU cores,


407
00:20:44.510 --> 00:20:48.481 line:-1 position:50%
thereby making sure
not to overcommit the system.


408
00:20:48.481 --> 00:20:52.551 line:-1 position:50%
Unlike GCD's concurrent queues,
which will spawn more threads


409
00:20:52.551 --> 00:20:54.487 line:-1 position:50%
when work items block,


410
00:20:54,487 --> 00:20:58,657 line:-1
with Swift threads can always
make forward progress.


411
00:20:58.657 --> 00:21:02.027 line:-1 position:50%
Therefore, the default runtime
can be judicious


412
00:21:02.027 --> 00:21:05.331 line:-1 position:50%
about controlling how many
threads are spawned.


413
00:21:05.331 --> 00:21:07.967 line:-1 position:50%
This lets us
give your applications


414
00:21:07,967 --> 00:21:11,103 line:-1
the concurrency you need
while making sure to avoid


415
00:21:11,103 --> 00:21:15,941 line:-1
the known pitfalls
of excessive concurrency.


416
00:21:15,941 --> 00:21:19,545 position:50%
In previous WWDC talks
about concurrency


417
00:21:19,545 --> 00:21:22,715 line:0
with Grand Central Dispatch,
we've recommended that


418
00:21:22,715 --> 00:21:26,018 position:50%
you structure your applications
into distinct subsystems


419
00:21:26,018 --> 00:21:30,055 line:0
and maintain one serial dispatch
queue per subsystem


420
00:21:30,055 --> 00:21:33,425 position:50%
to control the concurrency
of your application.


421
00:21:33,425 --> 00:21:37,096 position:50%
This meant that it was difficult
for you to get concurrency


422
00:21:37,096 --> 00:21:39,999 line:0
greater than one
within a subsystem


423
00:21:39,999 --> 00:21:43,736 line:0
without running the risk
of thread explosion.


424
00:21:43,736 --> 00:21:47,439 line:0
With Swift, the language
gives us strong invariants


425
00:21:47,439 --> 00:21:49,441 position:50%
which the runtime has leveraged,


426
00:21:49,441 --> 00:21:52,845 line:0
thereby being able
to transparently provide you


427
00:21:52,845 --> 00:21:54,813 position:50%
with better-controlled
concurrency


428
00:21:54,813 --> 00:21:59,118 line:0
in the default runtime.


429
00:21:59,118 --> 00:22:02,421 line:-1
Now that you understand a bit
more about the threading model


430
00:22:02.421 --> 00:22:03.956 line:-1 position:50%
for Swift concurrency,


431
00:22:03,956 --> 00:22:06,992 line:-1
let's go through some
considerations to keep in mind


432
00:22:06,992 --> 00:22:12,097 line:-1
while adopting these exciting
new features in your code.


433
00:22:12.097 --> 00:22:15.134 line:-1 position:50%
The first consideration that
you need to keep in mind


434
00:22:15,134 --> 00:22:18,404 line:-1
has to do with performance
when converting synchronous code


435
00:22:18,404 --> 00:22:20,739 line:-1
into asynchronous code.


436
00:22:20,739 --> 00:22:23,209 line:-1
Earlier, we talked through
some of the costs


437
00:22:23,209 --> 00:22:27,546 line:-1
associated with concurrency such
as additional memory allocations


438
00:22:27,546 --> 00:22:29,949 line:-1
and logic in the Swift runtime.


439
00:22:29,949 --> 00:22:34,253 line:-1
As such, you need to be careful
to only write new code


440
00:22:34.253 --> 00:22:36.589 line:-1 position:50%
with Swift concurrency
when the cost


441
00:22:36,589 --> 00:22:39,358 line:-1
of introducing concurrency
into your code


442
00:22:39,358 --> 00:22:43,395 line:-1
outweighs the cost
of managing it.


443
00:22:43,395 --> 00:22:46,432 line:-1
The code snippet here
might not actually benefit


444
00:22:46,432 --> 00:22:50,069 line:-1
from the additional concurrency
of spawning a child task


445
00:22:50,069 --> 00:22:53,739 line:-1
simply to read a value
from the user's defaults.


446
00:22:53,739 --> 00:22:57,576 line:-1
This is because the useful work
done by the child task


447
00:22:57,576 --> 00:23:02,147 line:-1
is diminished by the cost of
creating and managing the task.


448
00:23:02.147 --> 00:23:04.883 line:-1 position:50%
We therefore recommend
profiling your code


449
00:23:04,883 --> 00:23:06,719 line:-1
with Instruments system trace


450
00:23:06.719 --> 00:23:09.355 line:-1 position:50%
to understand it's
performance characteristics


451
00:23:09,355 --> 00:23:12,825 line:-1
as you adopt Swift concurrency.


452
00:23:12.825 --> 00:23:14.860 line:-1 position:50%
The second thing
to be careful about


453
00:23:14.860 --> 00:23:18.664 line:-1 position:50%
is the notion of atomicity
around an await.


454
00:23:18,664 --> 00:23:21,333 line:-1
Swift makes no guarantee
that the thread


455
00:23:21.333 --> 00:23:25.204 line:-1 position:50%
which executed the code before
the await is the same thread


456
00:23:25.204 --> 00:23:28.574 line:-1 position:50%
which will pick up
the continuation as well.


457
00:23:28.574 --> 00:23:32.211 line:-1 position:50%
In fact, await is an explicit
point in your code


458
00:23:32.211 --> 00:23:35.180 line:-1 position:50%
which indicates
that atomicity is broken


459
00:23:35,180 --> 00:23:39,585 line:-1
since the task may be
voluntarily descheduled.


460
00:23:39,585 --> 00:23:42,855 line:-1
As such, you should
be careful not to hold locks


461
00:23:42.855 --> 00:23:44.523 line:-1 position:50%
across an await.


462
00:23:44,523 --> 00:23:47,926 line:-1
Similarly, thread-specific data
will not be preserved


463
00:23:47.926 --> 00:23:50.229 line:-1 position:50%
across an await either.


464
00:23:50,229 --> 00:23:54,066 line:-1
Any assumptions in your code
which expect thread locality


465
00:23:54.066 --> 00:23:57.403 line:-1 position:50%
should be revisited to account
for the suspending behavior


466
00:23:57.403 --> 00:24:00.172 line:-1 position:50%
of await.


467
00:24:00,172 --> 00:24:02,941 line:-1
And lastly,
the final consideration


468
00:24:02.941 --> 00:24:06.612 line:-1 position:50%
has to do with the runtime
contract that is foundational


469
00:24:06,612 --> 00:24:10,716 line:-1
to the efficient
threading model in Swift.


470
00:24:10,716 --> 00:24:13,986 line:-1
Recall that with Swift,
the language allows us


471
00:24:13,986 --> 00:24:15,954 line:-1
to uphold a runtime contract


472
00:24:15.954 --> 00:24:20.025 line:-1 position:50%
that threads will always be able
to make forward progress.


473
00:24:20,025 --> 00:24:21,927 line:-1
It is based on this contract


474
00:24:21,927 --> 00:24:24,263 line:-1
that we have built
a cooperative thread pool


475
00:24:24.263 --> 00:24:28.233 line:-1 position:50%
to be the default
executor for Swift.


476
00:24:28,233 --> 00:24:30,269 line:-1
As you adopt Swift concurrency,


477
00:24:30,269 --> 00:24:33,339 line:-1
it is important to ensure that
you continue to maintain


478
00:24:33.339 --> 00:24:35.808 line:-1 position:50%
this contract
in your code as well


479
00:24:35.808 --> 00:24:40.746 line:-1 position:50%
so that the cooperative thread
pool can function optimally.


480
00:24:40,746 --> 00:24:43,482 line:-1
It is possible to maintain
this contract


481
00:24:43,482 --> 00:24:47,252 line:-1
within the cooperative thread
pool by using safe primitives


482
00:24:47.252 --> 00:24:51.957 line:-1 position:50%
that make the dependencies
in your code explicit and known.


483
00:24:51,957 --> 00:24:53,792 line:-1
With Swift concurrency
primitives


484
00:24:53,792 --> 00:24:56,729 line:-1
like await, actors,
and task groups,


485
00:24:56.729 --> 00:25:00.699 line:-1 position:50%
these dependencies
are made known at compile time.


486
00:25:00.699 --> 00:25:03.902 line:-1 position:50%
Therefore, the Swift compiler
enforces this


487
00:25:03,902 --> 00:25:07,639 line:-1
and helps you preserve
the runtime contract.


488
00:25:07.639 --> 00:25:12.277 line:-1 position:50%
Primitives like os_unfair_locks
and NSLocks are also safe


489
00:25:12,277 --> 00:25:16,248 line:-1
but caution is required
when using them.


490
00:25:16,248 --> 00:25:19,518 line:-1
Using a lock in synchronous
code is safe


491
00:25:19.518 --> 00:25:21.820 line:-1 position:50%
when used for
data synchronization


492
00:25:21,820 --> 00:25:25,391 line:-1
around a tight,
well-known critical section.


493
00:25:25,391 --> 00:25:28,293 line:-1
This is because
the thread holding the lock


494
00:25:28,293 --> 00:25:30,996 line:-1
is always able
to make forward progress


495
00:25:30.996 --> 00:25:33.365 line:-1 position:50%
towards releasing the lock.


496
00:25:33.365 --> 00:25:36.468 line:-1 position:50%
As such, while the primitive
may block a thread


497
00:25:36,468 --> 00:25:39,505 line:-1
for a short period of time
under contention,


498
00:25:39,505 --> 00:25:42,274 line:-1
it does not violate
the runtime contract


499
00:25:42,274 --> 00:25:44,743 line:-1
of forward progress.


500
00:25:44.743 --> 00:25:48.514 line:-1 position:50%
It is worth noting that unlike
Swift concurrency primitives,


501
00:25:48.514 --> 00:25:53.218 line:-1 position:50%
there is no compiler support to
aid in correct usage of locks,


502
00:25:53,218 --> 00:25:58,757 line:-1
so it is your responsibility
to use this primitive correctly.


503
00:25:58,757 --> 00:26:01,693 line:-1
On the other hand,
primitives like semaphores


504
00:26:01.693 --> 00:26:04.363 line:-1 position:50%
and condition variables
are unsafe to use


505
00:26:04,363 --> 00:26:06,498 line:-1
with Swift concurrency.


506
00:26:06,498 --> 00:26:09,468 line:-1
This is because they hide
dependency information


507
00:26:09,468 --> 00:26:11,069 line:-1
from the Swift runtime,


508
00:26:11.069 --> 00:26:15.841 line:-1 position:50%
but introduce a dependency
in execution in your code.


509
00:26:15.841 --> 00:26:18.777 line:-1 position:50%
Since the runtime is unaware
of this dependency,


510
00:26:18,777 --> 00:26:21,013 line:-1
it cannot make
the right scheduling decisions


511
00:26:21,013 --> 00:26:23,449 line:-1
and resolve them.


512
00:26:23.449 --> 00:26:26.485 line:-1 position:50%
In particular, do not use
primitives that create


513
00:26:26,485 --> 00:26:29,688 line:-1
unstructured tasks
and then retroactively


514
00:26:29,688 --> 00:26:33,125 line:-1
introduce a dependency
across task boundaries


515
00:26:33.125 --> 00:26:36.962 line:-1 position:50%
by using a semaphore
or an unsafe primitive.


516
00:26:36,962 --> 00:26:40,933 line:-1
Such a code pattern means that
a thread can block indefinitely


517
00:26:40.933 --> 00:26:43.669 line:-1 position:50%
against the semaphore
until another thread


518
00:26:43.669 --> 00:26:45.871 line:-1 position:50%
is able to unblock it.


519
00:26:45,871 --> 00:26:48,340 line:-1
This violates
the runtime contract


520
00:26:48.340 --> 00:26:51.844 line:-1 position:50%
of forward progress for threads.


521
00:26:51.844 --> 00:26:55.147 line:-1 position:50%
To help you identify uses
of such unsafe primitives


522
00:26:55,147 --> 00:26:58,650 line:-1
in your codebase,
we recommend testing your apps


523
00:26:58,650 --> 00:27:01,653 line:-1
with the following
environment variable.


524
00:27:01.653 --> 00:27:05.591 line:-1 position:50%
This runs your app under
a modified debug runtime,


525
00:27:05.591 --> 00:27:11.430 line:-1 position:50%
which enforces the invariant
of forward progress.


526
00:27:11,430 --> 00:27:14,600 line:-1
This environment variable
can be set in Xcode


527
00:27:14.600 --> 00:27:17.703 line:-1 position:50%
on the Run Arguments pane
of your project scheme


528
00:27:17.703 --> 00:27:21.139 line:-1 position:50%
as shown here.


529
00:27:21.139 --> 00:27:24.276 line:-1 position:50%
When running your apps
with this environment variable,


530
00:27:24,276 --> 00:27:27,279 line:-1
if you see a thread
from the cooperative thread pool


531
00:27:27.279 --> 00:27:28.981 line:-1 position:50%
that appears to be hung


532
00:27:28,981 --> 00:27:33,051 line:-1
it indicates the use
of an unsafe blocking primitive.


533
00:27:33.051 --> 00:27:36.255 line:-1 position:50%
Now, having learned about
how the threading model


534
00:27:36.255 --> 00:27:38.724 line:-1 position:50%
has been designed
for Swift concurrency,


535
00:27:38,724 --> 00:27:41,927 line:-1
let's discover a little bit more
about the primitives


536
00:27:41,927 --> 00:27:44,630 line:-1
that are available to us
to synchronize state


537
00:27:44,630 --> 00:27:47,232 line:-1
in this new world.


538
00:27:47.232 --> 00:27:49.768 line:-1 position:50%
Varun: In the Swift concurrency
talk on actors


539
00:27:49,768 --> 00:27:51,303 line:-1
you've seen how actors
can be used


540
00:27:51,303 --> 00:27:54,940 line:-1
to protect mutable state
from concurrent access.


541
00:27:54,940 --> 00:27:58,677 line:-1
Put differently, actors provide
a powerful new synchronization


542
00:27:58.677 --> 00:28:01.747 line:-1 position:50%
primitive that you can use.


543
00:28:01.747 --> 00:28:04.650 line:-1 position:50%
Recall that actors guarantee
mutual exclusion:


544
00:28:04,650 --> 00:28:08,887 line:-1
an actor may be executing at
most one method call at a time.


545
00:28:08.887 --> 00:28:11.657 line:-1 position:50%
Mutual exclusion means
that the actor's state


546
00:28:11,657 --> 00:28:15,827 line:-1
is not accessed concurrently,
preventing data races.


547
00:28:15.827 --> 00:28:17.429 line:-1 position:50%
Let's see how actors compare


548
00:28:17,429 --> 00:28:21,366 line:-1
to other forms
of mutual exclusion.


549
00:28:21,366 --> 00:28:23,335 line:0
Consider the earlier example


550
00:28:23,335 --> 00:28:25,804 line:0
of updating the database
with some articles


551
00:28:25,804 --> 00:28:28,273 line:0
by syncing to a serial queue.


552
00:28:28,273 --> 00:28:30,108 line:0
If the queue is not
already running


553
00:28:30,108 --> 00:28:32,678 position:50%
we say that there
is no contention.


554
00:28:32,678 --> 00:28:35,781 line:0
In this case, the calling thread
is reused to execute


555
00:28:35,781 --> 00:28:39,885 position:50%
the new work item on the queue
without any context switch.


556
00:28:39,885 --> 00:28:43,088 position:50%
Instead, if the serial queue
is already running


557
00:28:43,088 --> 00:28:45,691 line:0
the queue is said
to be under contention.


558
00:28:45,691 --> 00:28:49,161 position:50%
In this situation,
the calling thread is blocked.


559
00:28:49,161 --> 00:28:52,464 position:50%
This blocking behavior is what
triggered thread explosion


560
00:28:52,464 --> 00:28:55,734 line:0
as Rokhini described
earlier in the talk.


561
00:28:55,734 --> 00:28:58,236 line:0
Locks have this same behavior.


562
00:28:58,236 --> 00:29:00,672 position:50%
Because of the problems
associated with blocking,


563
00:29:00,672 --> 00:29:02,274 line:0
we have generally advised


564
00:29:02,274 --> 00:29:05,944 line:0
that you prefer
using dispatch async.


565
00:29:05,944 --> 00:29:10,382 position:50%
The primary benefit of dispatch
async is that it is nonblocking.


566
00:29:10,382 --> 00:29:12,217 line:0
So even under contention,


567
00:29:12,217 --> 00:29:14,786 line:0
it will not lead
to thread explosion.


568
00:29:14,786 --> 00:29:18,457 position:50%
The downside of using dispatch
async with a serial queue


569
00:29:18,457 --> 00:29:20,492 line:0
is that when
there is no contention


570
00:29:20,492 --> 00:29:22,694 line:0
Dispatch needs to request
a new thread


571
00:29:22,694 --> 00:29:24,363 line:0
to do the async work


572
00:29:24,363 --> 00:29:27,633 line:0
while the calling thread
continues to do something else.


573
00:29:27,633 --> 00:29:30,602 position:50%
Hence, frequent use
of dispatch async can lead


574
00:29:30,602 --> 00:29:34,206 position:50%
to excess thread wakeups
and context switches.


575
00:29:34,206 --> 00:29:37,709 position:50%
This brings us to actors.


576
00:29:37.709 --> 00:29:40.412 line:-1 position:50%
Swift's actors combine
the best of both worlds


577
00:29:40.412 --> 00:29:42.748 line:-1 position:50%
by taking advantage
of the cooperative thread pool


578
00:29:42,748 --> 00:29:45,183 line:-1
for efficient scheduling.


579
00:29:45.183 --> 00:29:48.186 line:-1 position:50%
When you call a method
on an actor that is not running,


580
00:29:48,186 --> 00:29:52,391 line:-1
the calling thread can be reused
to execute the method call.


581
00:29:52,391 --> 00:29:55,227 line:-1
In the case where the called
actor is already running,


582
00:29:55.227 --> 00:29:58.397 line:-1 position:50%
the calling thread can suspend
the function it is executing


583
00:29:58.397 --> 00:30:00.766 line:-1 position:50%
and pick up other work.


584
00:30:00,766 --> 00:30:02,401 line:-1
Let's look at how
these two properties


585
00:30:02.401 --> 00:30:07.139 line:-1 position:50%
work in the context
of the example news app.


586
00:30:07,139 --> 00:30:11,143 line:-1
Let's focus on the database
and networking subsystems.


587
00:30:11,143 --> 00:30:14,479 line:-1
When updating the application
to use Swift concurrency,


588
00:30:14.479 --> 00:30:16.415 line:-1 position:50%
the serial queue
for the database


589
00:30:16,415 --> 00:30:19,818 line:-1
would be replaced
by a database actor.


590
00:30:19,818 --> 00:30:21,553 line:-1
The concurrent queue
for networking


591
00:30:21.553 --> 00:30:25.457 line:-1 position:50%
could be replaced by one actor
for each news feed.


592
00:30:25,457 --> 00:30:28,593 line:-1
For simplicity, I've only shown
three feed actors here --


593
00:30:28,593 --> 00:30:30,462 line:-1
for the sports feed,
the weather feed


594
00:30:30.462 --> 00:30:31.763 line:-1 position:50%
and the health feed --


595
00:30:31.763 --> 00:30:34.900 line:-1 position:50%
but in practice,
there will be many more.


596
00:30:34,900 --> 00:30:39,304 line:-1
These actors would run on
the cooperative thread pool.


597
00:30:39,304 --> 00:30:41,840 line:0
The feed actors interact
with the database


598
00:30:41,840 --> 00:30:45,377 position:50%
to save articles
and perform other actions.


599
00:30:45,377 --> 00:30:47,946 position:50%
This interaction involves
execution switching


600
00:30:47,946 --> 00:30:49,748 line:0
from one actor to another.


601
00:30:49,748 --> 00:30:52,718 line:0
We call this process
actor hopping.


602
00:30:52,718 --> 00:30:55,921 position:50%
Let's discuss
how actor hopping works.


603
00:30:55,921 --> 00:30:58,156 line:-1
Suppose that the actor
for the sports feed


604
00:30:58.156 --> 00:31:00.892 line:-1 position:50%
is running on a thread
from the cooperative pool,


605
00:31:00.892 --> 00:31:04.463 line:-1 position:50%
and it decides to save
some articles into the database.


606
00:31:04,463 --> 00:31:08,633 position:50%
For now, let's consider that
the database is not being used.


607
00:31:08,633 --> 00:31:10,936 line:0
This is the uncontended case.


608
00:31:10,936 --> 00:31:13,505 line:0
The thread can directly hop
from the sports feed actor


609
00:31:13,505 --> 00:31:15,240 position:50%
to the database actor.


610
00:31:15,240 --> 00:31:18,610 line:0
There are two things
to notice here.


611
00:31:18,610 --> 00:31:22,314 position:50%
First, the thread did not block
while hopping actors.


612
00:31:22,314 --> 00:31:25,717 position:50%
Second, hopping did not
require a different thread;


613
00:31:25,717 --> 00:31:27,352 position:50%
the runtime can directly suspend


614
00:31:27,352 --> 00:31:29,588 line:0
the work item
for the sports feed actor


615
00:31:29,588 --> 00:31:34,559 line:0
and create a new work item
for the database actor.


616
00:31:34,559 --> 00:31:36,661 position:50%
Say the database actor
runs for a while


617
00:31:36,661 --> 00:31:39,831 position:50%
but it has not completed
the first work item.


618
00:31:39,831 --> 00:31:42,834 position:50%
At this moment, suppose
that the weather feed actor


619
00:31:42,834 --> 00:31:46,538 line:0
tries to save some articles
in the database.


620
00:31:46,538 --> 00:31:51,143 position:50%
This creates a new work item
for the database actor.


621
00:31:51,143 --> 00:31:54,980 position:50%
An actor ensures safety by
guaranteeing mutual exclusion;


622
00:31:54,980 --> 00:31:58,316 line:0
at most, one work item
may be active at a given time.


623
00:31:58,316 --> 00:32:01,920 line:0
Since there is already
one active work item, D1,


624
00:32:01,920 --> 00:32:05,757 line:0
the new work item, D2,
will be kept pending.


625
00:32:05,757 --> 00:32:07,959 line:0
Actors are also nonblocking.


626
00:32:07,959 --> 00:32:12,297 line:0
In this situation, the weather
feed actor will be suspended


627
00:32:12,297 --> 00:32:14,833 line:0
and the thread it was
executing on is now freed up


628
00:32:14,833 --> 00:32:17,869 position:50%
to do other work.


629
00:32:17,869 --> 00:32:18,970 line:0
After a little while,


630
00:32:18,970 --> 00:32:21,640 line:0
the initial database request
is completed,


631
00:32:21,640 --> 00:32:26,044 line:0
so the active work item for
the database actor is removed.


632
00:32:26,044 --> 00:32:29,047 position:50%
At this point, the runtime
may choose to start executing


633
00:32:29,047 --> 00:32:31,817 line:0
the pending work item
for the database actor.


634
00:32:31,817 --> 00:32:34,786 line:0
Or it may choose to resume
one of the feed actors.


635
00:32:34,786 --> 00:32:39,090 position:50%
Or it could do some other work
on the freed-up thread.


636
00:32:39,090 --> 00:32:41,293 line:-1
When there is a lot
of asynchronous work,


637
00:32:41.293 --> 00:32:43.395 line:-1 position:50%
and especially
a lot of contention,


638
00:32:43.395 --> 00:32:44.996 line:-1 position:50%
the system needs
to make trade-offs


639
00:32:44.996 --> 00:32:47.465 line:-1 position:50%
based on what work
is more important.


640
00:32:47,465 --> 00:32:50,235 line:-1
Ideally, high-priority work
such as that


641
00:32:50,235 --> 00:32:52,237 line:-1
involving user interaction,
would take precedence


642
00:32:52,237 --> 00:32:55,373 line:-1
over background work,
such as saving backups.


643
00:32:55,373 --> 00:32:59,444 line:-1
Actors are designed to allow the
system to prioritize work well


644
00:32:59.444 --> 00:33:01.746 line:-1 position:50%
due to the notion of reentrancy.


645
00:33:01,746 --> 00:33:04,616 line:-1
But to understand why reentrancy
is important here,


646
00:33:04,616 --> 00:33:09,855 line:-1
let's first take a look
at how GCD handles priorities.


647
00:33:09,855 --> 00:33:12,190 line:-1
Consider the original
news application


648
00:33:12.190 --> 00:33:14.059 line:-1 position:50%
with the serial database queue.


649
00:33:14.059 --> 00:33:16.595 line:-1 position:50%
Suppose the database receives
some high-priority work,


650
00:33:16.595 --> 00:33:18.163 line:-1 position:50%
such as for fetching
the latest data


651
00:33:18,163 --> 00:33:19,598 line:-1
to update the UI.


652
00:33:19,598 --> 00:33:21,466 position:50%
It also receives
low-priority work,


653
00:33:21,466 --> 00:33:24,202 line:0
such as for backing up
the database to iCloud.


654
00:33:24,202 --> 00:33:26,171 line:0
This needs to be done
at some point,


655
00:33:26,171 --> 00:33:29,174 line:0
but not necessarily immediately.


656
00:33:29,174 --> 00:33:32,077 line:0
As the code runs,
new work items are created


657
00:33:32,077 --> 00:33:36,047 line:0
and added to the database queue
in some interleaved order.


658
00:33:36,047 --> 00:33:38,683 line:0
Dispatch queues
execute the items received


659
00:33:38,683 --> 00:33:42,654 line:0
in a strict first-in,
first-out order.


660
00:33:42,654 --> 00:33:46,324 line:0
Unfortunately, this means that
after item A has executed


661
00:33:46,324 --> 00:33:48,627 line:0
five low-priority items
need to execute


662
00:33:48,627 --> 00:33:51,363 position:50%
before getting to the next
high-priority item.


663
00:33:51,363 --> 00:33:54,199 position:50%
This is called
a priority inversion.


664
00:33:54,199 --> 00:33:57,002 line:0
Serial queues work around
priority inversion


665
00:33:57,002 --> 00:34:00,238 line:0
by boosting the priority
of all of the work in the queue


666
00:34:00,238 --> 00:34:02,674 line:0
that's ahead
of the high-priority work.


667
00:34:02,674 --> 00:34:05,210 line:0
In practice, this means
that the work in the queue


668
00:34:05,210 --> 00:34:08,346 position:50%
will be done sooner.


669
00:34:08,346 --> 00:34:11,049 position:50%
However, it does not resolve
the main issue,


670
00:34:11,049 --> 00:34:14,319 position:50%
which is that items 1 through 5
still needed to complete


671
00:34:14,319 --> 00:34:17,355 line:0
before item B
could start executing.


672
00:34:17,355 --> 00:34:20,792 line:0
Solving this issue requires
changing the semantic model


673
00:34:20,792 --> 00:34:24,996 position:50%
away from strict
first-in, first-out.


674
00:34:24.996 --> 00:34:27.699 line:-1 position:50%
This brings us
to actor reentrancy.


675
00:34:27.699 --> 00:34:30.402 line:-1 position:50%
Let's explore how reentrancy
is connected to ordering


676
00:34:30.402 --> 00:34:32.570 line:-1 position:50%
with an example.


677
00:34:34,239 --> 00:34:37,642 line:0
Consider the database actor
executing on a thread.


678
00:34:37,642 --> 00:34:41,880 line:0
Suppose that it is suspended,
awaiting some work,


679
00:34:41,880 --> 00:34:46,451 position:50%
and the sports feed actor
starts executing on that thread.


680
00:34:46,451 --> 00:34:47,786 line:0
Suppose after a while,


681
00:34:47,786 --> 00:34:50,522 position:50%
the sports feed actor
calls the database actor


682
00:34:50,522 --> 00:34:53,425 position:50%
to save some articles.


683
00:34:53,425 --> 00:34:56,127 line:0
Since the database actor
is uncontended,


684
00:34:56,127 --> 00:34:58,163 position:50%
the thread can hop
to the database actor


685
00:34:58,163 --> 00:35:03,501 position:50%
even though it has
one pending work item.


686
00:35:03,501 --> 00:35:05,470 position:50%
To perform the save operation,


687
00:35:05,470 --> 00:35:09,140 line:0
a new work item will be created
for the database actor.


688
00:35:09,140 --> 00:35:12,143 line:0
This is what actor
reentrancy means;


689
00:35:12,143 --> 00:35:14,679 position:50%
new work items on an actor
can make progress


690
00:35:14,679 --> 00:35:18,283 line:0
while one or more older
work items on it are suspended.


691
00:35:18,283 --> 00:35:20,785 line:0
The actor still maintains
mutual exclusion:


692
00:35:20,785 --> 00:35:25,123 position:50%
at most one work item can be
executing at a given time.


693
00:35:25,123 --> 00:35:29,227 line:0
After some time,
item D2 will finish executing.


694
00:35:29,227 --> 00:35:32,630 line:0
Notice that D2 finished
executing before D1,


695
00:35:32,630 --> 00:35:35,500 line:0
even though it was created
after D1.


696
00:35:35,500 --> 00:35:38,403 line:0
Hence, support for actor
reentrancy means


697
00:35:38,403 --> 00:35:40,905 line:0
that actors can execute
items in an order


698
00:35:40,905 --> 00:35:45,143 line:0
that is not strictly
first-in, first-out.


699
00:35:45,143 --> 00:35:47,612 position:50%
Let's revisit the example
from before


700
00:35:47,612 --> 00:35:51,383 line:0
but with a database actor
instead of a serial queue.


701
00:35:51,383 --> 00:35:55,553 position:50%
First, work item A will execute,
as it has high priority.


702
00:35:55,553 --> 00:35:56,855 line:0
Once that's done,


703
00:35:56,855 --> 00:36:00,291 position:50%
there is the same priority
inversion as before.


704
00:36:00,291 --> 00:36:02,994 line:0
Since actors are designed
for reentrancy,


705
00:36:02,994 --> 00:36:05,530 position:50%
the runtime may choose
to move the higher-priority item


706
00:36:05,530 --> 00:36:06,965 position:50%
to the front of the queue,


707
00:36:06,965 --> 00:36:10,502 position:50%
ahead of the
lower-priority items.


708
00:36:10,502 --> 00:36:14,005 position:50%
This way, higher-priority work
could be executed first,


709
00:36:14,005 --> 00:36:17,475 position:50%
with lower-priority work
following later.


710
00:36:17,475 --> 00:36:20,979 position:50%
This directly addresses the
problem of priority inversion,


711
00:36:20,979 --> 00:36:23,048 position:50%
allowing for more
effective scheduling


712
00:36:23,048 --> 00:36:26,418 position:50%
and resource utilization.


713
00:36:26,418 --> 00:36:27,419 line:-1
I've talked a bit


714
00:36:27.419 --> 00:36:30.021 line:-1 position:50%
about how actors
using the cooperative pool


715
00:36:30.021 --> 00:36:32.190 line:-1 position:50%
are designed to maintain
mutual exclusion


716
00:36:32.190 --> 00:36:35.593 line:-1 position:50%
and support effective
prioritization of work.


717
00:36:35,593 --> 00:36:38,263 line:-1
There is another kind of actor,
the main actor,


718
00:36:38.263 --> 00:36:41.132 line:-1 position:50%
and its characteristics
are somewhat different


719
00:36:41,132 --> 00:36:44,636 line:-1
since it abstracts over an
existing notion in the system:


720
00:36:44.636 --> 00:36:47.572 line:-1 position:50%
the main thread.


721
00:36:47.572 --> 00:36:50.809 line:-1 position:50%
Consider the example
news app using actors.


722
00:36:50,809 --> 00:36:52,911 line:-1
When updating
the user interface,


723
00:36:52,911 --> 00:36:56,448 line:-1
you will need to make calls
to and from MainActor.


724
00:36:56,448 --> 00:36:58,716 line:-1
Since the main thread
is disjoint from the threads


725
00:36:58,716 --> 00:37:02,554 line:-1
in the cooperative pool,
this requires a context switch.


726
00:37:02,554 --> 00:37:04,489 line:-1
Let's look at
the performance implications


727
00:37:04,489 --> 00:37:08,326 line:-1
of this with a code example.


728
00:37:08,326 --> 00:37:09,661 line:-1
Consider the following code


729
00:37:09,661 --> 00:37:13,198 line:-1
where we have a function
updateArticles on MainActor,


730
00:37:13.198 --> 00:37:15.900 line:-1 position:50%
which loads articles
out of the database


731
00:37:15,900 --> 00:37:19,571 line:-1
and updates the UI
for each article.


732
00:37:19,571 --> 00:37:21,339 line:-1
Each iteration of the loop


733
00:37:21.339 --> 00:37:23.441 line:-1 position:50%
requires at least two
context switches:


734
00:37:23.441 --> 00:37:25.844 line:-1 position:50%
one to hop from the main actor
to the database actor


735
00:37:25,844 --> 00:37:27,946 line:-1
and one to hop back.


736
00:37:27,946 --> 00:37:33,485 line:-1
Let's see how the CPU usage
for such a loop would look like.


737
00:37:33,485 --> 00:37:37,155 line:0
Since each loop iteration
requires two context switches,


738
00:37:37,155 --> 00:37:39,858 position:50%
there is a repeating pattern
where two threads


739
00:37:39,858 --> 00:37:43,761 line:0
run one after another
for a short span of time.


740
00:37:43,761 --> 00:37:45,797 line:0
If the number of loop
iterations is low,


741
00:37:45,797 --> 00:37:48,967 line:0
and substantial work
is being done in each iteration,


742
00:37:48,967 --> 00:37:51,202 position:50%
that is probably all right.


743
00:37:51,202 --> 00:37:53,738 line:0
However,
if execution hops on and off


744
00:37:53,738 --> 00:37:55,473 position:50%
the main actor frequently,


745
00:37:55,473 --> 00:38:01,146 line:0
the overhead of switching
threads can start to add up.


746
00:38:01,146 --> 00:38:03,882 line:0
If your application spends
a large fraction of time


747
00:38:03,882 --> 00:38:07,252 line:0
in context switching,
you should restructure your code


748
00:38:07,252 --> 00:38:12,524 position:50%
so that work for the main actor
is batched up.


749
00:38:12,524 --> 00:38:15,627 position:50%
You can batch work by pushing
the loop into the loadArticles


750
00:38:15,627 --> 00:38:17,428 position:50%
and updateUI method calls,


751
00:38:17,428 --> 00:38:21,766 position:50%
making sure they process arrays
instead of one value at a time.


752
00:38:21,766 --> 00:38:25,837 position:50%
Batching up work reduces
the number of context switches.


753
00:38:25,837 --> 00:38:29,374 position:50%
While hopping between actors
on the cooperative pool is fast,


754
00:38:29,374 --> 00:38:33,111 position:50%
you still need to be mindful of
hops to and from the main actor


755
00:38:33,111 --> 00:38:35,780 line:0
when writing your app.


756
00:38:35,780 --> 00:38:38,082 line:-1
Looking back, in this talk


757
00:38:38,082 --> 00:38:40,318 line:-1
you've learned how we've worked
on making the system


758
00:38:40,318 --> 00:38:42,353 line:-1
the most efficient it can be,


759
00:38:42,353 --> 00:38:44,389 line:-1
from the design of the
cooperative thread pool --


760
00:38:44.389 --> 00:38:46.758 line:-1 position:50%
the mechanism
for nonblocking suspension --


761
00:38:46,758 --> 00:38:48,526 line:-1
to how actors are implemented.


762
00:38:48.526 --> 00:38:52.630 line:-1 position:50%
At each step, we're using some
aspect of the runtime contract


763
00:38:52,630 --> 00:38:55,700 line:-1
to improve the performance
of your applications.


764
00:38:55.700 --> 00:38:57.435 line:-1 position:50%
We are excited
to see how you use


765
00:38:57,435 --> 00:39:00,672 line:-1
these incredible new language
features to write clear,


766
00:39:00,672 --> 00:39:03,274 line:-1
efficient, and delightful
Swift code.


767
00:39:03,274 --> 00:39:06,044 line:-1
Thank you for watching
and have a great WWDC.


768
00:39:06,044 --> 00:39:08,846 line:0 align:center size:2%
♪

